{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "import torch.nn.functional as F\n",
    "import minigrid\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from tqdm.notebook import tqdm\n",
    "from minigrid.envs.doorkey import DoorKeyEnv\n",
    "import pandas as pd\n",
    "from gym.envs.registration import registry, register\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from typing_extensions import Self\n",
    "import matplotlib.pyplot as plt\n",
    "from minigrid.wrappers import ObservationWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device() -> torch.device:\n",
    "    \"\"\"\n",
    "    Returns the device to use for training.\n",
    "    \"\"\"\n",
    "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def set_random_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function from https://github.com/ikostrikov/pytorch-a2c-ppo-acktr/blob/master/model.py\n",
    "def init_params(m):\n",
    "    \"\"\"\n",
    "    Initialize parameters of the network.\n",
    "    m: torch.nn.Module\n",
    "    \"\"\"\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Linear\") != -1:\n",
    "        m.weight.data.normal_(0, 1)\n",
    "        m.weight.data *= 1 / torch.sqrt(m.weight.data.pow(2).sum(1, keepdim=True))\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0)\n",
    "\n",
    "class MyDoorKeyEnv(DoorKeyEnv):\n",
    "    def __init__(self, size):\n",
    "        self.render_mode = \"rgb_array\"\n",
    "        super().__init__(size=size)\n",
    "\n",
    "    def _reward(self):\n",
    "        \"\"\"\n",
    "        Compute the reward to be given upon success\n",
    "        \"\"\"\n",
    "        return 1\n",
    "\n",
    "class ImgObsWrapper(ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Use the image as the only observation output, no language/mission.\n",
    "\n",
    "    Parameters:\n",
    "    - env (gym.Env): The environment to wrap.\n",
    "\n",
    "    Methods:\n",
    "    - observation(self, obs): Returns the image from the observation.\n",
    "    - reset(self): Resets the environment and returns the initial observation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        \"\"\"\n",
    "        Initializes the ImgObsWrapper with the given environment.\n",
    "\n",
    "        Parameters:\n",
    "        - env (gym.Env): The environment whose observations are to be wrapped.\n",
    "        \"\"\"\n",
    "        super().__init__(env)\n",
    "        self.observation_space = env.observation_space.spaces[\"image\"]\n",
    "\n",
    "    def observation(self, obs):\n",
    "        \"\"\"\n",
    "        Extracts and returns the image data from the observation.\n",
    "\n",
    "        Parameters:\n",
    "        - obs (dict or tuple): The original observation from the environment, which could be either\n",
    "        a dictionary or a tuple containing a dictionary.\n",
    "\n",
    "        Returns:\n",
    "        - np.ndarray: The image data extracted from the observation.\n",
    "        \"\"\"\n",
    "        if type(obs) == tuple:\n",
    "            return obs[0][\"image\"]\n",
    "        return obs[\"image\"]\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the environment and returns the initial observation image.\n",
    "\n",
    "        Returns:\n",
    "        - np.ndarray: The initial observation image of the reset environment.\n",
    "        \"\"\"\n",
    "        obs = super().reset()\n",
    "        return obs[0]\n",
    "    \n",
    "def get_door_key_env(size):\n",
    "    \"\"\"\n",
    "    Returns a DoorKeyEnv environment with the given size.\n",
    "    \"\"\"\n",
    "    env = MyDoorKeyEnv(size=size)\n",
    "    env = ImgObsWrapper(env)\n",
    "\n",
    "    env.reset()\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    # get an RGB image corresponding to the whole environment or the agent's point of view (https://github.com/Farama-Foundation/Minigrid/blob/master/minigrid/minigrid_env.py#L716)\n",
    "    #            highlight (bool): If true, the agent's field of view or point of view is highlighted with a lighter gray color.\n",
    "    #            tile_size (int): How many pixels will form a tile from the NxM grid.\n",
    "    #            agent_pov (bool): If true, the rendered frame will only contain the point of view of the agent.\n",
    "    frame = env.get_frame(highlight=env.highlight, tile_size=env.tile_size, agent_pov=env.agent_pov)\n",
    "    # show an image to the notebook.\n",
    "    plt.imshow(frame)\n",
    "\n",
    "    return env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"\n",
    "    Stores algorithmic hyperparameters.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                score_threshold=0.93,\n",
    "                discount=0.995,\n",
    "                lr=1e-3,\n",
    "                max_grad_norm=0.5,\n",
    "                log_interval=10,\n",
    "                gae_lambda=0.95,\n",
    "                clip_ratio=0.2,\n",
    "                target_kl=0.01,\n",
    "                train_ac_iters=5,\n",
    "                use_discounted_reward=True,\n",
    "                use_gae=True,\n",
    "                importance_sampling_clip=2.0,\n",
    "                bad_fit_threshold=0.8,\n",
    "                bad_fit_increment=None,\n",
    "                replay_buffer_capacity=10,\n",
    "                large_buffer_capacity=20):\n",
    "\n",
    "        self.score_threshold = score_threshold # criterion for early stopping. If the rolling average reward (over the last 100 episodes) is greater than it, it ends.\n",
    "        self.discount = discount # discount factor\n",
    "        self.lr = lr # learning rate\n",
    "        self.max_grad_norm = max_grad_norm # the maximum gradient norm (https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html)\n",
    "        self.log_interval = log_interval # logging interval\n",
    "        self.clip_ratio = clip_ratio # clip_ratio of PPO.\n",
    "        self.target_kl = target_kl # target KL divergence for early stoping train_ac_iters for PPO\n",
    "        self.train_ac_iters = train_ac_iters # how many time to train ac_model using current computed old_logps\n",
    "        self.gae_lambda=gae_lambda # lambda in Generalized Advantage Estimation (GAE)\n",
    "        self.use_discounted_reward=use_discounted_reward # whether use discounted reward or not.\n",
    "        self.use_gae = use_gae # whether to use GAE or not.\n",
    "        self.importance_sampling_clip = importance_sampling_clip # importance sampling clip threshold\n",
    "        self.bad_fit_threshold = bad_fit_threshold # threshold for bad fit.\n",
    "        if bad_fit_increment is None:\n",
    "            bad_fit_increment = (1.0 - bad_fit_threshold) / replay_buffer_capacity\n",
    "        self.bad_fit_increment = bad_fit_increment # increment for bad fit.\n",
    "        self.replay_buffer_capacity = replay_buffer_capacity # capacity of replay buffer.\n",
    "        self.large_buffer_capacity = large_buffer_capacity # capacity of large replay buffer.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Machine:\n",
    "    def __init__(self, entropy_coef, init_model:nn.Module, args:Config=None):\n",
    "        \"\"\"\n",
    "        A Machine object consists of a Model and its entropy_coef\n",
    "\n",
    "        Args:\n",
    "            entropy_coef: Entropy coefficient.\n",
    "            init_model: Initial model.\n",
    "            args\n",
    "        \"\"\"\n",
    "        if args is None:\n",
    "            self.args = Config()\n",
    "        else:\n",
    "            self.args = args\n",
    "\n",
    "        self.model = init_model\n",
    "        self.optim = torch.optim.Adam(self.model.parameters(), lr=self.args.lr)\n",
    "        self.coef = entropy_coef\n",
    "        self.device = get_device()\n",
    "\n",
    "    def copy_model(self, other_model:nn.Module) -> None:\n",
    "        \"\"\"\n",
    "        Copy state dict from 'model'. Reset rs.\n",
    "        \"\"\"\n",
    "        state_dict = other_model.state_dict()\n",
    "        for key, v in state_dict.items():\n",
    "            if key in self.model.state_dict():\n",
    "                self.model.state_dict()[key].copy_(v)\n",
    "\n",
    "    def copy_machine(self, other:Self) -> None:\n",
    "        \"\"\"\n",
    "        Copy state dict from 'other'. Reset rs.\n",
    "        \"\"\"\n",
    "        self.copy_model(other.model)\n",
    "\n",
    "    def _compute_discounted_return(self, rewards):\n",
    "        \"\"\"\n",
    "            rewards: reward obtained at timestep.  Shape: (T,)\n",
    "            discount: discount factor. float\n",
    "\n",
    "        ----\n",
    "        returns: sum of discounted rewards. Shape: (T,)\n",
    "        \"\"\"\n",
    "        returns = torch.zeros(*rewards.shape, device=self.device)\n",
    "\n",
    "        R = 0\n",
    "        for t in reversed(range((rewards.shape[0]))):\n",
    "            R = rewards[t] + self.args.discount * R\n",
    "            returns[t] = R\n",
    "        return returns\n",
    "\n",
    "    def _compute_advantage_gae(self, values, rewards, T):\n",
    "        \"\"\"\n",
    "        Compute Adavantage wiht GAE. See Section 4.4.2 in the lecture notes.\n",
    "\n",
    "        values: value at each timestep (T,)\n",
    "        rewards: reward obtained at each timestep.  Shape: (T,)\n",
    "        T: the number of frames, float\n",
    "        gae_lambda: hyperparameter, float\n",
    "        discount: discount factor, float\n",
    "\n",
    "        -----\n",
    "\n",
    "        returns:\n",
    "\n",
    "        advantages : tensor.float. Shape [T,]\n",
    "\n",
    "                    gae advantage term for timesteps 0 to T\n",
    "\n",
    "        \"\"\"\n",
    "        advantages = torch.zeros_like(values)\n",
    "        for i in reversed(range(T)):\n",
    "            next_value = values[i+1]\n",
    "            next_advantage = advantages[i+1]\n",
    "\n",
    "            delta = rewards[i] + self.args.discount * next_value  - values[i]\n",
    "            advantages[i] = delta + self.args.discount * self.args.gae_lambda * next_advantage\n",
    "        return advantages[:T]\n",
    "    \n",
    "    def collect_experiences(self, env:gym.Env):\n",
    "        \"\"\"\n",
    "        Collects rollouts and computes advantages.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        exps : dict\n",
    "            Contains actions, rewards, advantages etc as attributes.\n",
    "            Each attribute, e.g. `exps['reward']` has a shape\n",
    "            (self.num_frames, ...).\n",
    "        logs : dict\n",
    "            Useful stats about the training process, including the average\n",
    "            reward, policy loss, value loss, etc.\n",
    "        \"\"\"\n",
    "        device = get_device()\n",
    "\n",
    "        MAX_FRAMES_PER_EP = 300\n",
    "        shape = (MAX_FRAMES_PER_EP, )\n",
    "\n",
    "        actions = torch.zeros(*shape, device=device, dtype=torch.int)\n",
    "        values = torch.zeros(*shape, device=device)\n",
    "        rewards = torch.zeros(*shape, device=device)\n",
    "        log_probs = torch.zeros(*shape, device=device)\n",
    "        obss = [None]*MAX_FRAMES_PER_EP\n",
    "\n",
    "        obs = env.reset()\n",
    "\n",
    "        total_return = 0\n",
    "\n",
    "        T = 0\n",
    "\n",
    "        while True:\n",
    "            # Do one agent-environment interaction\n",
    "            with torch.no_grad():\n",
    "                dist, value = self.model(obs)\n",
    "\n",
    "            dist: Categorical\n",
    "            action = dist.sample()[0]\n",
    "\n",
    "            obss[T] = obs\n",
    "            obs, reward, done, _, _ = env.step(action.item())\n",
    "\n",
    "            # Update experiences values\n",
    "            actions[T] = action\n",
    "            values[T] = value\n",
    "            rewards[T] = reward\n",
    "            log_probs[T] = dist.log_prob(action)\n",
    "\n",
    "            total_return += reward\n",
    "            T += 1\n",
    "\n",
    "            if done or T >= MAX_FRAMES_PER_EP-1:\n",
    "                break\n",
    "\n",
    "        success = (total_return > 0.5)\n",
    "        \n",
    "        discounted_reward = self._compute_discounted_return(rewards[:T])\n",
    "        exps = dict(\n",
    "            obs = torch.tensor(obss[:T], device=device),\n",
    "            action = actions[:T],\n",
    "            value  = values[:T],\n",
    "            reward = rewards[:T],\n",
    "            log_prob = log_probs[:T],\n",
    "            discounted_reward = discounted_reward,\n",
    "            T = T\n",
    "        )\n",
    "\n",
    "        logs = {\n",
    "            \"return_per_episode\": total_return,\n",
    "            \"num_frames\": T,\n",
    "            'success': success\n",
    "        }\n",
    "\n",
    "        return exps, logs\n",
    "\n",
    "    def _compute_policy_loss_ppo(self, dist:Categorical, factors, indices, old_logp, actions, advantages):\n",
    "        \"\"\"\n",
    "        Computes the policy loss for PPO.\n",
    "\n",
    "        obs: observeration to pass into acmodel. shape: (T,)\n",
    "        init_logp: log probabilities we get from the agent performing the action. shape: (T,)\n",
    "        old_logp: log probabilities from previous timestep. shape: (T,)\n",
    "        actions: action at this timestep. shape: (T,ImWidth,ImHeight,Channels)\n",
    "        advantages: the computed advantages. shape: (T,)\n",
    "\n",
    "        ---\n",
    "        returns\n",
    "\n",
    "        policy_loss : ppo policy loss as shown in line 6 of PPO alg. tensor.float. Shape (,1)\n",
    "        approx_kl: an appoximation of the kl_divergence. tensor.float. Shape (,1)\n",
    "        \"\"\"\n",
    "        policy_loss, approx_kl = 0, 0\n",
    "\n",
    "        coef = self.coef\n",
    "\n",
    "        entropy = dist.entropy()\n",
    "        logps = dist.log_prob(actions)\n",
    "        r_terms = torch.exp(logps - old_logp)\n",
    "        ppo_loss = torch.min(r_terms * advantages, torch.clamp(r_terms, 1 - self.args.clip_ratio, 1 + self.args.clip_ratio) * advantages)\n",
    "        \n",
    "        policy_loss_tensor = factors * ppo_loss + coef * entropy\n",
    "\n",
    "        policy_loss = - torch.mean(policy_loss_tensor[indices])\n",
    "\n",
    "        # approx_kl = torch.sum(torch.exp(old_logp) * (old_logp - logps)) / torch.sum(torch.exp(old_logp))\n",
    "        approx_kl = torch.mean((old_logp - logps) ** 2) / 2\n",
    "\n",
    "        return policy_loss, approx_kl\n",
    "    \n",
    "    def _compute_value_loss(self, values, returns):\n",
    "        value_loss = torch.mean((values - returns) ** 2)\n",
    "\n",
    "        return value_loss\n",
    "\n",
    "    def update_parameters(self, sb, update_v=True):\n",
    "        MAX_FRAMES_PER_EP = 300\n",
    "        T = sb['T']\n",
    "        with torch.no_grad():\n",
    "            dist, values = self.model(sb['obs'])\n",
    "        values = values.reshape(-1)\n",
    "        dist: Categorical\n",
    "        old_logp = dist.log_prob(sb['action'])\n",
    "        init_logp = sb['log_prob']\n",
    "\n",
    "        # add 0 to end of values until it has length MAX_FRAMES_PER_EP\n",
    "        values_extended = torch.cat([values, torch.zeros((MAX_FRAMES_PER_EP - len(values), ), device=get_device())], dim=0)\n",
    "        full_reward = torch.cat([sb['reward'], torch.zeros((MAX_FRAMES_PER_EP - len(sb['reward']), ), device=get_device())], dim=0)\n",
    "\n",
    "        if self.args.use_gae:\n",
    "            advantage = self._compute_advantage_gae(values_extended, full_reward, T)\n",
    "        else:\n",
    "            advantage = sb['discounted_reward'] - values.reshape(-1)\n",
    "\n",
    "        for i in range(self.args.train_ac_iters):\n",
    "            self.optim.zero_grad()\n",
    "            dist, values = self.model(sb['obs'])\n",
    "            values = values.reshape(-1)\n",
    "            # policy loss\n",
    "            factors = torch.exp(old_logp - init_logp)\n",
    "            indices = factors < self.args.importance_sampling_clip\n",
    "            fit = torch.mean(indices.to(torch.float32))\n",
    "\n",
    "            loss_pi, approx_kl = self._compute_policy_loss_ppo(dist, factors, indices, old_logp, sb['action'], advantage)\n",
    "            if update_v:\n",
    "                loss_v = self._compute_value_loss(values, sb['discounted_reward'])\n",
    "            else:\n",
    "                loss_v = 0.0\n",
    "\n",
    "            if i == 0:\n",
    "                policy_loss = loss_pi\n",
    "                value_loss = loss_v\n",
    "\n",
    "            loss = loss_v + loss_pi\n",
    "            if approx_kl > 1.5 * self.args.target_kl:\n",
    "                break\n",
    "\n",
    "            loss.backward(retain_graph=True)\n",
    "            self.optim.step()\n",
    "\n",
    "        update_policy_loss = policy_loss.item()\n",
    "        update_value_loss = value_loss.item()\n",
    "\n",
    "        logs = {\n",
    "            \"policy_loss\": update_policy_loss,\n",
    "            \"value_loss\": update_value_loss,\n",
    "            \"fit\": fit.item()\n",
    "        }\n",
    "\n",
    "        return logs\n",
    "    \n",
    "    def decrease_prob(self, sb, lr=0.1) -> None:\n",
    "        self.optim.zero_grad()\n",
    "\n",
    "        dist, _ = self.model(sb['obs'])\n",
    "        dist: Categorical\n",
    "        logps = dist.log_prob(sb['action'])\n",
    "\n",
    "        loss = lr * torch.mean(logps)\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self, ACModelClass, env, args:Config=None, seed=0):\n",
    "        \"\"\"\n",
    "        The PPO agent.\n",
    "        \"\"\"\n",
    "\n",
    "        self.env = env\n",
    "\n",
    "        set_random_seed(seed)\n",
    "        model = ACModelClass(use_critic=True)\n",
    "\n",
    "        if args is None:\n",
    "            args = Config()\n",
    "        self.machine = Machine(0.01, model, args)\n",
    "    \n",
    "    def train(self, max_episodes:int=10000, nonstop:bool=False, max_frames=float('inf')) -> tuple[list[int], list[float]]:\n",
    "        \"\"\"\n",
    "        Train the PPO agent.\n",
    "\n",
    "        Returns:\n",
    "            num_frames, smooth_rs\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f'Start! Agent: PPO.')\n",
    "\n",
    "        is_solved = False\n",
    "\n",
    "        SMOOTH_REWARD_WINDOW = 50\n",
    "\n",
    "        rewards = [0]*SMOOTH_REWARD_WINDOW\n",
    "\n",
    "        total_smooth_rs = []\n",
    "        total_num_frames = []\n",
    "\n",
    "        num_frames = 0\n",
    "\n",
    "        pbar = tqdm(range(max_episodes))\n",
    "        for update in pbar:\n",
    "\n",
    "            exps, logs1 = self.machine.collect_experiences(self.env)\n",
    "\n",
    "            logs2 = self.machine.update_parameters(exps)\n",
    "\n",
    "            logs = {**logs1, **logs2}\n",
    "\n",
    "            total_num_frames.append(num_frames)\n",
    "            num_frames += logs[\"num_frames\"]\n",
    "\n",
    "            rewards.append(logs[\"return_per_episode\"])\n",
    "\n",
    "            smooth_reward = np.mean(rewards[-SMOOTH_REWARD_WINDOW:])\n",
    "\n",
    "            total_smooth_rs.append(smooth_reward)\n",
    "\n",
    "            data = {'num_frames':num_frames, 'smooth_reward':smooth_reward,\n",
    "                'reward':logs[\"return_per_episode\"], 'policy_loss':logs[\"policy_loss\"], 'value_loss': logs['value_loss'], 'episode':update}\n",
    "\n",
    "            pbar.set_postfix(data)\n",
    "\n",
    "            if not nonstop and smooth_reward >= self.machine.args.score_threshold:\n",
    "                is_solved = True\n",
    "                break\n",
    "            if num_frames >= max_frames:\n",
    "                break\n",
    "\n",
    "        if not nonstop and is_solved:\n",
    "            print('Solved!')\n",
    "\n",
    "        return total_num_frames, total_smooth_rs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/atticusw/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import minigrid\n",
    "from tqdm.notebook import tqdm\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from torch import optim\n",
    "from typing import Any\n",
    "from copy import deepcopy\n",
    "from collections import deque\n",
    "from minigrid.wrappers import ObservationWrapper\n",
    "from minigrid.envs.doorkey import DoorKeyEnv\n",
    "\n",
    "class DQNetwork(nn.Module):\n",
    "    def __init__(self, action_dim):\n",
    "        \"\"\"\n",
    "        Initializes the DQNetwork with a convolutional neural network architecture.\n",
    "\n",
    "        Parameters:\n",
    "        - action_dim (int): The number of possible actions, determining the output size of the network.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.conv_net = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, (3, 3)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, (3, 3)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, (3, 3)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fcs = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, ob):\n",
    "        \"\"\"\n",
    "        Processes an observation through the network to predict Q values for each action.\n",
    "\n",
    "        Parameters:\n",
    "        - ob (torch.Tensor): The input observation image of shape [batch_size, height, width, channels].\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: The predicted Q values for each action, of shape [batch_size, action_dim].\n",
    "        \"\"\"\n",
    "        #### TODO (5pts): get the Q values for each action given the input\n",
    "        #### the input shape is: [batch_size, H, W, 3]\n",
    "        #### output shape should be: [batch_size, # of actions]\n",
    "        ob = ob.permute(0, 3, 1, 2)\n",
    "        bs = ob.size(0)\n",
    "        out = self.conv_net(ob)\n",
    "        out = out.view(bs, -1)\n",
    "        out = self.fcs(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "# create a replay buffer\n",
    "class CyclicBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = []\n",
    "        self.capacity = capacity\n",
    "        self.cur_pos = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.buffer[item]\n",
    "\n",
    "    def append(self, data):\n",
    "        \"\"\"\n",
    "        Adds a new piece of data to the buffer.\n",
    "\n",
    "        Parameters:\n",
    "        - data: The data to be added to the buffer.\n",
    "        \"\"\"\n",
    "        #### TODO (10pts): add data to the buffer\n",
    "        #### if the buffer is not full yet, you can simply append the data to the buffer\n",
    "        #### otherwise, you need to replace the oldest data with the current data (FIFO)\n",
    "        #### Hint: you may find self.cur_pos useful, it can be used as a position index\n",
    "        #### to keep track of where to add data\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(data)\n",
    "            self.cur_pos = (self.cur_pos + 1) % self.capacity\n",
    "        else:\n",
    "            self.buffer[self.cur_pos] = data\n",
    "            self.cur_pos = (self.cur_pos + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Randomly selects a batch of data from the buffer. The size of the batch is the minimum of the requested\n",
    "        batch size and the current size of the buffer. If the requested batch size equals the buffer size, all\n",
    "        data in the buffer is returned.\n",
    "\n",
    "        Parameters:\n",
    "        - batch_size (int): The size of the batch to sample.\n",
    "\n",
    "        Returns:\n",
    "        - list: A list containing the sampled batch of data.\n",
    "        \"\"\"\n",
    "        #### TODO (10pts): sample a batch from the buffer\n",
    "        bs = min(batch_size, len(self.buffer))\n",
    "        return random.sample(self.buffer, bs)\n",
    "\n",
    "    def get_all(self):\n",
    "        \"\"\"\n",
    "        Retrieves all data stored in the buffer.\n",
    "\n",
    "        Returns:\n",
    "        - list: A deepcopy of all data currently stored in the buffer.\n",
    "        \"\"\"\n",
    "        return deepcopy(self.buffer)\n",
    "\n",
    "    def clear(self):\n",
    "        \"\"\"\n",
    "        Removes all data from the buffer, effectively resetting its state.\n",
    "        \"\"\"\n",
    "        self.buffer.clear()\n",
    "        self.cur_pos = 0\n",
    "\n",
    "@dataclass\n",
    "class DQNAgent:\n",
    "    env: gym.Env\n",
    "    learning_rate: float\n",
    "    gamma: float\n",
    "    memory_size: int # We use memory/buffer interchangeably\n",
    "    initial_epsilon: float\n",
    "    min_epsilon: float\n",
    "    max_epsilon_decay_steps: int\n",
    "    warmup_steps: int\n",
    "    batch_size: int\n",
    "    target_update_freq: int\n",
    "    enable_double_q: bool = False\n",
    "    disable_target_net: bool = False\n",
    "    device: str = None\n",
    "    tau: float = 0.005\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the agent to its initial state.\n",
    "        \"\"\"\n",
    "        if self.device is None:\n",
    "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        #### TODO: create a Deep Q network Agent.\n",
    "        #### For our purposes include the following:\n",
    "        #### -a replay buffer with capacity=self.memory_size. Memory = Buffer\n",
    "        self.memory = CyclicBuffer(self.memory_size)\n",
    "        #### -an Adam optimizer with lr=self.learning_rate,\n",
    "        self.qnet = DQNetwork(action_dim=7).to(self.device)\n",
    "        self.optim = torch.optim.Adam(self.qnet.parameters(), lr=self.learning_rate)\n",
    "        #### -SmoothL1 loss instance,\n",
    "        self.loss = nn.SmoothL1Loss()\n",
    "        ####  -a deep Q network\n",
    "        ####  -A deep Q TARGET network (Make sure to set to eval mode)\n",
    "        self.target_qnet = DQNetwork(action_dim=7).to(self.device)\n",
    "        self.target_qnet.eval()\n",
    "        #### -Make sure the networks are on the correct device!!! use [tensor].to(self.device)\n",
    "\n",
    "        ####\n",
    "        self.epsilon = self.initial_epsilon\n",
    "        self.ep_reduction = (self.epsilon - self.min_epsilon) / float(self.max_epsilon_decay_steps)\n",
    "        if self.disable_target_net:\n",
    "            #### TODO: set the target_update_freq to be proper value so that the target Q network will always be same as the Q network\n",
    "            #### You don't need to fill in this value until Q4.3\n",
    "            self.target_update_freq = 1\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_action(self, ob, greedy_only=False):\n",
    "        \"\"\"\n",
    "        Selects an action based on the current state observation using an epsilon-greedy policy.\n",
    "\n",
    "        Parameters:\n",
    "        - ob (numpy.ndarray): The current state observation.\n",
    "        - greedy_only (bool): If True, the method always selects the action with the highest Q value. If False, it\n",
    "        selects a random action with probability epsilon.\n",
    "\n",
    "        Returns:\n",
    "        - int: The selected action.\n",
    "        \"\"\"\n",
    "        ob = ob[np.newaxis, :]\n",
    "        ob = torch.from_numpy(ob).float().to(self.device)\n",
    "        q_val = self.qnet(ob)\n",
    "        action = self.epsilon_greedy_policy(q_val, greedy_only=greedy_only)\n",
    "        return action\n",
    "\n",
    "    def epsilon_greedy_policy(self, q_values, greedy_only=False):\n",
    "        \"\"\"\n",
    "        Implements an epsilon-greedy policy for action selection.\n",
    "\n",
    "        Parameters:\n",
    "        - q_values (torch.Tensor): The Q values for all actions in the current state.\n",
    "        - greedy_only (bool): If True, ignores epsilon and selects the action with the highest Q value.\n",
    "\n",
    "        Returns:\n",
    "        - int: The index of the selected action.\n",
    "        \"\"\"\n",
    "        #### TODO: epsilon greedy exploration\n",
    "        #### we have an extra flag `greedy_only` here,\n",
    "        #### if greedy_only is True, then we need to return the action that\n",
    "        #### has the maximum Q values.\n",
    "        #### if greedy_only is False, we do epsilon greedy.\n",
    "        mode = 'random'\n",
    "        if greedy_only:\n",
    "            mode = 'greedy'\n",
    "        else:\n",
    "            if random.random() < self.epsilon:\n",
    "                mode = 'random'\n",
    "            else:\n",
    "                mode = 'greedy'\n",
    "\n",
    "        if mode == 'random':\n",
    "            action = np.random.randint(0, 7)\n",
    "        elif mode == 'greedy':\n",
    "            action = torch.argmax(q_values).item()\n",
    "        else:\n",
    "            raise ValueError('Unknown mode')\n",
    "\n",
    "        return action\n",
    "\n",
    "    def add_to_memory(self, ob, next_ob, action, reward, done):\n",
    "        \"\"\"\n",
    "        Adds an experience tuple to the replay buffer.\n",
    "\n",
    "        Parameters:\n",
    "        - ob: The current state observation.\n",
    "        - next_ob: The next state observation after taking the action.\n",
    "        - action: The action taken in the current state.\n",
    "        - reward: The reward received after taking the action.\n",
    "        - done: A boolean indicating whether the episode has ended.\n",
    "        \"\"\"\n",
    "        #### TODO: add data to the replay buffer. Avoid np.arrays.\n",
    "        data = (ob, next_ob, action, reward, done)\n",
    "        self.memory.append(data)\n",
    "\n",
    "    def update_Q(self):\n",
    "        \"\"\"\n",
    "        Performs a single update step of the Q-network based on a batch of experiences from the replay buffer.\n",
    "\n",
    "        Returns:\n",
    "        - float: The loss value of the update step.\n",
    "        \"\"\"\n",
    "        # we only start updating the Q network if there are enough samples in the replay buffer\n",
    "        if len(self.memory) < self.warmup_steps:\n",
    "            return 0\n",
    "\n",
    "        #### TODO: sample data from the replay buffer, and put them on the correct device (use [tensor].to(self.device))\n",
    "        #### you need to make sure the variables are in the right tensor shape.\n",
    "        data = self.memory.sample(self.batch_size)\n",
    "\n",
    "\n",
    "        #### TODO: update Q function with Bellman backup. Torch.gather() may prove useful for this section\n",
    "        ##### get Q(s_t, a_t)\n",
    "        obs, next_obs, actions, rewards, dones = zip(*data)\n",
    "        obs = np.array(obs)\n",
    "        next_obs = np.array(next_obs)\n",
    "        actions = np.array(actions)\n",
    "        rewards = np.array(rewards)\n",
    "        dones = np.array(dones)\n",
    "        obs = torch.tensor(obs, device=self.device, dtype=torch.float32)\n",
    "        next_obs = torch.tensor(next_obs, device=self.device, dtype=torch.float32)\n",
    "        actions = torch.tensor(actions, device=self.device)\n",
    "        rewards = torch.tensor(rewards, device=self.device, dtype=torch.float32)\n",
    "        dones = torch.tensor(dones, device=self.device, dtype=torch.int32)\n",
    "\n",
    "        q_values = self.qnet(obs)\n",
    "        # then select the index from q_values according to actions\n",
    "        q_values = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "\n",
    "        ##### get maxQ(s_{t+1}, a_{t+1})\n",
    "        ##### you will need to implement both DQN and double DQN here\n",
    "        ##### i.e., you need to check `if self.enable_double_q`\n",
    "        ##### remember to not propogate gradient when working with target\n",
    "        if self.enable_double_q:\n",
    "            with torch.no_grad():\n",
    "                out = self.qnet(next_obs)\n",
    "                max_q_indices = torch.max(out, dim=1)[1]\n",
    "\n",
    "                out = self.target_qnet(next_obs)\n",
    "                max_q_values = out.gather(1, max_q_indices.unsqueeze(1)).squeeze(1)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                out = self.target_qnet(next_obs)\n",
    "                max_q_values = torch.max(out, dim=1)[0]\n",
    "\n",
    "\n",
    "        ##### get the target Q value from the bellman equation\n",
    "        target_q_values = rewards + (1 - dones) * self.gamma * max_q_values\n",
    "\n",
    "        ##### update the Q network\n",
    "        self.optim.zero_grad()\n",
    "        loss = self.loss(target_q_values, q_values)\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"\n",
    "        Reduces epsilon linearly over time until it reaches min_epsilon, ensuring that the exploration rate decreases\n",
    "        as the agent learns more about the environment.\n",
    "        \"\"\"\n",
    "        #### TODO: linearly decay epsilon\n",
    "        #### reduce epsilon value by ep_reduction every time the function is called,\n",
    "        #### make sure epsilon is not smaller than self.min_epsilon\n",
    "        self.epsilon = max(self.epsilon - self.ep_reduction, self.min_epsilon)\n",
    "\n",
    "    def set_epsilon(self, eps) -> None:\n",
    "        \"\"\"\n",
    "        Sets the epsilon value to a specified value.\n",
    "\n",
    "        Parameters:\n",
    "        - eps (float): The new epsilon value.\n",
    "        \"\"\"\n",
    "        self.epsilon = eps\n",
    "\n",
    "    def update_target_qnet(self, step, soft=True):\n",
    "        \"\"\"\n",
    "        Updates the target Q-network.\n",
    "\n",
    "        Parameters:\n",
    "        - step (int): The current step number, used to determine when to perform hard updates.\n",
    "        - soft (bool): If True, performs a soft update; otherwise, performs a hard update.\n",
    "        \"\"\"\n",
    "        if not soft:\n",
    "            if step % self.target_update_freq == 0:\n",
    "                #### TODO: update the target Q function in a \"hard\" way\n",
    "                #### copy the parameter values in self.qnet into self.target_qnet\n",
    "                ### use .copy_() to avoid pointer issues\n",
    "                state_dict = self.qnet.state_dict()\n",
    "                for key, v in state_dict.items():\n",
    "                    self.target_qnet.state_dict()[key].copy_(v)\n",
    "                self.target_qnet.eval()\n",
    "        else:\n",
    "            #### TODO: soft update on taget Q network.\n",
    "            #### similar to polyak averaging, we update the target Q network slowly\n",
    "            #### $\\theta_Qtgt = \\tau*\\theta_Qtgt + (1-\\tau)*\\theta_Q\n",
    "            ###  use .copy_() to avoid pointer issues\n",
    "            state_dict = self.qnet.state_dict()\n",
    "            for key, v in state_dict.items():\n",
    "                self.target_qnet.state_dict()[key].copy_((1 - self.tau) * self.target_qnet.state_dict()[key] + self.tau * v)\n",
    "            self.target_qnet.eval()\n",
    "\n",
    "# you don't need to modify the following code.\n",
    "@dataclass\n",
    "class DQNEngine:\n",
    "    env: gym.Env\n",
    "    agent: DQNAgent\n",
    "    max_steps: int\n",
    "    show_progress: bool = False\n",
    "    show_video: bool = False\n",
    "\n",
    "    def test(self, env=None, render=False):\n",
    "        \"\"\"\n",
    "        Evaluates the agent's performance in the given environment.\n",
    "\n",
    "        Parameters:\n",
    "        - env (gym.Env, optional): The environment to test the agent in. If None, uses the engine's environment.\n",
    "        - render (bool): Whether to render the environment at each step.\n",
    "\n",
    "        Returns:\n",
    "        - float: The total reward accumulated over the episode.\n",
    "        \"\"\"\n",
    "        env = self.env if env is None else env\n",
    "        ob = env.reset()\n",
    "        ret = 0\n",
    "        for i in range(100):\n",
    "            if render:\n",
    "                env.render()\n",
    "            action = self.agent.get_action(ob, greedy_only=True)\n",
    "            next_ob, reward, done, truncated, info = env.step(action)\n",
    "            ret += reward\n",
    "            ob = next_ob\n",
    "            if done:\n",
    "                break\n",
    "        return ret\n",
    "\n",
    "    def run(self, n_runs=1):\n",
    "        \"\"\"\n",
    "        Executes multiple runs of the agent in the environment, training the agent in each run.\n",
    "\n",
    "        Parameters:\n",
    "        - n_runs (int): The number of separate runs to execute.\n",
    "\n",
    "        Returns:\n",
    "        - list: A list of pandas DataFrame logs for each run, containing columns for the cumulative reward ('return'),\n",
    "                the step count ('steps'), the episode number ('episode'), and the initial epsilon value ('epsilon').\n",
    "        \"\"\"\n",
    "        eps = 0.999\n",
    "\n",
    "        rewards = []\n",
    "        log = []\n",
    "\n",
    "        for i in tqdm(range(n_runs), desc='Runs'):\n",
    "            ep_rewards = []\n",
    "            ep_steps = []\n",
    "            self.agent.reset()\n",
    "            # we plot the smoothed return values\n",
    "            smooth_ep_return = deque(maxlen=10)\n",
    "            ob = self.env.reset()\n",
    "            ret = 0\n",
    "            num_ep = 0\n",
    "            for t in tqdm(range(self.max_steps), desc='Step'):\n",
    "                if len(self.agent.memory) < self.agent.warmup_steps:\n",
    "                    action = self.env.action_space.sample()\n",
    "                else:\n",
    "                    action = self.agent.get_action(ob)\n",
    "                next_ob, reward, done, truncated, info = self.env.step(action)\n",
    "                true_done = done and not info.get('TimeLimit.truncated', False)\n",
    "                self.agent.add_to_memory(ob, next_ob, action, reward, true_done)\n",
    "                self.agent.update_Q()\n",
    "                ret += reward\n",
    "                ob = next_ob\n",
    "                if done or truncated:\n",
    "                    ob = self.env.reset()\n",
    "                    smooth_ep_return.append(ret)\n",
    "                    ep_rewards.append(np.mean(smooth_ep_return))\n",
    "                    ep_steps.append(t)\n",
    "                    ret = 0\n",
    "                    num_ep += 1\n",
    "                    if self.show_progress:\n",
    "                        print(f'Step:{t}  epsilon:{self.agent.epsilon}  '\n",
    "                            f'Smoothed Training Return:{np.mean(smooth_ep_return)}')\n",
    "                    if num_ep % 10 == 0:\n",
    "                        test_ret = self.test()\n",
    "                        if self.show_progress:\n",
    "                            print('==========================')\n",
    "                            print(f'Step:{t} Testing Return: {test_ret}')\n",
    "                    \n",
    "                self.agent.decay_epsilon()\n",
    "                self.agent.update_target_qnet(t, soft=not self.agent.disable_target_net)\n",
    "\n",
    "            rewards.append(ep_rewards)\n",
    "            run_log = pd.DataFrame({'return': ep_rewards,\n",
    "                                    'steps': ep_steps,\n",
    "                                    'episode': np.arange(len(ep_rewards)),\n",
    "                                    'epsilon': self.agent.initial_epsilon})\n",
    "            log.append(run_log)\n",
    "        return log\n",
    "\n",
    "def dqn_sweep(agents, labels, n_runs=1, max_steps=100000, show_progress=False):\n",
    "    \"\"\"\n",
    "    Performs a sweep over different DQN agents to compare their performance.\n",
    "    This function takes a list of DQN agents and their labels, then runs each agent for a specified number of runs and\n",
    "    steps, tracking their performance. Useful for comparing the effects of different hyperparameters or architectures\n",
    "    across multiple agents.\n",
    "\n",
    "    Parameters:\n",
    "    - agents (list): A list of DQNAgent instances to be evaluated.\n",
    "    - labels (list): A list of strings representing labels for each agent, used in the logs to identify the agents.\n",
    "    - n_runs (int): The number of runs to execute for each agent.\n",
    "    - max_steps (int): The maximum number of steps to execute in each run.\n",
    "    - show_progress (bool): Whether to display progress bars during execution.\n",
    "\n",
    "    Returns:\n",
    "    - pandas.DataFrame: A concatenated DataFrame containing the logs of all runs for all agents, with an additional\n",
    "                         'Agent' column indicating the label of the agent for each log entry.\n",
    "    \"\"\"\n",
    "    logs = dict()\n",
    "    for idx, agent in enumerate(tqdm(agents)):\n",
    "        engine = DQNEngine(env=agent.env, agent=agent,\n",
    "                           max_steps=max_steps, show_progress=show_progress)\n",
    "        ep_log = engine.run(n_runs)\n",
    "        ep_log = pd.concat(ep_log, ignore_index=True)\n",
    "        ep_log['Agent'] = labels[idx]\n",
    "        logs[f'{idx}'] = ep_log\n",
    "    logs = pd.concat(logs, ignore_index=True)\n",
    "    return logs\n",
    "\n",
    "def get_default_config():\n",
    "    config = dict(\n",
    "        learning_rate=0.00025,\n",
    "        gamma=0.99,\n",
    "        memory_size=200000,\n",
    "        initial_epsilon=1.0,\n",
    "        min_epsilon=0.1,\n",
    "        max_epsilon_decay_steps=150000,\n",
    "        warmup_steps=500,\n",
    "        target_update_freq=2000,\n",
    "        batch_size=32,\n",
    "        device=None,\n",
    "        disable_target_net=False,\n",
    "        enable_double_q=True\n",
    "    )\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RRR:\n",
    "    def __init__(self, ACModelClass, env, args:Config=None, seed=0):\n",
    "        \"\"\"\n",
    "        The RRR agent.\n",
    "        \"\"\"\n",
    "\n",
    "        self.env = env\n",
    "\n",
    "        set_random_seed(seed)\n",
    "        m1 = ACModelClass(use_critic=True)\n",
    "\n",
    "        if args is None:\n",
    "            args = Config()\n",
    "\n",
    "        self.exploiter = Machine(0.01, m1, args)\n",
    "\n",
    "        m2 = ACModelClass(use_critic=False)\n",
    "        m3 = ACModelClass(use_critic=False)\n",
    "        m4 = ACModelClass(use_critic=False)\n",
    "\n",
    "        self.explorer_random = Machine(0.03, m2, args)\n",
    "        self.explorer_thirsty = Machine(0.02, m3, args)\n",
    "        self.explorer_thirsty.copy_machine(self.exploiter)\n",
    "\n",
    "        self.temp_machine = Machine(0.01, m3, args)\n",
    "        self.explorer_bengbuzhu = Machine(0.5, m4, args)\n",
    "\n",
    "    def _replay(self, machine:Machine, buffer:list, cutoff:float) -> float:\n",
    "        \"\"\"\n",
    "        Replay random sample from buffer on machine.\n",
    "\n",
    "        Args:\n",
    "            machine: Machine to replay on\n",
    "            buffer: list of experiences\n",
    "            cutoff: if fit < cutoff, delete sb from buffer\n",
    "\n",
    "        Returns:\n",
    "            fit\n",
    "        \"\"\"\n",
    "\n",
    "        idx = np.random.randint(len(buffer))\n",
    "        sb = buffer[idx]\n",
    "        logs_replay = machine.update_parameters(sb)\n",
    "        fit = logs_replay['fit']\n",
    "        if fit < cutoff:\n",
    "            # delete sb from buffer\n",
    "            buffer.pop(idx)\n",
    "        \n",
    "        return fit\n",
    "    \n",
    "    def _add_sb_to_buffer(self, exps, buffer:list, capacity:int) -> None:\n",
    "        buffer.append(exps)\n",
    "        if len(buffer) > capacity:\n",
    "            buffer.pop(0)\n",
    "\n",
    "\n",
    "    def train(self, max_episodes:int=10000, nonstop:bool=False, max_frames=float('inf')) -> tuple[list[int], list[float], list[float]]:\n",
    "        \"\"\"\n",
    "        Train the agent.\n",
    "\n",
    "        Returns:\n",
    "            num_frames, smooth_rs, fits\n",
    "        \"\"\"\n",
    "\n",
    "        print('Start! Agent: RRR.')\n",
    "        RANDOM_MODE = 0\n",
    "        EXPLORE_MODE = 1\n",
    "        EXPLOIT_MODE = 2\n",
    "        mode = RANDOM_MODE\n",
    "\n",
    "        is_solved = False\n",
    "\n",
    "        SMOOTH_REWARD_WINDOW = 50\n",
    "\n",
    "        rewards = [0]*SMOOTH_REWARD_WINDOW\n",
    "\n",
    "        total_smooth_rs = []\n",
    "        total_num_frames = []\n",
    "        larger_buffer_r = []\n",
    "        buffer_r = []\n",
    "        buffer_no_r = []\n",
    "        fits = []\n",
    "\n",
    "        num_frames = 0\n",
    "\n",
    "        pbar = tqdm(range(max_episodes))\n",
    "        for update in pbar:\n",
    "            total_num_frames.append(num_frames)\n",
    "\n",
    "            if mode == RANDOM_MODE:\n",
    "                exps, logs1 = self.explorer_bengbuzhu.collect_experiences(self.env)\n",
    "                self.explorer_bengbuzhu.update_parameters(exps)\n",
    "                logs2 = self.exploiter.update_parameters(exps)\n",
    "            \n",
    "            elif mode == EXPLORE_MODE:\n",
    "                if np.random.rand() < 0.5:\n",
    "                    m = self.explorer_random\n",
    "                else:\n",
    "                    m = self.explorer_thirsty\n",
    "\n",
    "                exps, logs1 = m.collect_experiences(self.env)\n",
    "\n",
    "                m.update_parameters(exps)\n",
    "                logs2 = self.exploiter.update_parameters(exps)\n",
    "\n",
    "                if len(larger_buffer_r) >= 1:\n",
    "                    self._replay(self.explorer_thirsty, larger_buffer_r, cutoff=0.0)\n",
    "\n",
    "            elif mode == EXPLOIT_MODE:\n",
    "                exps, logs1 = self.exploiter.collect_experiences(self.env)\n",
    "\n",
    "                logs2 = self.exploiter.update_parameters(exps)\n",
    "\n",
    "                assert len(buffer_r) > 0, f'buffer_r should not be empty.'\n",
    "\n",
    "                cutoff = self.exploiter.args.bad_fit_threshold + self.exploiter.args.bad_fit_increment * (len(buffer_r) - 1)\n",
    "                \n",
    "                if not logs1['success'] and total_smooth_rs[-1] <= 0.5:\n",
    "                    fit = self._replay(self.exploiter, buffer_r, cutoff)\n",
    "                    fits.append(fit)\n",
    "\n",
    "                if len(buffer_r) == 0:\n",
    "                    print('empty')\n",
    "                    mode = EXPLORE_MODE\n",
    "                    self.explorer_thirsty.copy_machine(self.exploiter)\n",
    "                    self.explorer_random.copy_machine(self.temp_machine)\n",
    "                    self.temp_machine.copy_machine(self.exploiter)\n",
    "                \n",
    "            else:\n",
    "                raise ValueError(f'Invalid mode: {mode}')\n",
    "            \n",
    "            logs = {**logs1, **logs2}\n",
    "\n",
    "            num_frames += logs[\"num_frames\"]\n",
    "\n",
    "            rewards.append(logs[\"return_per_episode\"])\n",
    "\n",
    "            if logs['success']:\n",
    "                self._add_sb_to_buffer(exps, buffer_r, self.exploiter.args.replay_buffer_capacity)\n",
    "                self._add_sb_to_buffer(exps, larger_buffer_r, self.exploiter.args.large_buffer_capacity)\n",
    "                mode = EXPLOIT_MODE\n",
    "            else:\n",
    "                self._add_sb_to_buffer(exps, buffer_no_r, self.exploiter.args.replay_buffer_capacity)\n",
    "\n",
    "            smooth_reward = np.mean(rewards[-SMOOTH_REWARD_WINDOW:])\n",
    "\n",
    "            total_smooth_rs.append(smooth_reward)\n",
    "\n",
    "            data = {'num_frames':num_frames, 'smooth_reward':smooth_reward,\n",
    "                'reward':logs[\"return_per_episode\"], 'episode':update}\n",
    "\n",
    "            pbar.set_postfix(data)\n",
    "\n",
    "            if not nonstop and smooth_reward >= self.exploiter.args.score_threshold:\n",
    "                    is_solved = True\n",
    "                    break\n",
    "            if num_frames >= max_frames:\n",
    "                break\n",
    "    \n",
    "        if is_solved:\n",
    "            print('Solved!')\n",
    "\n",
    "        return total_num_frames, total_smooth_rs, fits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACModel(nn.Module):\n",
    "    def __init__(self, use_critic=False):\n",
    "        \"\"\"\n",
    "        Represents an Actor Crictic model that takes a 2d, multi-channeled\n",
    "        image as input.\n",
    "\n",
    "        Parameters\n",
    "        ----\n",
    "        num_actions : int\n",
    "\n",
    "                      The action space of the environment.\n",
    "                      The action space for DoorKey5x5 is 7-dimensional:\n",
    "                      0: turn left,\n",
    "                      1: turn right,\n",
    "                      2: forward,\n",
    "                      3: pickup an object,\n",
    "                      4: drop an object,\n",
    "                      5: activate an object,\n",
    "                      6: done completing task\n",
    "\n",
    "        use_critics : bool\n",
    "\n",
    "                      Critic network will be used in forward pass if flag is set\n",
    "                      to true.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.use_critic = use_critic\n",
    "\n",
    "        # Define actor's model\n",
    "        self.image_conv_actor = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, (2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 2)),\n",
    "            nn.Conv2d(16, 32, (2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, (2, 2)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 7)\n",
    "        )\n",
    "\n",
    "        # Define critic's model\n",
    "        if self.use_critic:\n",
    "            self.image_conv_critic = nn.Sequential(\n",
    "                nn.Conv2d(3, 16, (2, 2)),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d((2, 2)),\n",
    "                nn.Conv2d(16, 32, (2, 2)),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(32, 64, (2, 2)),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "            self.critic = nn.Sequential(\n",
    "                nn.Linear(64, 64),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(64, 1)\n",
    "            )\n",
    "\n",
    "        # Initialize parameters correctly\n",
    "        self.apply(init_params)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        \"\"\"\n",
    "        Performs a forward pass through the actor-critic network\n",
    "\n",
    "        Parameters\n",
    "        ----\n",
    "        obs : int tensor. Shape [Batch size, ImWidth, ImHeight, Channels]\n",
    "\n",
    "              input to the network.\n",
    "        ----\n",
    "\n",
    "        returns:\n",
    "\n",
    "        dist : torch.distribution\n",
    "            The distribution of actions from policy. A Categorical distribution\n",
    "            for discreet action spaces.\n",
    "        value : torch.Tensor (Batch size, 1)\n",
    "            value output by critic network\n",
    "        \"\"\"\n",
    "        obs = torch.tensor(obs).float() # convert to float tensor\n",
    "        if len(obs.shape) == 3:\n",
    "            obs = obs.unsqueeze(0) # add batch dimension if not already there\n",
    "            \n",
    "        conv_in = obs.transpose(1, 3).transpose(2, 3) # reshape into [b, c, h, w]\n",
    "\n",
    "        dist, value = None, None\n",
    "\n",
    "        x = self.image_conv_actor(conv_in)\n",
    "        embedding = x.reshape(x.shape[0], -1)\n",
    "\n",
    "        x = self.actor(embedding)\n",
    "        dist = Categorical(logits=F.log_softmax(x, dim=1))\n",
    "\n",
    "        if self.use_critic:\n",
    "            y = self.image_conv_critic(conv_in)\n",
    "            embedding = y.reshape(y.shape[0], -1)\n",
    "\n",
    "            value = self.critic(embedding).squeeze(1)\n",
    "        else:\n",
    "            value = torch.zeros((x.shape[0], 1), device=x.device)\n",
    "        \n",
    "        return dist, value\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "size = 8\n",
    "seed = 42\n",
    "\n",
    "env = get_door_key_env(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de6cf2ae9d9a4a40a19cff5bcc780bfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c24694b08e4945879f2e19be08238c75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Runs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2491bb387a7f4c61978acbf5fb5b99ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/500000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start! Agent: PPO.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28399070bce8429ea31433d1b1c55497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8m/r3kmm3xs1lngf9bhwg216mlc0000gn/T/ipykernel_39228/2068944089.py:84: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  obs = torch.tensor(obs).float() # convert to float tensor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start! Agent: RRR.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0525217133242389fc8bfb09df24b55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empty\n",
      "Solved!\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_frames = 500000\n",
    "max_episodes = 3000000\n",
    "nonstop = False\n",
    "set_random_seed(seed)\n",
    "\n",
    "config = get_default_config()\n",
    "config['max_epsilon_decay_steps'] = max_frames * 1.5\n",
    "agent = DQNAgent(env=env, **config)\n",
    "\n",
    "dqn_logs = dqn_sweep([agent], ['DDQN'], max_steps=max_frames)\n",
    "for label, df in dqn_logs.groupby('Agent'):\n",
    "    pass\n",
    "\n",
    "num_frames_DDQN = df['steps'].to_list()\n",
    "rewards_DDQN = df['return'].to_list()\n",
    "# first prepend 4 zeros to the rewards to make the plot start at 0\n",
    "rewards_DDQN = [0] * 4 + rewards_DDQN\n",
    "# smooth the rewards by averaging on 5 of them\n",
    "rewards_DDQN = [sum(rewards_DDQN[i:i+5])/5 for i in range(len(rewards_DDQN)-4)]\n",
    "\n",
    "agent_PPO = PPO(ACModel, env=env, args=Config(), seed=seed)\n",
    "num_frames_1, smooth_rs_1 = agent_PPO.train(max_episodes, nonstop=nonstop, max_frames=max_frames)\n",
    "agent_RRR_2 = RRR(ACModel, env=env, args=Config(bad_fit_threshold=0.77, importance_sampling_clip=2.0), seed=seed)\n",
    "num_frames_2, smooth_rs_2, fits_2 = agent_RRR_2.train(max_episodes, nonstop=nonstop, max_frames=max_frames)\n",
    "\n",
    "plt.plot(num_frames_1, smooth_rs_1, label='PPO')\n",
    "plt.plot(num_frames_DDQN, rewards_DDQN, label='DDQN')\n",
    "plt.plot(num_frames_2, smooth_rs_2, label='RRR')\n",
    "plt.xlabel('Time steps')\n",
    "plt.ylabel('Average reward (smoothed)')\n",
    "plt.title(f'DoorKeyEnv, size = {size}, seed = {seed}')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start! Agent: PPO.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e48e43532a024926bd0f5d5dc025e1cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8m/r3kmm3xs1lngf9bhwg216mlc0000gn/T/ipykernel_39228/2068944089.py:84: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  obs = torch.tensor(obs).float() # convert to float tensor\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/8m/r3kmm3xs1lngf9bhwg216mlc0000gn/T/ipykernel_39228/1437258175.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0magent_PPO\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPPO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mACModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mnum_frames_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth_rs_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent_PPO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnonstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_frames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0magent_RRR_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRRR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mACModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbad_fit_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.77\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimportance_sampling_clip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mnum_frames_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth_rs_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfits_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent_RRR_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnonstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_frames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/8m/r3kmm3xs1lngf9bhwg216mlc0000gn/T/ipykernel_39228/287862128.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, max_episodes, nonstop, max_frames)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mupdate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mexps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmachine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_experiences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mlogs2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmachine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/8m/r3kmm3xs1lngf9bhwg216mlc0000gn/T/ipykernel_39228/3684782419.py\u001b[0m in \u001b[0;36mcollect_experiences\u001b[0;34m(self, env)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;31m# Do one agent-environment interaction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                 \u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mdist\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/8m/r3kmm3xs1lngf9bhwg216mlc0000gn/T/ipykernel_39228/2068944089.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_conv_actor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "seed = 0\n",
    "max_episodes = 3000000\n",
    "\n",
    "agent_PPO = PPO(ACModel, env=env, args=Config(), seed=seed)\n",
    "num_frames_1, smooth_rs_1 = agent_PPO.train(max_episodes, nonstop=True, max_frames=100000)\n",
    "agent_RRR_2 = RRR(ACModel, env=env, args=Config(bad_fit_threshold=0.77, importance_sampling_clip=2.0), seed=seed)\n",
    "num_frames_2, smooth_rs_2, fits_2 = agent_RRR_2.train(max_episodes, nonstop=True, max_frames=100000)\n",
    "\n",
    "plt.plot(num_frames_1, smooth_rs_1, label='PPO')\n",
    "plt.plot(num_frames_2, smooth_rs_2, label='RRR, t = 0.77, c = 2.0')\n",
    "plt.xlabel('Time steps')\n",
    "plt.ylabel('Average reward (smoothed)')\n",
    "plt.title(f'DoorKeyEnv, size = {size}, seed = {seed}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 0.8679245114326477,\n",
       " 0.8301886916160583,\n",
       " 0.8820754885673523,\n",
       " 0.9433962106704712,\n",
       " 0.8726415038108826,\n",
       " 0.7641509175300598,\n",
       " 0.75,\n",
       " 0.7971698045730591,\n",
       " 0.7735849022865295,\n",
       " 0.7547169923782349,\n",
       " 0.7264150977134705,\n",
       " 0.7264150977134705,\n",
       " 0.7311320900917053,\n",
       " 0.8207547068595886,\n",
       " 0.7783018946647644,\n",
       " 0.7547169923782349,\n",
       " 0.7547169923782349,\n",
       " 0.7877358198165894,\n",
       " 0.7877358198165894,\n",
       " 0.7735849022865295,\n",
       " 0.7311320900917053,\n",
       " 0.7594339847564697,\n",
       " 0.8066037893295288,\n",
       " 0.7122641801834106,\n",
       " 0.7547169923782349,\n",
       " 0.8254716992378235,\n",
       " 0.7641509175300598,\n",
       " 0.75,\n",
       " 0.8066037893295288,\n",
       " 0.7547169923782349,\n",
       " 0.7971698045730591,\n",
       " 0.8113207817077637,\n",
       " 0.7783018946647644,\n",
       " 0.7641509175300598,\n",
       " 0.7594339847564697,\n",
       " 0.7547169923782349,\n",
       " 0.7877358198165894,\n",
       " 0.75,\n",
       " 0.7122641801834106,\n",
       " 0.7216981053352356,\n",
       " 0.7783018946647644,\n",
       " 0.7688679099082947,\n",
       " 0.7594339847564697,\n",
       " 0.7830188870429993,\n",
       " 0.7735849022865295,\n",
       " 0.8113207817077637,\n",
       " 0.7547169923782349,\n",
       " 0.7452830076217651,\n",
       " 0.8066037893295288,\n",
       " 0.75,\n",
       " 0.7830188870429993,\n",
       " 0.801886796951294,\n",
       " 0.7311320900917053,\n",
       " 0.7405660152435303,\n",
       " 0.7877358198165894,\n",
       " 0.7311320900917053,\n",
       " 0.7688679099082947,\n",
       " 0.7735849022865295,\n",
       " 0.7877358198165894,\n",
       " 0.7688679099082947,\n",
       " 0.7405660152435303,\n",
       " 0.801886796951294,\n",
       " 0.8207547068595886,\n",
       " 0.8066037893295288,\n",
       " 0.7688679099082947,\n",
       " 0.7877358198165894,\n",
       " 0.7783018946647644,\n",
       " 0.75,\n",
       " 0.7264150977134705,\n",
       " 0.7547169923782349,\n",
       " 0.7783018946647644,\n",
       " 0.7594339847564697,\n",
       " 0.7735849022865295,\n",
       " 0.7783018946647644,\n",
       " 0.7358490824699402,\n",
       " 0.7688679099082947,\n",
       " 0.7547169923782349,\n",
       " 0.7735849022865295,\n",
       " 0.7452830076217651,\n",
       " 0.7311320900917053,\n",
       " 0.7594339847564697,\n",
       " 0.7924528121948242,\n",
       " 0.7924528121948242,\n",
       " 0.7830188870429993,\n",
       " 0.7877358198165894,\n",
       " 0.7783018946647644,\n",
       " 0.8301886916160583,\n",
       " 0.801886796951294,\n",
       " 0.7877358198165894,\n",
       " 0.8254716992378235,\n",
       " 0.7735849022865295,\n",
       " 0.801886796951294,\n",
       " 0.8349056839942932,\n",
       " 0.8254716992378235,\n",
       " 0.7877358198165894,\n",
       " 0.7641509175300598,\n",
       " 0.8160377144813538,\n",
       " 0.7641509175300598,\n",
       " 0.7688679099082947,\n",
       " 0.7688679099082947,\n",
       " 0.8066037893295288,\n",
       " 0.7830188870429993,\n",
       " 0.7830188870429993,\n",
       " 0.7924528121948242,\n",
       " 0.801886796951294,\n",
       " 0.7735849022865295,\n",
       " 0.7830188870429993,\n",
       " 0.7877358198165894,\n",
       " 0.7971698045730591,\n",
       " 0.7594339847564697,\n",
       " 0.7594339847564697,\n",
       " 0.7311320900917053,\n",
       " 0.7547169923782349,\n",
       " 0.7452830076217651,\n",
       " 0.7405660152435303,\n",
       " 0.7311320900917053,\n",
       " 0.7688679099082947,\n",
       " 0.7547169923782349,\n",
       " 0.7594339847564697,\n",
       " 0.7830188870429993,\n",
       " 0.7688679099082947,\n",
       " 0.7547169923782349,\n",
       " 0.7452830076217651,\n",
       " 0.7783018946647644,\n",
       " 0.7641509175300598,\n",
       " 0.7594339847564697,\n",
       " 0.7688679099082947,\n",
       " 0.7594339847564697,\n",
       " 0.7594339847564697,\n",
       " 0.7735849022865295,\n",
       " 0.7641509175300598,\n",
       " 0.7830188870429993,\n",
       " 0.7405660152435303,\n",
       " 0.7735849022865295,\n",
       " 0.7735849022865295,\n",
       " 0.7783018946647644,\n",
       " 0.8254716992378235,\n",
       " 0.801886796951294,\n",
       " 0.7783018946647644,\n",
       " 0.7688679099082947,\n",
       " 0.7971698045730591,\n",
       " 0.8160377144813538,\n",
       " 0.7877358198165894,\n",
       " 0.8160377144813538,\n",
       " 0.801886796951294,\n",
       " 0.7877358198165894,\n",
       " 0.7830188870429993,\n",
       " 0.8207547068595886,\n",
       " 0.7924528121948242,\n",
       " 0.7971698045730591,\n",
       " 0.8066037893295288,\n",
       " 0.801886796951294,\n",
       " 0.7971698045730591,\n",
       " 0.7877358198165894,\n",
       " 0.8254716992378235,\n",
       " 0.7735849022865295,\n",
       " 0.7594339847564697,\n",
       " 0.7924528121948242,\n",
       " 0.75,\n",
       " 0.7783018946647644,\n",
       " 0.7783018946647644,\n",
       " 0.7924528121948242,\n",
       " 0.801886796951294,\n",
       " 0.7830188870429993,\n",
       " 0.7924528121948242,\n",
       " 0.7641509175300598,\n",
       " 0.7735849022865295,\n",
       " 0.7783018946647644,\n",
       " 0.7971698045730591,\n",
       " 0.7924528121948242,\n",
       " 0.7830188870429993,\n",
       " 0.7735849022865295,\n",
       " 0.8254716992378235,\n",
       " 0.8443396091461182,\n",
       " 0.849056601524353,\n",
       " 0.7971698045730591,\n",
       " 0.8160377144813538,\n",
       " 0.8160377144813538,\n",
       " 0.7971698045730591,\n",
       " 0.8396226167678833,\n",
       " 0.8207547068595886,\n",
       " 0.849056601524353,\n",
       " 0.8301886916160583,\n",
       " 0.8301886916160583,\n",
       " 0.8207547068595886,\n",
       " 0.7783018946647644,\n",
       " 0.7688679099082947,\n",
       " 0.7877358198165894,\n",
       " 0.7783018946647644,\n",
       " 0.8066037893295288,\n",
       " 0.8301886916160583,\n",
       " 0.8632075190544128,\n",
       " 0.7924528121948242,\n",
       " 0.7830188870429993,\n",
       " 0.7830188870429993,\n",
       " 0.8396226167678833,\n",
       " 0.8396226167678833,\n",
       " 0.8349056839942932,\n",
       " 0.8679245114326477,\n",
       " 0.8301886916160583,\n",
       " 0.8349056839942932,\n",
       " 0.8066037893295288,\n",
       " 0.8301886916160583,\n",
       " 0.849056601524353,\n",
       " 0.8066037893295288,\n",
       " 0.8349056839942932,\n",
       " 0.8160377144813538,\n",
       " 0.8207547068595886,\n",
       " 0.8207547068595886,\n",
       " 0.8113207817077637,\n",
       " 0.8443396091461182,\n",
       " 0.8066037893295288,\n",
       " 0.8113207817077637,\n",
       " 0.849056601524353,\n",
       " 0.8443396091461182,\n",
       " 0.8254716992378235,\n",
       " 0.8349056839942932,\n",
       " 0.8113207817077637,\n",
       " 0.801886796951294,\n",
       " 0.8349056839942932,\n",
       " 0.8254716992378235,\n",
       " 0.7971698045730591,\n",
       " 0.8396226167678833,\n",
       " 0.8113207817077637,\n",
       " 0.8301886916160583,\n",
       " 0.7877358198165894,\n",
       " 0.7877358198165894,\n",
       " 0.8443396091461182,\n",
       " 0.7924528121948242,\n",
       " 0.8396226167678833,\n",
       " 0.8301886916160583,\n",
       " 0.8301886916160583,\n",
       " 0.8349056839942932,\n",
       " 0.8207547068595886,\n",
       " 0.8443396091461182,\n",
       " 0.8113207817077637,\n",
       " 0.7924528121948242,\n",
       " 0.7830188870429993,\n",
       " 0.801886796951294,\n",
       " 0.7924528121948242,\n",
       " 0.801886796951294,\n",
       " 0.8254716992378235,\n",
       " 0.8207547068595886,\n",
       " 0.7971698045730591,\n",
       " 0.8349056839942932,\n",
       " 0.7971698045730591,\n",
       " 0.8584905862808228,\n",
       " 0.8301886916160583,\n",
       " 0.8396226167678833,\n",
       " 0.8160377144813538,\n",
       " 0.8584905862808228,\n",
       " 0.801886796951294,\n",
       " 0.7830188870429993,\n",
       " 0.7971698045730591,\n",
       " 0.8160377144813538,\n",
       " 0.8396226167678833,\n",
       " 0.8160377144813538,\n",
       " 0.8254716992378235,\n",
       " 0.7877358198165894,\n",
       " 0.8301886916160583,\n",
       " 0.8349056839942932,\n",
       " 0.8349056839942932,\n",
       " 0.7688679099082947,\n",
       " 0.75,\n",
       " 0.8066037893295288,\n",
       " 0.7830188870429993,\n",
       " 0.7877358198165894,\n",
       " 0.7924528121948242,\n",
       " 0.8207547068595886,\n",
       " 0.8301886916160583,\n",
       " 0.7971698045730591,\n",
       " 0.801886796951294,\n",
       " 0.8349056839942932,\n",
       " 0.8396226167678833,\n",
       " 0.7971698045730591,\n",
       " 0.8113207817077637,\n",
       " 0.8113207817077637,\n",
       " 0.8301886916160583,\n",
       " 0.8160377144813538,\n",
       " 0.8349056839942932,\n",
       " 0.8349056839942932,\n",
       " 0.7971698045730591,\n",
       " 0.7783018946647644,\n",
       " 0.7783018946647644,\n",
       " 0.8254716992378235,\n",
       " 0.8254716992378235,\n",
       " 0.7924528121948242,\n",
       " 0.7924528121948242,\n",
       " 0.7830188870429993,\n",
       " 0.8207547068595886,\n",
       " 0.8254716992378235,\n",
       " 0.7688679099082947,\n",
       " 0.7924528121948242,\n",
       " 0.7924528121948242,\n",
       " 0.7783018946647644,\n",
       " 0.7594339847564697,\n",
       " 0.8207547068595886,\n",
       " 0.7877358198165894,\n",
       " 0.8066037893295288,\n",
       " 0.8349056839942932,\n",
       " 0.8726415038108826,\n",
       " 0.8066037893295288,\n",
       " 0.7971698045730591,\n",
       " 0.8349056839942932,\n",
       " 0.849056601524353,\n",
       " 0.8396226167678833,\n",
       " 0.8537735939025879,\n",
       " 0.849056601524353,\n",
       " 0.8254716992378235,\n",
       " 0.8113207817077637,\n",
       " 0.8632075190544128,\n",
       " 0.8160377144813538,\n",
       " 0.8113207817077637,\n",
       " 0.8396226167678833,\n",
       " 0.8254716992378235,\n",
       " 0.8160377144813538,\n",
       " 0.8349056839942932,\n",
       " 0.8349056839942932,\n",
       " 0.8537735939025879,\n",
       " 0.849056601524353,\n",
       " 0.8584905862808228,\n",
       " 0.849056601524353,\n",
       " 0.8396226167678833,\n",
       " 0.8396226167678833,\n",
       " 0.7971698045730591,\n",
       " 0.8443396091461182,\n",
       " 0.8254716992378235,\n",
       " 0.7971698045730591,\n",
       " 0.8301886916160583,\n",
       " 0.849056601524353,\n",
       " 0.8207547068595886,\n",
       " 0.7971698045730591,\n",
       " 0.801886796951294,\n",
       " 0.8584905862808228,\n",
       " 0.8537735939025879,\n",
       " 0.8443396091461182,\n",
       " 0.8207547068595886,\n",
       " 0.849056601524353,\n",
       " 0.8632075190544128,\n",
       " 0.8254716992378235,\n",
       " 0.8301886916160583,\n",
       " 0.8160377144813538,\n",
       " 0.8537735939025879,\n",
       " 0.8160377144813538,\n",
       " 0.8254716992378235,\n",
       " 0.7877358198165894,\n",
       " 0.8254716992378235,\n",
       " 0.7971698045730591,\n",
       " 0.8207547068595886,\n",
       " 0.8066037893295288,\n",
       " 0.8113207817077637,\n",
       " 0.801886796951294,\n",
       " 0.8066037893295288,\n",
       " 0.7971698045730591,\n",
       " 0.8113207817077637,\n",
       " 0.849056601524353,\n",
       " 0.8301886916160583,\n",
       " 0.8160377144813538,\n",
       " 0.8207547068595886,\n",
       " 0.8396226167678833,\n",
       " 0.8349056839942932,\n",
       " 0.8207547068595886,\n",
       " 0.7877358198165894,\n",
       " 0.8113207817077637,\n",
       " 0.8160377144813538,\n",
       " 0.7783018946647644,\n",
       " 0.8113207817077637,\n",
       " 0.7783018946647644,\n",
       " 0.8113207817077637,\n",
       " 0.8066037893295288,\n",
       " 0.7971698045730591,\n",
       " 0.801886796951294,\n",
       " 0.7877358198165894,\n",
       " 0.7688679099082947,\n",
       " 0.7735849022865295,\n",
       " 0.7924528121948242,\n",
       " 0.75,\n",
       " 0.7783018946647644,\n",
       " 0.7735849022865295,\n",
       " 0.801886796951294,\n",
       " 0.7783018946647644,\n",
       " 0.7830188870429993,\n",
       " 0.7877358198165894,\n",
       " 0.7783018946647644,\n",
       " 0.7971698045730591,\n",
       " 0.7735849022865295,\n",
       " 0.7735849022865295,\n",
       " 0.7924528121948242,\n",
       " 0.8066037893295288,\n",
       " 0.7735849022865295,\n",
       " 0.7783018946647644,\n",
       " 0.7877358198165894,\n",
       " 0.7594339847564697,\n",
       " 0.7688679099082947,\n",
       " 0.7594339847564697,\n",
       " 0.7877358198165894,\n",
       " 0.8301886916160583,\n",
       " 0.7688679099082947,\n",
       " 0.7877358198165894,\n",
       " 0.801886796951294,\n",
       " 0.7877358198165894,\n",
       " 0.8113207817077637,\n",
       " 0.7924528121948242,\n",
       " 0.8113207817077637,\n",
       " 0.8301886916160583,\n",
       " 0.8160377144813538,\n",
       " 0.7783018946647644,\n",
       " 0.75,\n",
       " 0.7877358198165894,\n",
       " 0.7641509175300598,\n",
       " 0.7783018946647644,\n",
       " 0.7924528121948242,\n",
       " 0.8443396091461182,\n",
       " 0.7877358198165894,\n",
       " 0.7735849022865295,\n",
       " 0.7830188870429993,\n",
       " 0.7924528121948242,\n",
       " 0.7547169923782349,\n",
       " 0.7594339847564697,\n",
       " 0.7783018946647644,\n",
       " 0.7688679099082947,\n",
       " 0.7971698045730591,\n",
       " 0.75,\n",
       " 0.7830188870429993,\n",
       " 0.7877358198165894,\n",
       " 0.8113207817077637,\n",
       " 0.7830188870429993,\n",
       " 0.8160377144813538,\n",
       " 0.7924528121948242,\n",
       " 0.7688679099082947,\n",
       " 0.7735849022865295,\n",
       " 0.7735849022865295,\n",
       " 0.7830188870429993,\n",
       " 0.7830188870429993,\n",
       " 0.7783018946647644,\n",
       " 0.7735849022865295,\n",
       " 0.7452830076217651,\n",
       " 0.7924528121948242,\n",
       " 0.8113207817077637,\n",
       " 0.849056601524353,\n",
       " 0.7830188870429993,\n",
       " 0.801886796951294,\n",
       " 0.7688679099082947,\n",
       " 0.8066037893295288,\n",
       " 0.7641509175300598,\n",
       " 0.7830188870429993,\n",
       " 0.7877358198165894,\n",
       " 0.8160377144813538,\n",
       " 0.8207547068595886,\n",
       " 0.7924528121948242,\n",
       " 0.8632075190544128,\n",
       " 0.7877358198165894,\n",
       " 0.801886796951294,\n",
       " 0.7877358198165894,\n",
       " 0.7971698045730591,\n",
       " 0.7735849022865295,\n",
       " 0.7688679099082947,\n",
       " 0.7688679099082947,\n",
       " 0.8113207817077637,\n",
       " 0.7971698045730591,\n",
       " 0.7783018946647644,\n",
       " 0.8066037893295288,\n",
       " 0.8160377144813538,\n",
       " 0.8113207817077637,\n",
       " 0.7971698045730591,\n",
       " 0.8113207817077637,\n",
       " 0.8160377144813538,\n",
       " 0.8066037893295288,\n",
       " 0.7452830076217651,\n",
       " 0.7547169923782349,\n",
       " 0.8066037893295288,\n",
       " 0.7547169923782349,\n",
       " 0.75,\n",
       " 0.7877358198165894,\n",
       " 0.7971698045730591,\n",
       " 0.7688679099082947,\n",
       " 0.7452830076217651,\n",
       " 0.7783018946647644,\n",
       " 0.8113207817077637,\n",
       " 0.7594339847564697,\n",
       " 0.801886796951294,\n",
       " 0.7735849022865295,\n",
       " 0.8160377144813538,\n",
       " 0.8113207817077637,\n",
       " 0.7971698045730591,\n",
       " 0.7877358198165894,\n",
       " 0.8301886916160583,\n",
       " 0.8207547068595886,\n",
       " 0.849056601524353,\n",
       " 0.8066037893295288,\n",
       " 0.8113207817077637,\n",
       " 0.7877358198165894,\n",
       " 0.7877358198165894,\n",
       " 0.7688679099082947,\n",
       " 0.801886796951294,\n",
       " 0.8349056839942932,\n",
       " 0.8254716992378235,\n",
       " 0.8349056839942932,\n",
       " 0.8066037893295288,\n",
       " 0.8113207817077637,\n",
       " 0.801886796951294,\n",
       " 0.7924528121948242,\n",
       " 0.8254716992378235,\n",
       " 0.7924528121948242,\n",
       " 0.8160377144813538,\n",
       " 0.7735849022865295,\n",
       " 0.8207547068595886,\n",
       " 0.8066037893295288,\n",
       " 0.7783018946647644,\n",
       " 0.7971698045730591,\n",
       " 0.7830188870429993,\n",
       " 0.7594339847564697,\n",
       " 0.7877358198165894,\n",
       " 0.7924528121948242,\n",
       " 0.9957982897758484,\n",
       " 0.7641509175300598,\n",
       " 0.7971698045730591,\n",
       " 0.7830188870429993,\n",
       " 0.9075630307197571,\n",
       " 0.7877358198165894,\n",
       " 0.9075630307197571,\n",
       " 0.8781512379646301,\n",
       " 0.7877358198165894,\n",
       " 0.8537735939025879,\n",
       " 0.7877358198165894,\n",
       " 0.7971698045730591,\n",
       " 0.8529411554336548,\n",
       " 0.8781512379646301,\n",
       " 0.8613445162773132,\n",
       " 0.8655462265014648,\n",
       " 0.819327712059021,\n",
       " 0.831932783126831,\n",
       " 0.7641509175300598,\n",
       " 0.7547169923782349,\n",
       " 0.7688679099082947,\n",
       " 0.8529411554336548,\n",
       " 0.7594339847564697,\n",
       " 0.8529411554336548,\n",
       " 0.7830188870429993,\n",
       " 0.7877358198165894,\n",
       " 0.8235294222831726,\n",
       " 0.75,\n",
       " 0.8361344337463379,\n",
       " 0.8361344337463379,\n",
       " 0.7877358198165894,\n",
       " 0.7924528121948242,\n",
       " 0.831932783126831,\n",
       " 0.8109243512153625,\n",
       " 0.7924528121948242,\n",
       " 0.8067227005958557,\n",
       " 0.7735849022865295,\n",
       " 0.8254716992378235,\n",
       " 0.8254716992378235,\n",
       " 0.8066037893295288,\n",
       " 0.7971698045730591,\n",
       " 0.7877358198165894,\n",
       " 0.8823529481887817,\n",
       " 0.8445377945899963,\n",
       " 0.831932783126831,\n",
       " 0.8403361439704895,\n",
       " 0.8025209903717041,\n",
       " 0.7877358198165894,\n",
       " 0.831932783126831,\n",
       " 0.8066037893295288,\n",
       " 0.819327712059021,\n",
       " 0.8067227005958557,\n",
       " 0.7971698045730591,\n",
       " 0.7641509175300598,\n",
       " 0.7452830076217651,\n",
       " 0.7452830076217651,\n",
       " 0.7358490824699402,\n",
       " 0.7547169923782349,\n",
       " 0.7641509175300598,\n",
       " 0.8445377945899963,\n",
       " 0.8277310729026794,\n",
       " 0.8067227005958557,\n",
       " 0.7641509175300598,\n",
       " 0.8067227005958557,\n",
       " 0.7594339847564697,\n",
       " 0.831932783126831,\n",
       " 0.7783018946647644,\n",
       " 0.8235294222831726,\n",
       " 0.7877358198165894,\n",
       " 0.7547169923782349,\n",
       " 0.8254716992378235,\n",
       " 0.7783018946647644,\n",
       " 0.7924528121948242,\n",
       " 0.7877358198165894,\n",
       " 0.8113207817077637,\n",
       " 0.8151260614395142,\n",
       " 0.8235294222831726,\n",
       " 0.801886796951294,\n",
       " 0.8151260614395142,\n",
       " 0.8066037893295288,\n",
       " 0.7783018946647644,\n",
       " 0.7877358198165894,\n",
       " 0.75,\n",
       " 0.8361344337463379,\n",
       " 0.8025209903717041,\n",
       " 0.7773109078407288,\n",
       " 0.8066037893295288,\n",
       " 0.8254716992378235,\n",
       " 0.8025209903717041,\n",
       " 0.8067227005958557,\n",
       " 0.7773109078407288,\n",
       " 0.7857142686843872,\n",
       " 0.8066037893295288,\n",
       " 0.756302535533905,\n",
       " 0.7971698045730591,\n",
       " 0.7647058963775635,\n",
       " 0.7830188870429993,\n",
       " 0.7815126180648804,\n",
       " 0.8207547068595886,\n",
       " 0.7983193397521973,\n",
       " 0.7688679099082947,\n",
       " 0.7830188870429993,\n",
       " 0.7830188870429993,\n",
       " 0.7971698045730591,\n",
       " 0.7877358198165894,\n",
       " 0.7941176295280457,\n",
       " 0.7899159789085388,\n",
       " 0.8025209903717041,\n",
       " 0.8066037893295288,\n",
       " 0.801886796951294,\n",
       " 0.8235294222831726,\n",
       " 0.7924528121948242,\n",
       " 0.7983193397521973,\n",
       " 0.7924528121948242,\n",
       " 0.7783018946647644,\n",
       " 0.7941176295280457,\n",
       " 0.7689075469970703,\n",
       " 0.7773109078407288,\n",
       " 0.7605041861534119,\n",
       " 0.8254716992378235,\n",
       " 0.7971698045730591,\n",
       " 0.7452830076217651,\n",
       " 0.7547169923782349,\n",
       " 0.7830188870429993,\n",
       " 0.8151260614395142,\n",
       " 0.7877358198165894,\n",
       " 0.7688679099082947,\n",
       " 0.7899159789085388,\n",
       " 0.7735849022865295,\n",
       " 0.8066037893295288,\n",
       " 0.7688679099082947,\n",
       " 0.7605041861534119,\n",
       " 0.8025209903717041,\n",
       " 0.8109243512153625,\n",
       " 0.7731092572212219,\n",
       " 0.7521008253097534,\n",
       " 0.7773109078407288,\n",
       " 0.7877358198165894,\n",
       " 0.8254716992378235,\n",
       " 0.7899159789085388,\n",
       " 0.7731092572212219,\n",
       " 0.7783018946647644,\n",
       " 0.7731092572212219,\n",
       " 0.8066037893295288,\n",
       " 0.756302535533905,\n",
       " 0.7641509175300598,\n",
       " 0.7857142686843872,\n",
       " 0.8207547068595886,\n",
       " 0.7877358198165894,\n",
       " 0.7735849022865295,\n",
       " 0.8361344337463379,\n",
       " 0.8113207817077637,\n",
       " 0.7689075469970703,\n",
       " 0.7689075469970703,\n",
       " 0.7647058963775635,\n",
       " 0.8066037893295288,\n",
       " 0.7924528121948242,\n",
       " 0.7815126180648804,\n",
       " 0.7783018946647644,\n",
       " 0.9671052694320679,\n",
       " 0.743697464466095,\n",
       " 0.8881579041481018,\n",
       " 0.7857142686843872,\n",
       " 0.756302535533905,\n",
       " 0.7647058963775635,\n",
       " 0.7773109078407288,\n",
       " 0.801886796951294,\n",
       " 0.9013158082962036,\n",
       " 0.7899159789085388,\n",
       " 0.8025209903717041,\n",
       " 0.831932783126831,\n",
       " 0.8349056839942932,\n",
       " 0.8207547068595886,\n",
       " 0.9473684430122375,\n",
       " 0.9210526347160339,\n",
       " 0.9210526347160339,\n",
       " 0.8025209903717041,\n",
       " 0.8815789222717285,\n",
       " 0.8445377945899963,\n",
       " 0.7924528121948242,\n",
       " 0.8947368264198303,\n",
       " 0.9078947305679321,\n",
       " 0.8486841917037964,\n",
       " 0.831932783126831,\n",
       " 0.8113207817077637,\n",
       " 0.8618420958518982,\n",
       " 0.7830188870429993,\n",
       " 0.95333331823349,\n",
       " 0.8815789222717285,\n",
       " 0.875,\n",
       " 0.8618420958518982,\n",
       " 0.7828947305679321,\n",
       " 0.8355262875556946,\n",
       " 0.831932783126831,\n",
       " 0.9133333563804626,\n",
       " 0.8109243512153625,\n",
       " 0.8349056839942932,\n",
       " 0.8999999761581421,\n",
       " 0.831932783126831,\n",
       " 0.8109243512153625,\n",
       " 0.7941176295280457,\n",
       " 0.7924528121948242,\n",
       " 0.8151260614395142,\n",
       " 0.8277310729026794,\n",
       " 0.801886796951294,\n",
       " 0.8881579041481018,\n",
       " 0.8846153616905212,\n",
       " 0.8025209903717041,\n",
       " 0.8733333349227905,\n",
       " 0.7971698045730591,\n",
       " 0.8552631735801697,\n",
       " 0.8403361439704895,\n",
       " 0.8780487775802612,\n",
       " 0.807692289352417,\n",
       " 0.8866666555404663,\n",
       " 0.8600000143051147,\n",
       " 0.8947368264198303,\n",
       " 0.8223684430122375,\n",
       " 0.8461538553237915,\n",
       " 0.8536585569381714,\n",
       " 0.8066666722297668,\n",
       " 0.8292682766914368,\n",
       " 0.8109243512153625,\n",
       " 0.807692289352417,\n",
       " 0.8684210777282715,\n",
       " 0.8292682766914368,\n",
       " 0.7692307829856873,\n",
       " 0.8421052694320679,\n",
       " 0.8157894611358643,\n",
       " 0.7933333516120911,\n",
       " 0.9295774698257446,\n",
       " 0.9238095283508301,\n",
       " 0.9238095283508301,\n",
       " 0.8484848737716675,\n",
       " 0.7804877758026123,\n",
       " 0.8333333134651184,\n",
       " 0.8881579041481018,\n",
       " 0.8684210777282715,\n",
       " 0.9142857193946838,\n",
       " 0.8403361439704895,\n",
       " 0.8552631735801697,\n",
       " 0.9718309640884399,\n",
       " 0.9577465057373047,\n",
       " 0.8873239159584045,\n",
       " 0.9090909361839294,\n",
       " 0.8025209903717041,\n",
       " 0.8552631735801697,\n",
       " 0.9154929518699646,\n",
       " 0.9012345671653748,\n",
       " 0.9610389471054077,\n",
       " 0.9066147804260254,\n",
       " 0.8636363744735718,\n",
       " 0.9350649118423462,\n",
       " 0.9153439402580261,\n",
       " 0.9154929518699646,\n",
       " 0.8765432238578796,\n",
       " 0.9260700345039368,\n",
       " 0.8395061492919922,\n",
       " 0.9428571462631226,\n",
       " 0.9182879328727722,\n",
       " 0.9100528955459595,\n",
       " 0.9143968820571899,\n",
       " 0.9873417615890503,\n",
       " 0.9675324559211731,\n",
       " 0.9025974273681641,\n",
       " 0.9295774698257446,\n",
       " 0.9333333373069763,\n",
       " 0.8881579041481018,\n",
       " 0.9481481313705444,\n",
       " 0.9481481313705444,\n",
       " 0.9428571462631226,\n",
       " 0.9296296238899231,\n",
       " 0.9181034564971924,\n",
       " 0.9037036895751953,\n",
       " 0.9409282803535461,\n",
       " 0.9333333373069763,\n",
       " 0.9281768202781677,\n",
       " 0.9100528955459595,\n",
       " 0.8663793206214905,\n",
       " 0.8472222089767456,\n",
       " 0.8945147395133972,\n",
       " 0.9238095283508301,\n",
       " 0.9142857193946838,\n",
       " 0.928909957408905,\n",
       " 1.0,\n",
       " 0.9146919250488281,\n",
       " 0.9904761910438538,\n",
       " 0.9047619104385376,\n",
       " 0.887159526348114,\n",
       " 0.9281437397003174,\n",
       " 0.9488189220428467,\n",
       " 0.9409449100494385,\n",
       " 0.9428571462631226,\n",
       " 0.8987341523170471,\n",
       " 1.0,\n",
       " 0.8472222089767456,\n",
       " 0.9454545378684998,\n",
       " 1.0,\n",
       " 0.8867924809455872,\n",
       " 0.910179615020752,\n",
       " 0.8999999761581421,\n",
       " 0.9444444179534912,\n",
       " 0.9362549781799316,\n",
       " 0.9322709441184998,\n",
       " 0.9375,\n",
       " 0.8931623697280884,\n",
       " 0.8681318759918213,\n",
       " 0.9788732528686523,\n",
       " 0.9196428656578064,\n",
       " 0.9714285731315613,\n",
       " 0.9942528605461121,\n",
       " 0.94017094373703,\n",
       " 0.9779411554336548,\n",
       " 0.970588207244873,\n",
       " 0.9163346886634827,\n",
       " 0.8839285969734192,\n",
       " 0.9358974099159241,\n",
       " 0.954023003578186,\n",
       " 0.94017094373703,\n",
       " 0.982758641242981,\n",
       " 0.94017094373703,\n",
       " 0.9425287246704102,\n",
       " 0.8839285969734192,\n",
       " 0.8818897604942322,\n",
       " 0.959770143032074,\n",
       " 0.9264705777168274,\n",
       " 0.9870129823684692,\n",
       " 0.9014084339141846,\n",
       " 0.8976377844810486,\n",
       " 0.8928571343421936,\n",
       " 0.8700787425041199,\n",
       " 0.9714285731315613,\n",
       " 0.9252873659133911,\n",
       " 0.9567099809646606,\n",
       " 0.9428571462631226,\n",
       " 0.959770143032074,\n",
       " 0.9017857313156128,\n",
       " 0.9886363744735718,\n",
       " 1.0,\n",
       " 0.8571428656578064,\n",
       " 0.9220778942108154,\n",
       " 0.9886363744735718,\n",
       " 0.9444444179534912,\n",
       " 0.9273504018783569,\n",
       " 0.939393937587738,\n",
       " 0.9590163826942444,\n",
       " 0.9090909361839294,\n",
       " 0.9022988677024841,\n",
       " 0.9188033938407898,\n",
       " 0.9590163826942444,\n",
       " 0.9090909361839294,\n",
       " 0.9485294222831726,\n",
       " 0.9444444179534912,\n",
       " 1.0,\n",
       " 0.8888888955116272,\n",
       " 0.9111111164093018,\n",
       " 0.948113203048706,\n",
       " 1.0,\n",
       " 0.9571428298950195,\n",
       " 0.9965277910232544,\n",
       " 0.9386792182922363,\n",
       " 0.8852459192276001,\n",
       " 0.8909090757369995,\n",
       " 0.963302731513977,\n",
       " 0.949999988079071,\n",
       " 0.9724770784378052,\n",
       " 0.9954338073730469,\n",
       " 0.9433962106704712,\n",
       " 0.9816513657569885,\n",
       " 0.9816513657569885,\n",
       " 0.9411764740943909,\n",
       " 1.0,\n",
       " 0.9411764740943909,\n",
       " 0.9726027250289917,\n",
       " 0.9965277910232544,\n",
       " 0.9724770784378052,\n",
       " 0.9965277910232544,\n",
       " 0.9724770784378052,\n",
       " 0.9661017060279846,\n",
       " 0.9726027250289917,\n",
       " 0.9117646813392639,\n",
       " 0.9292452931404114,\n",
       " 0.9492753744125366,\n",
       " 0.913241982460022,\n",
       " 0.9965277910232544,\n",
       " 0.948113203048706,\n",
       " 0.9391891956329346,\n",
       " 0.9459459185600281,\n",
       " 0.9032257795333862,\n",
       " 0.84375,\n",
       " 0.949999988079071,\n",
       " 0.9572649598121643,\n",
       " 0.9826388955116272,\n",
       " 0.949999988079071,\n",
       " 0.9318181872367859,\n",
       " 0.932584285736084,\n",
       " 0.9491525292396545,\n",
       " 0.90625,\n",
       " 0.9449541568756104,\n",
       " 0.9357798099517822,\n",
       " 0.9082568883895874,\n",
       " 0.8974359035491943,\n",
       " 0.9161290526390076,\n",
       " 0.9661017060279846,\n",
       " 0.8666666746139526,\n",
       " 0.9322034120559692,\n",
       " 0.9965277910232544,\n",
       " 1.0,\n",
       " 0.9017093777656555,\n",
       " 1.0,\n",
       " 0.9130434989929199,\n",
       " 0.9800000190734863,\n",
       " 0.9399999976158142,\n",
       " 0.9514563083648682,\n",
       " 0.9350649118423462,\n",
       " 0.9223300814628601,\n",
       " 0.9661017060279846,\n",
       " 0.9861111044883728,\n",
       " 0.8426966071128845,\n",
       " 0.9835164546966553,\n",
       " 0.9450549483299255,\n",
       " 0.9558823704719543,\n",
       " 0.8983050584793091,\n",
       " 0.890625,\n",
       " 0.9583333134651184,\n",
       " 0.9200000166893005,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9449999928474426,\n",
       " 0.9599999785423279,\n",
       " 0.920634925365448,\n",
       " 0.8543689250946045,\n",
       " 0.9482758641242981,\n",
       " 0.970588207244873,\n",
       " 0.9876543283462524,\n",
       " 0.9818181991577148,\n",
       " 0.9482758641242981,\n",
       " 0.8844221234321594,\n",
       " 0.9411764740943909,\n",
       " 0.9242424368858337,\n",
       " 0.9896551966667175,\n",
       " 0.934482753276825,\n",
       " 1.0,\n",
       " 0.9184397459030151,\n",
       " 0.9583333134651184,\n",
       " 0.9908257126808167,\n",
       " 0.9793814420700073,\n",
       " 0.9375,\n",
       " 0.9816513657569885,\n",
       " 0.8828125,\n",
       " 1.0,\n",
       " 0.9793814420700073,\n",
       " 0.9514563083648682,\n",
       " 0.9440000057220459,\n",
       " 0.9508196711540222,\n",
       " 0.9427312612533569,\n",
       " 0.9626168012619019,\n",
       " 0.9626168012619019,\n",
       " 0.9722222089767456,\n",
       " 0.9718309640884399,\n",
       " 0.9470587968826294,\n",
       " 0.9765258431434631,\n",
       " 0.8888888955116272,\n",
       " 0.9343065619468689,\n",
       " 0.9305555820465088,\n",
       " 0.9436619877815247,\n",
       " 0.95652174949646,\n",
       " 0.938144326210022,\n",
       " 0.9813084006309509,\n",
       " 0.9626168012619019,\n",
       " 0.9532710313796997,\n",
       " 0.9028339982032776,\n",
       " 0.9166666865348816,\n",
       " 0.9833333492279053,\n",
       " 0.936170220375061,\n",
       " 1.0,\n",
       " 0.9611111283302307,\n",
       " 0.9252336621284485,\n",
       " 0.9252336621284485,\n",
       " 0.9530516266822815,\n",
       " 0.9117646813392639,\n",
       " 0.949999988079071,\n",
       " 0.9830508232116699,\n",
       " 0.9802631735801697,\n",
       " 0.9555555582046509,\n",
       " ...]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fits_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
