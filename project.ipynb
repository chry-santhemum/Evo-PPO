{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "import torch.nn.functional as F\n",
    "import minigrid\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from tqdm.notebook import tqdm\n",
    "from minigrid.envs.doorkey import DoorKeyEnv\n",
    "import pandas as pd\n",
    "from gym.envs.registration import registry, register\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from typing_extensions import Self\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device() -> torch.device:\n",
    "    \"\"\"\n",
    "    Returns the device to use for training.\n",
    "    \"\"\"\n",
    "\n",
    "    if torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    elif torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "DEVICE = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stopwatch:\n",
    "    def __init__(self, default_time=60):\n",
    "        \"\"\"\n",
    "        A stopwatch to count down a variable\n",
    "        \"\"\"\n",
    "\n",
    "        # check default_time is a non-negative integer\n",
    "        t = default_time\n",
    "        assert isinstance(t, int) and t >= 0, f\"Default time {t} must be a non-negative integer.\"\n",
    "\n",
    "        self.DEFAULT = t\n",
    "        self.current = 0\n",
    "\n",
    "    def activate(self, time=None):\n",
    "        \"\"\"\n",
    "        Set the time to time or the maximum time if time is None.\n",
    "\n",
    "        Args:\n",
    "            time: int or None\n",
    "        \"\"\"\n",
    "\n",
    "        if time is None:\n",
    "            self.current = self.DEFAULT\n",
    "        else:\n",
    "            # check time is a non-negative integer\n",
    "            assert isinstance(time, int) and time >= 0, f\"Time {time} must be a non-negative integer.\"\n",
    "            \n",
    "            self.current = time\n",
    "\n",
    "    def count_down(self) -> bool:\n",
    "        \"\"\"\n",
    "        Count down the stopwatch by 1.\n",
    "\n",
    "        Returns:\n",
    "            False if the stopwatch has reached 0, True otherwise\n",
    "        \"\"\"\n",
    "\n",
    "        if self.current > 0:\n",
    "            self.current -= 1\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def is_on(self) -> bool:\n",
    "        \"\"\"\n",
    "        Check if the stopwatch is on.\n",
    "\n",
    "        Returns:\n",
    "            True if the stopwatch is on, False otherwise\n",
    "        \"\"\"\n",
    "\n",
    "        return self.current > 0\n",
    "\n",
    "    def deactivate(self):\n",
    "        self.current = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousBanditV4:\n",
    "    \"\"\"\n",
    "    Action space: R^1\n",
    "    Objective: maxiamize the expected reward\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 lb=-float('inf'), \n",
    "                 ub=float('inf'), \n",
    "                 mean=None, \n",
    "                 std=None, \n",
    "                 min_std=0,\n",
    "                 max_std=None,\n",
    "                 sample_number=20, \n",
    "                 alpha=0.2,\n",
    "                 lr=None,\n",
    "                 mean_lr=0.03,\n",
    "                 std_lr=0.03,\n",
    "                 reward_std_lr=0.1,\n",
    "                 reincarnation=50,\n",
    "                 restart_pos=None,\n",
    "                 restart_lr=0.1,\n",
    "                 database_capacity=100):\n",
    "        \"\"\"\n",
    "        lb: lower bound of action space\n",
    "        ub: upper bound of action space\n",
    "        mean: initial mean of the normal distribution\n",
    "        std: initial standard deviation of the normal distribution\n",
    "        min_std: minimum standard deviation allowed\n",
    "        max_std: maximum standard deviation allowed\n",
    "        sample_number: the number of actions to sample for each pull\n",
    "        alpha: the parameter for UCB computation\n",
    "        lr: the learning rate for mean and std (and reward_std)\n",
    "        mean_lr: the learning rate for mean\n",
    "        std_lr: the learning rate for std\n",
    "        reward_std_lr: the learning rate for reward_std\n",
    "        reincarnation: the period of reincarnation\n",
    "        restart_pos: the position to restart the mean\n",
    "        database_capacity: the capacity of the database to store the historical data\n",
    "        \"\"\"\n",
    "\n",
    "        self.lb = lb\n",
    "        self.ub = ub\n",
    "        self.can_use_bounds = (lb != -float('inf') and ub != float('inf'))\n",
    "\n",
    "        if mean is not None:\n",
    "            self.mean = mean\n",
    "        elif self.can_use_bounds:\n",
    "            self.mean = (self.ub + self.lb) / 2\n",
    "        elif lb != -float('inf'):\n",
    "            self.mean = self.lb + 1.0\n",
    "        elif ub != float('inf'):\n",
    "            self.mean = self.ub - 1.0\n",
    "        else:\n",
    "            self.mean = 0.0\n",
    "        if std is not None:\n",
    "            self.std = std\n",
    "        elif lb != -float('inf') and ub != float('inf'):\n",
    "            self.std = (self.ub - self.lb) / np.sqrt(12)\n",
    "        else:\n",
    "            self.std = 1.0\n",
    "\n",
    "        if max_std is not None:\n",
    "            self.max_std = max_std\n",
    "        elif self.can_use_bounds:\n",
    "            self.max_std = (self.ub - self.lb) / 2\n",
    "        else:\n",
    "            self.max_std = float('inf')\n",
    "        \n",
    "        self.min_std = min_std\n",
    "        self.sample_number = sample_number\n",
    "        self.alpha = alpha\n",
    "        # self.epsilon = epsilon\n",
    "        self.database = self.DataBase(capacity=database_capacity) # to store the historical data\n",
    "        self.reward_std = 1.0 # to store the std of the reward distribution\n",
    "        if lr is not None:\n",
    "            self.mean_lr = lr\n",
    "            self.std_lr = lr\n",
    "            self.reward_std_lr = lr\n",
    "        else:\n",
    "            self.mean_lr = mean_lr\n",
    "            self.std_lr = std_lr\n",
    "            self.reward_std_lr = reward_std_lr\n",
    "\n",
    "        self.STD_FACTOR = np.sqrt(np.pi / 2)\n",
    "\n",
    "        self.n = 0 # number of times we pull the arm\n",
    "        self.previous_a = None # to store the previous action\n",
    "        self.previous_expected_r = None # to store the previous expected reward\n",
    "\n",
    "        # these are the memory to draw diagram\n",
    "        self.col_means = []\n",
    "        self.col_as = []\n",
    "        self.col_rs = []\n",
    "        self.col_std = []\n",
    "\n",
    "        self.reincarnation = reincarnation\n",
    "        self.restart_lr = restart_lr\n",
    "        if restart_pos is not None:\n",
    "            self.restart_pos = restart_pos\n",
    "        else:\n",
    "            self.restart_pos = self.mean\n",
    "\n",
    "\n",
    "    class DataBase:\n",
    "        \"\"\"\n",
    "        A database to store the historical data of the one-armed bandit problem.\n",
    "        Data consists of a list of tuples (action, reward)\n",
    "        \"\"\"\n",
    "        \n",
    "        def __init__(self, capacity: int=100):\n",
    "            self.data = []\n",
    "            self.capacity = capacity\n",
    "            self.bests = [] # store best (a, r, t)'s\n",
    "            self.protection = Stopwatch(default_time=5) # to protect the bests from being removed too frequently\n",
    "\n",
    "        def get_length(self) -> int:\n",
    "            return len(self.data)\n",
    "\n",
    "        def add_data(self, action: float, reward: float, time_step: int) -> None:\n",
    "            self.data.append((action, reward, time_step))\n",
    "            self.bests.append((action, reward, time_step))\n",
    "            # if exceed capacity, remove the oldest data\n",
    "            if self.get_length() > self.capacity:\n",
    "                assert self.get_length() == self.capacity + 1, f'#data = {self.get_length()} > 1 + capacity = {1 + self.capacity}'\n",
    "                self.data.pop(0)\n",
    "                # delete the worst reward from bests\n",
    "                self.bests = sorted(self.bests, key=lambda x: x[1])\n",
    "                self.bests.pop(0)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        def get_data(self) -> list:\n",
    "            return self.data\n",
    "        \n",
    "        def sample_data(self, size: int) -> list:\n",
    "            return np.random.choice(self.data, size=size)\n",
    "        \n",
    "        def fill_bests(self) -> None:\n",
    "            \"\"\"\n",
    "            If bests is not full, fill it with highest current data\n",
    "            \"\"\"\n",
    "            length_bests = len(self.bests)\n",
    "            assert length_bests <= self.capacity, f'#bests {length_bests} > capacity {self.capacity}.'\n",
    "\n",
    "            if length_bests < self.capacity:\n",
    "                num_to_fill = self.capacity - len(self.bests)\n",
    "                # take num_to_fill bests from data\n",
    "                best_data = sorted(self.data, key=lambda x: x[1], reverse=True)[:num_to_fill]\n",
    "                self.bests.extend(best_data)\n",
    "        \n",
    "        def compute_std(self, std: float) -> tuple[int, float]:\n",
    "            \"\"\"\n",
    "            Compute std of rewards according to the bests\n",
    "\n",
    "            Args:\n",
    "                std: the std of the policy\n",
    "\n",
    "            Returns:\n",
    "                mode: 0 for normal\n",
    "                        1 for need adjustment\n",
    "                        2 for too small database\n",
    "                std: the std of the reward distribution\n",
    "                        if mode = 1, then is the target std to adjust self.std above\n",
    "            \"\"\"\n",
    "            def helper_radius(A: list[float], b: float, are_sort=True) -> float:\n",
    "                \"\"\"\n",
    "                Compute min(abs(a - b)) for a in A\n",
    "\n",
    "                Args:\n",
    "                    A: a list of floats\n",
    "                    b: a float\n",
    "                    are_sort: if True, A is sorted in ascending order\n",
    "                \n",
    "                Returns:\n",
    "                    min(abs(a - b)) for a in A\n",
    "                \"\"\"\n",
    "                if not are_sort:\n",
    "                    A = sorted(A)\n",
    "\n",
    "                low = 0\n",
    "                high = len(A) - 1\n",
    "                if A[low] > b:\n",
    "                    return A[low] - b  \n",
    "                elif A[high] < b:\n",
    "                    return b - A[high]\n",
    "                \n",
    "                while low < high - 1:\n",
    "                    mid = (low + high) // 2\n",
    "                    if A[mid] < b:\n",
    "                        low = mid\n",
    "                    else:\n",
    "                        high = mid\n",
    "\n",
    "                radius = min(A[high] - b, b - A[low])\n",
    "                assert radius >= 0, f'radius = {radius} < 0'\n",
    "                \n",
    "                return radius\n",
    "\n",
    "            def helper_min_radius(A: list[float], B: list[float], num_to_fill: int) -> float:\n",
    "                \"\"\"\n",
    "                Compute the minimum radius r such that num_to_fill elements of data from B is within r from A\n",
    "                \"\"\"\n",
    "\n",
    "                # sort A\n",
    "                A = sorted(A)\n",
    "                radius_list = []\n",
    "\n",
    "                for b in B:\n",
    "                    radius = helper_radius(A, b)\n",
    "                    radius_list.append(radius)\n",
    "\n",
    "                # then select the num_to_fill smallest of radius_list\n",
    "                radius_list = sorted(radius_list)\n",
    "\n",
    "                return radius_list[num_to_fill - 1]\n",
    "\n",
    "            if len(self.bests) <= 3:\n",
    "                return 2, 0.0\n",
    "            elif self.get_length() < self.capacity:\n",
    "                return 0, np.std([reward for _, reward, _ in self.bests])\n",
    "            \n",
    "            good_data = []\n",
    "\n",
    "            # take 20 elites from self.bests\n",
    "            elites = sorted(self.bests, key=lambda x: x[1], reverse=True)[:20]\n",
    "            elite_as = [a for a, _, _ in elites]\n",
    "            elite_as = sorted(elite_as)\n",
    "\n",
    "            for action, reward, t in self.get_data():\n",
    "                radius = helper_radius(elite_as, action)\n",
    "                if radius < 2 * std:\n",
    "                    good_data.append((action, reward, t))\n",
    "\n",
    "            if len(good_data) >= 10:\n",
    "                self.protection.count_down()\n",
    "                return 0, np.std([reward for _, reward, _ in good_data])\n",
    "            else:\n",
    "                data_as = [a for a, _, _ in self.get_data()]\n",
    "                target_std = helper_min_radius(elite_as, data_as, self.get_length() // 2)\n",
    "                assert target_std >= 2 * std, f'elite_as: {elite_as}\\n data_as: {data_as}\\n std: {std}\\n target_std: {target_std}\\n good_data: {good_data}'\n",
    "\n",
    "                # remove the earliest data in elites from bests\n",
    "                if not self.protection.is_on():\n",
    "                    elites = sorted(elites, key=lambda x: x[2])\n",
    "                    earliest_data = elites[0]\n",
    "                    self.bests.remove(earliest_data)\n",
    "                    self.fill_bests()\n",
    "                \n",
    "                self.protection.activate()\n",
    "\n",
    "                return 1, target_std\n",
    "\n",
    "\n",
    "    def get_mean(self) -> float:\n",
    "        return self.mean\n",
    "    \n",
    "    def get_std(self) -> float:\n",
    "        return self.std\n",
    "\n",
    "    def get_sample_number(self) -> int:\n",
    "        return self.sample_number\n",
    "    \n",
    "    def set_sample_number(self, sample_number: int) -> None:\n",
    "        self.sample_number = sample_number\n",
    "\n",
    "    def sample_actions(self, size: int) -> np.ndarray[float]:\n",
    "        \"\"\"\n",
    "        sample action according to mean and variance\n",
    "        args:\n",
    "            size: the number of actions to sample\n",
    "        returns:\n",
    "            actions\n",
    "        \"\"\"\n",
    "        a = np.random.normal(self.mean, self.std, size=size)\n",
    "        a = np.clip(a, self.lb, self.ub)\n",
    "        return a\n",
    "    \n",
    "    def get_neighbors(self, x: float, k: int) -> list[tuple[float, float, float]]:\n",
    "        \"\"\"\n",
    "        get k nearest neighbors of x in the database\n",
    "        args:\n",
    "            x: the query point\n",
    "            k: the number of neighbors to return\n",
    "        returns:\n",
    "            list of length k, each element is a tuple\n",
    "                    (action, reward, distance)\n",
    "        \"\"\"\n",
    "        distances = [np.abs(x - action) for action, _, _ in self.database.get_data()]\n",
    "        # if database is too small, return all data\n",
    "        length_of_database = self.database.get_length()\n",
    "        if length_of_database < k:\n",
    "            return [(self.database.get_data()[i][0], self.database.get_data()[i][1], distances[i]) for i in range(length_of_database)]\n",
    "        \n",
    "        indices = np.argsort(distances)[:k]\n",
    "        return [(self.database.get_data()[i][0], self.database.get_data()[i][1], distances[i]) for i in indices]\n",
    "\n",
    "    def get_ucb(self, a: float) -> float:\n",
    "        \"\"\"\n",
    "        get the upper confidence bound of action a\n",
    "        args:\n",
    "            a: the action\n",
    "        returns:\n",
    "            the upper confidence bound\n",
    "        \"\"\"\n",
    "        length_of_database = len(self.database.data)\n",
    "        if length_of_database == 0:\n",
    "            return float('inf')\n",
    "        \n",
    "        neighbors = self.get_neighbors(a, k=20)\n",
    "        rewards = [reward for _, reward, _ in neighbors]\n",
    "        value = np.mean(rewards)\n",
    "        self.previous_expected_r = value\n",
    "        # normalize value\n",
    "        value /= self.reward_std + 1e-8\n",
    "        # unexplored = sum(np.log(distance + 1e-8) for _, _, distance in neighbors) / len(neighbors)\n",
    "        unexplored = -sum(np.sqrt(self.std / (distance + 1e-8)) for _, _, distance in neighbors) / len(neighbors)\n",
    "        return value + self.alpha * unexplored\n",
    "\n",
    "    def pull(self) -> float:\n",
    "        \"\"\"\n",
    "        returns:\n",
    "            best_action\n",
    "        \"\"\"\n",
    "        # sample sample_number actions\n",
    "        actions = self.sample_actions(self.get_sample_number())\n",
    "        # get the action with the highest upper confidence bound\n",
    "        best_action = max(actions, key=self.get_ucb)\n",
    "\n",
    "        self.get_ucb(best_action) # just to update self.previous_expected_r\n",
    "\n",
    "        self.previous_a = best_action\n",
    "\n",
    "        self.n += 1\n",
    "\n",
    "        return best_action\n",
    "    \n",
    "    def update_with_a(self, a: float, reward: float) -> None:\n",
    "        \"\"\"\n",
    "        use (a, reward) to update the mean, std, and database\n",
    "\n",
    "        args:\n",
    "            a: the action\n",
    "            reward: the reward\n",
    "        \"\"\"\n",
    "        def update_mean(target: float, deviation:float) -> None:\n",
    "            \"\"\"\n",
    "            Args:\n",
    "                target: the target mean\n",
    "                deviation: how much std(rewards) was current r shifted from expected r\n",
    "            \"\"\"\n",
    "            def helper_sigmoid(x: float) -> float:\n",
    "                return 1 / (1 + np.exp(-x))\n",
    "            def helper_sigmoid_inverse(y: float) -> float:\n",
    "                return np.log(y / (1 - y))\n",
    "            \n",
    "            base = helper_sigmoid_inverse(self.mean_lr)\n",
    "            base = base + deviation\n",
    "            tau = helper_sigmoid(base)\n",
    "            tau = np.clip(tau, self.mean_lr / 3, float('inf'))\n",
    "\n",
    "            self.mean = (1 - tau) * self.mean + tau * target\n",
    "        \n",
    "        def update_std(target: float) -> None:\n",
    "            self.std = (1 - self.std_lr) * self.std + self.std_lr * target\n",
    "\n",
    "        def update_reward_std(target: float) -> None:\n",
    "            self.reward_std = (1 - self.reward_std_lr) * self.reward_std + self.reward_std_lr * target\n",
    "\n",
    "        # first of all, append data into collections\n",
    "        self.col_means.append(self.mean)\n",
    "        self.col_as.append(a)\n",
    "        self.col_rs.append(reward)\n",
    "        self.col_std.append(self.std)\n",
    "\n",
    "        # add data to database\n",
    "        self.database.add_data(a, reward, self.n)\n",
    "\n",
    "        # update mean\n",
    "        current_mean = self.mean\n",
    "        if self.previous_expected_r is not None:\n",
    "            deviation = (reward - self.previous_expected_r) / (self.reward_std + 1e-8)\n",
    "        else:\n",
    "            deviation = 0.0\n",
    "        update_mean(a, deviation)\n",
    "        \n",
    "        # update reward_std\n",
    "        mode, std = self.database.compute_std(self.std)\n",
    "        if mode == 0:\n",
    "            update_reward_std(std)\n",
    "        elif mode == 1:\n",
    "            # increase std\n",
    "            #print('adjusted')\n",
    "            update_std(std)\n",
    "        elif mode == 2:\n",
    "            pass\n",
    "        # update mean and std\n",
    "        update_std(self.STD_FACTOR * np.abs(a - current_mean))\n",
    "        self.std = np.clip(self.std, self.min_std, self.max_std)\n",
    "\n",
    "        # do reincarnation\n",
    "        if self.n % self.reincarnation == 0 and self.n >= 2 * self.reincarnation:\n",
    "            average_r_before = np.mean(self.col_rs[-2 * self.reincarnation: -self.reincarnation])\n",
    "            average_r_after = np.mean(self.col_rs[-self.reincarnation:])\n",
    "            if average_r_after <= average_r_before:\n",
    "                self.restart()\n",
    "    \n",
    "    def update(self, reward: float) -> None:\n",
    "        \"\"\"\n",
    "        use (a, reward) to update the mean, std, and database\n",
    "        a = self.previous_a\n",
    "\n",
    "        args:\n",
    "            reward: the reward\n",
    "        \"\"\"\n",
    "        self.update_with_a(self.previous_a, reward)\n",
    "\n",
    "    def restart(self) -> None:\n",
    "        \"\"\"\n",
    "        restart the mean to restart_pos\n",
    "        \"\"\"\n",
    "        shift = self.mean - self.restart_pos\n",
    "        self.mean = self.restart_pos\n",
    "        # soft update restart_pos\n",
    "        self.restart_pos += self.restart_lr * shift\n",
    "\n",
    "    def draw(self, low=0, high=None, draw_mean=True, draw_a=True, draw_r=False, draw_std=False) -> None:\n",
    "        \"\"\"\n",
    "        draw the diagram of the one-armed bandit problem\n",
    "        \"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        length = len(self.col_means)\n",
    "\n",
    "        if high is None:\n",
    "            high = length\n",
    "\n",
    "        plt.figure()\n",
    "        x_axis = range(length)\n",
    "        if draw_mean:\n",
    "            plt.plot(x_axis[low:high], self.col_means[low:high], label='mean')\n",
    "        if draw_a:\n",
    "            plt.plot(x_axis[low:high], self.col_as[low:high], label='a')\n",
    "        if draw_std:\n",
    "            plt.plot(x_axis[low:high], self.col_std[low:high], label='std')\n",
    "        \n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        if draw_r:\n",
    "            plt.figure()\n",
    "            plt.plot(x_axis[low:high], self.col_rs[low:high], label='r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function from https://github.com/ikostrikov/pytorch-a2c-ppo-acktr/blob/master/model.py\n",
    "def init_params(m):\n",
    "    \"\"\"\n",
    "    Initialize parameters of the network.\n",
    "    m: torch.nn.Module\n",
    "    \"\"\"\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Linear\") != -1:\n",
    "        m.weight.data.normal_(0, 1)\n",
    "        m.weight.data *= 1 / torch.sqrt(m.weight.data.pow(2).sum(1, keepdim=True))\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0)\n",
    "\n",
    "def preprocess_obss(obss, device=None):\n",
    "    \"\"\"\n",
    "    Convert observation into Torch.Tensor\n",
    "\n",
    "    Parameters\n",
    "    ----\n",
    "    obss: dictionary or np.ndarray\n",
    "    device: target device of torch.Tensor ('cpu', 'cuda')\n",
    "\n",
    "    Return\n",
    "    ----\n",
    "    Torch Tensor\n",
    "    \"\"\"\n",
    "    if isinstance(obss, dict):\n",
    "        images = np.array([obss[\"image\"]])\n",
    "    else:\n",
    "        images = np.array([o[\"image\"] for o in obss])\n",
    "\n",
    "    return torch.tensor(images, device=device, dtype=torch.float)\n",
    "\n",
    "class DoorKeyEnv5x5(DoorKeyEnv):\n",
    "    def __init__(self):\n",
    "        self.render_mode = \"rgb_array\"\n",
    "        super().__init__(size=5)\n",
    "\n",
    "    def _reward(self):\n",
    "        \"\"\"\n",
    "        Compute the reward to be given upon success\n",
    "        \"\"\"\n",
    "        return 1\n",
    "    \n",
    "class DoorKeyEnv6x6(DoorKeyEnv):\n",
    "    def __init__(self):\n",
    "        self.render_mode = \"rgb_array\"\n",
    "        super().__init__(size=6)\n",
    "\n",
    "    def _reward(self):\n",
    "        \"\"\"\n",
    "        Compute the reward to be given upon success\n",
    "        \"\"\"\n",
    "        return 1\n",
    "    \n",
    "class DoorKeyEnv8x8(DoorKeyEnv):\n",
    "    def __init__(self):\n",
    "        self.render_mode = \"rgb_array\"\n",
    "        super().__init__(size=8)\n",
    "\n",
    "    def _reward(self):\n",
    "        \"\"\"\n",
    "        Compute the reward to be given upon success\n",
    "        \"\"\"\n",
    "        return 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"\n",
    "    Stores algorithmic hyperparameters.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                score_threshold=0.93,\n",
    "                discount=0.995,\n",
    "                lr=1e-3,\n",
    "                max_grad_norm=0.5,\n",
    "                log_interval=10,\n",
    "                max_episodes=2000,\n",
    "                gae_lambda=0.95,\n",
    "                use_critic=False,\n",
    "                clip_ratio=0.2,\n",
    "                target_kl=0.01,\n",
    "                train_ac_iters=5,\n",
    "                use_discounted_reward=False,\n",
    "                entropy_coef=0.01,\n",
    "                use_gae=True,\n",
    "                my_var=0):\n",
    "\n",
    "        self.score_threshold = score_threshold # criterion for early stopping. If the rolling average reward (over the last 100 episodes) is greater than it, it ends.\n",
    "        self.discount = discount # discount factor\n",
    "        self.lr = lr # learning rate\n",
    "        self.max_grad_norm = max_grad_norm # the maximum gradient norm (https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html)\n",
    "        self.log_interval = log_interval # logging interval\n",
    "        self.max_episodes = max_episodes # the maximum number of episodes.\n",
    "        self.use_critic = use_critic # whether to use critic or not.\n",
    "        self.clip_ratio = clip_ratio # clip_ratio of PPO.\n",
    "        self.target_kl = target_kl # target KL divergence for early stoping train_ac_iters for PPO\n",
    "        self.train_ac_iters = train_ac_iters # how many time to train ac_model using current computed old_logps\n",
    "        self.gae_lambda=gae_lambda # lambda in Generalized Advantage Estimation (GAE)\n",
    "        self.use_discounted_reward=use_discounted_reward # whether use discounted reward or not.\n",
    "        self.entropy_coef = entropy_coef # entropy coefficient for PPO\n",
    "        self.use_gae = use_gae # whether to use GAE or not.\n",
    "        self.my_var = my_var # a variable for testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x16d7a89d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = DoorKeyEnv6x6() # define environment.\n",
    "env.reset()\n",
    "action = env.action_space.sample()\n",
    "obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "# get an RGB image corresponding to the whole environment or the agent's point of view (https://github.com/Farama-Foundation/Minigrid/blob/master/minigrid/minigrid_env.py#L716)\n",
    "#            highlight (bool): If true, the agent's field of view or point of view is highlighted with a lighter gray color.\n",
    "#            tile_size (int): How many pixels will form a tile from the NxM grid.\n",
    "#            agent_pov (bool): If true, the rendered frame will only contain the point of view of the agent.\n",
    "frame = env.get_frame(highlight=env.highlight, tile_size=env.tile_size, agent_pov=env.agent_pov)\n",
    "# show an image to the notebook.\n",
    "plt.imshow(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "class ACModel(nn.Module):\n",
    "    def __init__(self, num_actions, use_critic=False):\n",
    "        \"\"\"\n",
    "        Represents an Actor Crictic model that takes a 2d, multi-channeled\n",
    "        image as input.\n",
    "\n",
    "        Parameters\n",
    "        ----\n",
    "        num_actions : int\n",
    "\n",
    "                      The action space of the environment.\n",
    "                      The action space for DoorKey5x5 is 7-dimensional:\n",
    "                      0: turn left,\n",
    "                      1: turn right,\n",
    "                      2: forward,\n",
    "                      3: pickup an object,\n",
    "                      4: drop an object,\n",
    "                      5: activate an object,\n",
    "                      6: done completing task\n",
    "\n",
    "        use_critics : bool\n",
    "\n",
    "                      Critic network will be used in forward pass if flag is set\n",
    "                      to true.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.use_critic = use_critic\n",
    "\n",
    "        # Define actor's model\n",
    "        self.image_conv_actor = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, (2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 2)),\n",
    "            nn.Conv2d(16, 32, (2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, (2, 2)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, num_actions)\n",
    "        )\n",
    "\n",
    "        # Define critic's model\n",
    "        if self.use_critic:\n",
    "            self.image_conv_critic = nn.Sequential(\n",
    "                nn.Conv2d(3, 16, (2, 2)),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d((2, 2)),\n",
    "                nn.Conv2d(16, 32, (2, 2)),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(32, 64, (2, 2)),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "            self.critic = nn.Sequential(\n",
    "                nn.Linear(64, 64),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(64, 1)\n",
    "            )\n",
    "\n",
    "        # Initialize parameters correctly\n",
    "        self.apply(init_params)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        \"\"\"\n",
    "        Performs a forward pass through the actor-critic network\n",
    "\n",
    "        Parameters\n",
    "        ----\n",
    "        obs : int tensor. Shape [Batch size, ImWidth, ImHeight, Channels]\n",
    "\n",
    "              input to the network.\n",
    "        ----\n",
    "\n",
    "        returns:\n",
    "\n",
    "        dist : torch.distribution\n",
    "            The distribution of actions from policy. A Categorical distribution\n",
    "            for discreet action spaces.\n",
    "        value : torch.Tensor (Batch size, 1)\n",
    "            value output by critic network\n",
    "        \"\"\"\n",
    "        conv_in = obs.transpose(1, 3).transpose(2, 3) # reshape into [b, c, h, w]\n",
    "\n",
    "        dist, value = None, None\n",
    "\n",
    "        x = self.image_conv_actor(conv_in)\n",
    "        embedding = x.reshape(x.shape[0], -1)\n",
    "\n",
    "        x = self.actor(embedding)\n",
    "        dist = Categorical(logits=F.log_softmax(x, dim=1))\n",
    "\n",
    "        if self.use_critic:\n",
    "            y = self.image_conv_critic(conv_in)\n",
    "            embedding = y.reshape(y.shape[0], -1)\n",
    "\n",
    "            value = self.critic(embedding).squeeze(1)\n",
    "        else:\n",
    "            value = torch.zeros((x.shape[0], 1), device=x.device)\n",
    "        \n",
    "        return dist, value\n",
    "\n",
    "def compute_discounted_return(rewards, discount, device=DEVICE):\n",
    "    \"\"\"\n",
    "\t\trewards: reward obtained at timestep.  Shape: (T,)\n",
    "\t\tdiscount: discount factor. float\n",
    "\n",
    "    ----\n",
    "    returns: sum of discounted rewards. Shape: (T,)\n",
    "\t\t\"\"\"\n",
    "    returns = torch.zeros(*rewards.shape, device=device)\n",
    "\n",
    "    R = 0\n",
    "    for t in reversed(range((rewards.shape[0]))):\n",
    "        R = rewards[t] + discount * R\n",
    "        returns[t] = R\n",
    "    return returns\n",
    "\n",
    "def compute_advantage_gae(values, rewards, T, gae_lambda, discount):\n",
    "    \"\"\"\n",
    "    Compute Adavantage wiht GAE. See Section 4.4.2 in the lecture notes.\n",
    "\n",
    "    values: value at each timestep (T,)\n",
    "    rewards: reward obtained at each timestep.  Shape: (T,)\n",
    "    T: the number of frames, float\n",
    "    gae_lambda: hyperparameter, float\n",
    "    discount: discount factor, float\n",
    "\n",
    "    -----\n",
    "\n",
    "    returns:\n",
    "\n",
    "    advantages : tensor.float. Shape [T,]\n",
    "\n",
    "                 gae advantage term for timesteps 0 to T\n",
    "\n",
    "    \"\"\"\n",
    "    advantages = torch.zeros_like(values)\n",
    "    for i in reversed(range(T)):\n",
    "        next_value = values[i+1]\n",
    "        next_advantage = advantages[i+1]\n",
    "\n",
    "        delta = rewards[i] + discount * next_value  - values[i]\n",
    "        advantages[i] = delta + discount * gae_lambda * next_advantage\n",
    "    return advantages[:T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Categorical(logits: torch.Size([1, 7])),\n",
       " tensor([-0.1168], grad_fn=<SqueezeBackward1>))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TESTS\n",
    "\n",
    "model = ACModel(num_actions=7, use_critic=True).to(DEVICE)\n",
    "obss = env.reset()\n",
    "obs = preprocess_obss(obss[0], DEVICE)\n",
    "model(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Machine:\n",
    "    def __init__(self, coef, label, num_actions=7, args=Config()):\n",
    "        \"\"\"\n",
    "        A Machine object consists of a ACModel, its hyperparameters, and its rewards.\n",
    "\n",
    "        Args:\n",
    "            coef: Entropy coefficient. TODO: Put this in config.\n",
    "            label: ???\n",
    "            num_actions\n",
    "            args: Model hyperparameters\n",
    "        \"\"\"\n",
    "        self.args = args\n",
    "        self.model = ACModel(num_actions, use_critic=self.args.use_critic).to(DEVICE)\n",
    "        self.optim = torch.optim.Adam(self.model.parameters(), lr=self.args.lr)\n",
    "        self.coef = coef\n",
    "        self.rewards = []\n",
    "        self.label = label\n",
    "\n",
    "    def reset_rs(self) -> None:\n",
    "        self.rewards = []\n",
    "\n",
    "    def copy_model(self, model: ACModel) -> None:\n",
    "        \"\"\"\n",
    "        Copy state dict from 'model'. Reset rs.\n",
    "        \"\"\"\n",
    "        state_dict = model.state_dict()\n",
    "        for key, v in state_dict.items():\n",
    "            self.model.state_dict()[key].copy_(v)\n",
    "\n",
    "        self.reset_rs()\n",
    "\n",
    "    def copy_machine(self, other: Self) -> None:\n",
    "        \"\"\"\n",
    "        Copy state dict from 'other'. Reset rs.\n",
    "        \"\"\"\n",
    "        self.copy_model(other.model)\n",
    "\n",
    "    def add_r(self, r: float) -> None:\n",
    "        self.rewards.append(r)\n",
    "\n",
    "    def avg_r(self) -> float:\n",
    "        assert len(self.rewards) > 0, 'No rewards yet'\n",
    "        return sum(self.rewards) / len(self.rewards)\n",
    "    \n",
    "    def collect_experiences(self, env:gym.Env):\n",
    "        \"\"\"\n",
    "        Collects rollouts and computes advantages.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        exps : dict\n",
    "            Contains actions, rewards, advantages etc as attributes.\n",
    "            Each attribute, e.g. `exps['reward']` has a shape\n",
    "            (self.num_frames, ...).\n",
    "        logs : dict\n",
    "            Useful stats about the training process, including the average\n",
    "            reward, policy loss, value loss, etc.\n",
    "        \"\"\"\n",
    "        device = DEVICE\n",
    "        acmodel = self.model\n",
    "\n",
    "        MAX_FRAMES_PER_EP = 300\n",
    "        shape = (MAX_FRAMES_PER_EP, )\n",
    "\n",
    "        actions = torch.zeros(*shape, device=device, dtype=torch.int)\n",
    "        values = torch.zeros(*shape, device=device)\n",
    "        rewards = torch.zeros(*shape, device=device)\n",
    "        log_probs = torch.zeros(*shape, device=device)\n",
    "        obss = [None]*MAX_FRAMES_PER_EP\n",
    "\n",
    "        obs, _ = env.reset()\n",
    "\n",
    "        total_return = 0\n",
    "\n",
    "        T = 0\n",
    "\n",
    "        while True:\n",
    "            # Do one agent-environment interaction\n",
    "\n",
    "            preprocessed_obs = preprocess_obss(obs, device=device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                dist, value = acmodel(preprocessed_obs)\n",
    "\n",
    "            dist: Categorical\n",
    "            action = dist.sample()[0]\n",
    "\n",
    "            obss[T] = obs\n",
    "            obs, reward, done, _, _ = env.step(action.item())\n",
    "\n",
    "\n",
    "            # Update experiences values\n",
    "            actions[T] = action\n",
    "            values[T] = value\n",
    "            rewards[T] = reward\n",
    "            log_probs[T] = dist.log_prob(action)\n",
    "\n",
    "\n",
    "            total_return += reward\n",
    "            T += 1\n",
    "\n",
    "            if done or T>=MAX_FRAMES_PER_EP-1:\n",
    "                break\n",
    "\n",
    "        discounted_reward = compute_discounted_return(rewards[:T], self.args.discount, device)\n",
    "        exps = dict(\n",
    "            obs = preprocess_obss([\n",
    "                obss[i]\n",
    "                for i in range(T)\n",
    "            ], device=device),\n",
    "            action = actions[:T],\n",
    "            value  = values[:T],\n",
    "            reward = rewards[:T],\n",
    "            log_prob = log_probs[:T],\n",
    "            discounted_reward = discounted_reward,\n",
    "            label = self.label,\n",
    "            T = T\n",
    "        )\n",
    "\n",
    "        logs = {\n",
    "            \"return_per_episode\": total_return,\n",
    "            \"num_frames\": T\n",
    "        }\n",
    "\n",
    "        return exps, logs\n",
    "\n",
    "    def _compute_policy_loss_ppo(self, obs, init_logp, old_logp, actions, advantages):\n",
    "        \"\"\"\n",
    "        Computes the policy loss for PPO.\n",
    "\n",
    "        obs: observeration to pass into acmodel. shape: (T,)\n",
    "        init_logp: log probabilities we get from the agent performing the action. shape: (T,)\n",
    "        old_logp: log probabilities from previous timestep. shape: (T,)\n",
    "        actions: action at this timestep. shape: (T,ImWidth,ImHeight,Channels)\n",
    "        advantages: the computed advantages. shape: (T,)\n",
    "\n",
    "        ---\n",
    "        returns\n",
    "\n",
    "        policy_loss : ppo policy loss as shown in line 6 of PPO alg. tensor.float. Shape (,1)\n",
    "        approx_kl: an appoximation of the kl_divergence. tensor.float. Shape (,1)\n",
    "        \"\"\"\n",
    "        policy_loss, approx_kl = 0, 0\n",
    "\n",
    "        dist, _ = self.model(obs)\n",
    "        dist: Categorical\n",
    "\n",
    "        coef = self.coef\n",
    "\n",
    "        entropy = dist.entropy()\n",
    "        logps = dist.log_prob(actions)\n",
    "        r_terms = torch.exp(logps - old_logp)\n",
    "        ppo_loss = torch.min(r_terms * advantages, torch.clamp(r_terms, 1 - self.args.clip_ratio, 1 + self.args.clip_ratio) * advantages)\n",
    "        \n",
    "        # Importance sampling factor\n",
    "        factors = torch.exp(old_logp - init_logp)\n",
    "        \n",
    "        indices = factors < 100\n",
    "        policy_loss_tensor = factors * ppo_loss + coef * entropy\n",
    "\n",
    "        policy_loss = - torch.mean(policy_loss_tensor[indices])\n",
    "\n",
    "        # approx_kl = torch.sum(torch.exp(old_logp) * (old_logp - logps)) / torch.sum(torch.exp(old_logp))\n",
    "        approx_kl = torch.mean((old_logp - logps) ** 2) / 2\n",
    "\n",
    "        return policy_loss, approx_kl\n",
    "    \n",
    "    def _compute_value_loss(self, obs, returns):\n",
    "        _, value = self.model(obs)\n",
    "\n",
    "        value_loss = torch.mean((value - returns) ** 2)\n",
    "\n",
    "        return value_loss\n",
    "\n",
    "\n",
    "    def update_parameters(self, sb):\n",
    "        \n",
    "        MAX_FRAMES_PER_EP = 300\n",
    "        T = sb['T']\n",
    "        with torch.no_grad():\n",
    "            dist, values = self.model(sb['obs'])\n",
    "        values = values.reshape(-1)\n",
    "        dist: Categorical\n",
    "        old_logp = dist.log_prob(sb['action']).detach()\n",
    "        init_logp = sb['log_prob']\n",
    "\n",
    "        # add 0 to end of values until it has length MAX_FRAMES_PER_EP\n",
    "        values_extended = torch.cat([values, torch.zeros((MAX_FRAMES_PER_EP - len(values), ), device=DEVICE)], dim=0)\n",
    "        full_reward = torch.cat([sb['reward'], torch.zeros((MAX_FRAMES_PER_EP - len(sb['reward']), ), device=DEVICE)], dim=0)\n",
    "\n",
    "        if self.args.use_gae:\n",
    "            advantage = compute_advantage_gae(values_extended, full_reward, T, self.args.gae_lambda, self.args.discount)\n",
    "        else:\n",
    "            advantage = sb['discounted_reward'] - values.reshape(-1)\n",
    "\n",
    "        policy_loss, _ = self._compute_policy_loss_ppo(sb['obs'], init_logp, old_logp, sb['action'], advantage)\n",
    "        value_loss = self._compute_value_loss(sb['obs'], sb['discounted_reward'])\n",
    "\n",
    "        for i in range(self.args.train_ac_iters):\n",
    "            self.optim.zero_grad()\n",
    "            loss_pi, approx_kl = self._compute_policy_loss_ppo(sb['obs'], init_logp, old_logp, sb['action'], advantage)\n",
    "            if sb['label'] == self.label:\n",
    "                loss_v = self._compute_value_loss(sb['obs'], sb['discounted_reward'])\n",
    "            else:\n",
    "                loss_v = 0.0\n",
    "\n",
    "            loss = loss_v + loss_pi\n",
    "            if approx_kl > 1.5 * self.args.target_kl:\n",
    "                break\n",
    "\n",
    "            loss.backward(retain_graph=True)\n",
    "            self.optim.step()\n",
    "\n",
    "        update_policy_loss = policy_loss.item()\n",
    "        update_value_loss = value_loss.item()\n",
    "\n",
    "        logs = {\n",
    "            \"policy_loss\": update_policy_loss,\n",
    "            \"value_loss\": update_value_loss,\n",
    "        }\n",
    "\n",
    "        return logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x2ae117370>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(coefs:list[float], max_steps:int, env=DoorKeyEnv6x6(), args=Config(), seed=0):\n",
    "    \"\"\"\n",
    "    Upper level function for running experiments to analyze reinforce and\n",
    "    policy gradient methods. Instantiates a model, collects epxeriences, and\n",
    "    then updates the neccessary parameters.\n",
    "\n",
    "    Args:\n",
    "        coefs: a list of coefs each machine has\n",
    "        max_steps: maximum number of steps to run the experiment for.\n",
    "        env: the environment to use. \n",
    "        args: Config object with hyperparameters.\n",
    "        seed: random seed. int\n",
    "\n",
    "    Returns:\n",
    "        DataFrame indexed by episode\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    device = DEVICE\n",
    "\n",
    "    machines = [Machine(coef, label=idx, args=args) for idx, coef in enumerate(coefs)]\n",
    "    winners = []\n",
    "    full_rs = []\n",
    "\n",
    "    is_solved = False\n",
    "\n",
    "    SMOOTH_REWARD_WINDOW = 50\n",
    "\n",
    "    pd_logs, rewards = [], [0]*SMOOTH_REWARD_WINDOW\n",
    "\n",
    "    num_frames = 0\n",
    "\n",
    "    pbar = tqdm(range(max_steps))\n",
    "    for update in pbar:\n",
    "\n",
    "        label = update % len(machines)\n",
    "\n",
    "        machine = machines[label]\n",
    "        \n",
    "        exps, logs1 = machine.collect_experiences(env)\n",
    "\n",
    "        for m in machines:\n",
    "            if m is machine:\n",
    "                logs2 = m.update_parameters(exps)\n",
    "            else:\n",
    "                m.update_parameters(exps)\n",
    "\n",
    "        logs = {**logs1, **logs2}\n",
    "\n",
    "        num_frames += logs[\"num_frames\"]\n",
    "\n",
    "        rewards.append(logs[\"return_per_episode\"])\n",
    "\n",
    "        smooth_reward = np.mean(rewards[-SMOOTH_REWARD_WINDOW:])\n",
    "\n",
    "        data = {'episode':update, 'num_frames':num_frames, 'smooth_reward':smooth_reward,\n",
    "               'reward':logs[\"return_per_episode\"], 'policy_loss':logs[\"policy_loss\"]}\n",
    "\n",
    "        if args.use_critic:\n",
    "            data['value_loss'] = logs[\"value_loss\"]\n",
    "\n",
    "        pd_logs.append(data)\n",
    "\n",
    "        pbar.set_postfix(data)\n",
    "\n",
    "        machine.add_r(rewards[-1])\n",
    "        if update % (20 * len(machines)) == 0 and update > 0:\n",
    "            full_rs.append([m.avg_r() for m in machines])\n",
    "            winner = survival_fittest(machines)\n",
    "            winners.append(winner)\n",
    "\n",
    "        # # Early terminate\n",
    "        # if smooth_reward >= args.score_threshold:\n",
    "        #     is_solved = True\n",
    "        #     break\n",
    "\n",
    "    if is_solved:\n",
    "        print('Solved!')\n",
    "\n",
    "    for m in machines:\n",
    "        print(m.avg_r())\n",
    "    print(winners)\n",
    "    print(full_rs)\n",
    "\n",
    "    return pd.DataFrame(pd_logs).set_index('episode')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run_experiment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m args \u001b[38;5;241m=\u001b[39m Config()\n\u001b[1;32m      2\u001b[0m env \u001b[38;5;241m=\u001b[39m DoorKeyEnv5x5()\n\u001b[0;32m----> 4\u001b[0m df_ppo \u001b[38;5;241m=\u001b[39m \u001b[43mrun_experiment\u001b[49m([\u001b[38;5;241m0.003\u001b[39m, \u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m0.03\u001b[39m], \u001b[38;5;241m200\u001b[39m, env, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m48\u001b[39m)\n\u001b[1;32m      6\u001b[0m df_ppo\u001b[38;5;241m.\u001b[39mplot(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_frames\u001b[39m\u001b[38;5;124m'\u001b[39m, y\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreward\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msmooth_reward\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'run_experiment' is not defined"
     ]
    }
   ],
   "source": [
    "args = Config()\n",
    "env = DoorKeyEnv5x5()\n",
    "\n",
    "df_ppo = run_experiment([0.003, 0.01, 0.03], 200, env, seed=48)\n",
    "\n",
    "df_ppo.plot(x='num_frames', y=['reward', 'smooth_reward'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_semi(max_steps:int=10000, env=DoorKeyEnv6x6(), args=Config(), seed=0, initial_model=None, bandit:ContinuousBanditV4=None):\n",
    "    \"\"\"\n",
    "    Upper level function for running experiments to analyze reinforce and\n",
    "    policy gradient methods. Instantiates a model, collects epxeriences, and\n",
    "    then updates the neccessary parameters.\n",
    "\n",
    "    Args:\n",
    "        coefs: a list of coefs each machine has\n",
    "        max_steps: maximum number of steps to run the experiment for.\n",
    "        env: the environment to use. \n",
    "        args: Config object with hyperparameters.\n",
    "        seed: random seed. int\n",
    "\n",
    "    Returns:\n",
    "        rs, smooth_rs\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    device = DEVICE\n",
    "\n",
    "    machine = Machine(0.01, 0, args=args)\n",
    "    if initial_model is not None:\n",
    "        machine.copy_model(initial_model)\n",
    "\n",
    "    is_solved = False\n",
    "\n",
    "    SMOOTH_REWARD_WINDOW = 50\n",
    "\n",
    "    pd_logs, rewards = [], [0]*SMOOTH_REWARD_WINDOW\n",
    "\n",
    "    total_rs, total_smooth_rs = [], []\n",
    "\n",
    "    num_frames = 0\n",
    "\n",
    "    pbar = tqdm(range(max_steps))\n",
    "    for update in pbar:\n",
    "        if bandit is not None:\n",
    "            pull = bandit.pull()\n",
    "            coef = np.exp(pull)\n",
    "            machine.coef = coef\n",
    "\n",
    "        exps, logs1 = machine.collect_experiences(env)\n",
    "\n",
    "        if update > 0 and bandit is not None:\n",
    "            bandit.update(total_rs[-1] + np.random.normal(0, 0.1))\n",
    "\n",
    "        logs2 = machine.update_parameters(exps)\n",
    "\n",
    "        logs = {**logs1, **logs2}\n",
    "\n",
    "        num_frames += logs[\"num_frames\"]\n",
    "\n",
    "        rewards.append(logs[\"return_per_episode\"])\n",
    "\n",
    "        smooth_reward = np.mean(rewards[-SMOOTH_REWARD_WINDOW:])\n",
    "\n",
    "        total_rs.append(logs[\"return_per_episode\"])\n",
    "        total_smooth_rs.append(smooth_reward)\n",
    "\n",
    "        data = {'episode':update, 'num_frames':num_frames, 'smooth_reward':smooth_reward,\n",
    "               'reward':logs[\"return_per_episode\"], 'policy_loss':logs[\"policy_loss\"]}\n",
    "\n",
    "        if args.use_critic:\n",
    "            data['value_loss'] = logs[\"value_loss\"]\n",
    "\n",
    "        pd_logs.append(data)\n",
    "\n",
    "        pbar.set_postfix(data)\n",
    "\n",
    "        # Early terminate\n",
    "        if smooth_reward >= args.score_threshold:\n",
    "            is_solved = True\n",
    "            break\n",
    "\n",
    "    if is_solved:\n",
    "        print('Solved!')\n",
    "\n",
    "    if bandit is not None:\n",
    "        bandit.draw()\n",
    "\n",
    "    return total_rs, total_smooth_rs\n",
    "\n",
    "def whole_pipeline(seed, bandit=ContinuousBanditV4(lb=-5.5, ub=-3.5), max_steps:int=10000, args=Config()):\n",
    "    set_random_seed(seed=seed)\n",
    "    env = DoorKeyEnv6x6()\n",
    "    env.reset()\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    frame = env.get_frame(highlight=env.highlight, tile_size=env.tile_size, agent_pov=env.agent_pov)\n",
    "    plt.imshow(frame)\n",
    "    plt.show()\n",
    "    initial_model = ACModel(7, use_critic=args.use_critic)\n",
    "\n",
    "    # first run PPO without bandit\n",
    "    _, smooth_rs_ppo = run_experiment_semi(max_steps=max_steps, env=env, args=args, seed=seed, initial_model=initial_model)\n",
    "    _, smooth_rs_bandit = run_experiment_semi(max_steps=max_steps, env=env, args=args, seed=seed, initial_model=initial_model, bandit=bandit)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(range(len(smooth_rs_ppo)), smooth_rs_ppo, label='PPO')\n",
    "    plt.plot(range(len(smooth_rs_bandit)), smooth_rs_bandit, label='Bandit')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 112/10000 [05:22<7:53:49,  2.88s/it, episode=111, num_frames=33354, smooth_reward=0, reward=0, policy_loss=-0.0195] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mwhole_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[31], line 97\u001b[0m, in \u001b[0;36mwhole_pipeline\u001b[0;34m(seed, bandit, max_steps, args)\u001b[0m\n\u001b[1;32m     94\u001b[0m initial_model \u001b[38;5;241m=\u001b[39m ACModel(\u001b[38;5;241m7\u001b[39m, use_critic\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39muse_critic)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# first run PPO without bandit\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m _, smooth_rs_ppo \u001b[38;5;241m=\u001b[39m \u001b[43mrun_experiment_semi\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m _, smooth_rs_bandit \u001b[38;5;241m=\u001b[39m run_experiment_semi(max_steps\u001b[38;5;241m=\u001b[39mmax_steps, env\u001b[38;5;241m=\u001b[39menv, args\u001b[38;5;241m=\u001b[39margs, seed\u001b[38;5;241m=\u001b[39mseed, initial_model\u001b[38;5;241m=\u001b[39minitial_model, bandit\u001b[38;5;241m=\u001b[39mbandit)\n\u001b[1;32m    100\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure()\n",
      "Cell \u001b[0;32mIn[31], line 44\u001b[0m, in \u001b[0;36mrun_experiment_semi\u001b[0;34m(max_steps, env, args, seed, initial_model, bandit)\u001b[0m\n\u001b[1;32m     41\u001b[0m     coef \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(pull)\n\u001b[1;32m     42\u001b[0m     machine\u001b[38;5;241m.\u001b[39mcoef \u001b[38;5;241m=\u001b[39m coef\n\u001b[0;32m---> 44\u001b[0m exps, logs1 \u001b[38;5;241m=\u001b[39m \u001b[43mmachine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_experiences\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m update \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m bandit \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m     bandit\u001b[38;5;241m.\u001b[39mupdate(total_rs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "Cell \u001b[0;32mIn[27], line 86\u001b[0m, in \u001b[0;36mMachine.collect_experiences\u001b[0;34m(self, env)\u001b[0m\n\u001b[1;32m     83\u001b[0m     dist, value \u001b[38;5;241m=\u001b[39m acmodel(preprocessed_obs)\n\u001b[1;32m     85\u001b[0m dist: Categorical\n\u001b[0;32m---> 86\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[43mdist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     88\u001b[0m obss[T] \u001b[38;5;241m=\u001b[39m obs\n\u001b[1;32m     89\u001b[0m obs, reward, done, _, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/leg/lib/python3.10/site-packages/torch/distributions/categorical.py:132\u001b[0m, in \u001b[0;36mCategorical.sample\u001b[0;34m(self, sample_shape)\u001b[0m\n\u001b[1;32m    130\u001b[0m     sample_shape \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mSize(sample_shape)\n\u001b[1;32m    131\u001b[0m probs_2d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprobs\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_events)\n\u001b[0;32m--> 132\u001b[0m samples_2d \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs_2d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_shape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m samples_2d\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extended_shape(sample_shape))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "whole_pipeline(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ningshanma/miniconda3/envs/rl/lib/python3.11/site-packages/gym/envs/registration.py:307: DeprecationWarning: The package name gym_minigrid has been deprecated in favor of minigrid. Please uninstall gym_minigrid and install minigrid with `pip install minigrid`. Future releases will be maintained under the new package name minigrid.\n",
      "  fn()\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib.axes import Axes\n",
    "from matplotlib import rc\n",
    "import random\n",
    "from torch import nn\n",
    "from gym.envs.registration import registry, register\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "np.bool = np.bool_\n",
    "\n",
    "rc('animation', html='jshtml')\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['animation.ffmpeg_path'] = '/path/to/ffmpeg'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartpoleDynamics:\n",
    "    def __init__(self,\n",
    "                 timestep=0.02,\n",
    "                 m_p=0.5,\n",
    "                 m_c=0.5,\n",
    "                 l=0.6,\n",
    "                 g=-9.81,\n",
    "                 u_range=15):\n",
    "        \"\"\"\n",
    "        Initializes the Cartpole Dynamics model with given parameters.\n",
    "\n",
    "        Parameters:\n",
    "        - timestep (float): The time step for the simulation.\n",
    "        - m_p (float): Mass of the pole.\n",
    "        - m_c (float): Mass of the cart.\n",
    "        - l (float): Length of the pole.\n",
    "        - g (float): Acceleration due to gravity. Negative values indicate direction.\n",
    "        - u_range (float): Range of the control input.\n",
    "        \"\"\"\n",
    "\n",
    "        self.m_p  = m_p\n",
    "        self.m_c  = m_c\n",
    "        self.l    = l\n",
    "        self.g    = -g\n",
    "        self.dt   = timestep\n",
    "\n",
    "        self.u_range = u_range\n",
    "\n",
    "        self.u_lb = torch.tensor([-1]).float()\n",
    "        self.u_ub = torch.tensor([1]).float()\n",
    "        self.q_shape = 4\n",
    "        self.u_shape = 1\n",
    "\n",
    "    def _qdotdot(self, q, u):\n",
    "        \"\"\"\n",
    "        Calculates the acceleration of both cart and pole as a function of the current state and control input.\n",
    "\n",
    "        Parameters:\n",
    "        - q (torch.Tensor): The current state of the system, [x, theta, xdot, thetadot].\n",
    "        - u (torch.Tensor): The current control input.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: Accelerations [x_dotdot, theta_dotdot] of the cart and the pole.\n",
    "        \"\"\"\n",
    "        x, theta, xdot, thetadot = q.T\n",
    "\n",
    "        if len(u.shape) == 2:\n",
    "            u = torch.flatten(u)\n",
    "\n",
    "        x_dotdot = (\n",
    "            u + self.m_p * torch.sin(theta) * (\n",
    "                self.l * torch.pow(thetadot,2) + self.g * torch.cos(theta)\n",
    "            )\n",
    "        ) / (self.m_c + self.m_p * torch.sin(theta)**2)\n",
    "\n",
    "        theta_dotdot = (\n",
    "            -u*torch.cos(theta) -\n",
    "            self.m_p * self.l * torch.pow(thetadot,2) * torch.cos(theta) * torch.sin(theta) -\n",
    "            (self.m_c + self.m_p) * self.g * torch.sin(theta)\n",
    "        ) / (self.l * (self.m_c + self.m_p * torch.sin(theta)**2))\n",
    "\n",
    "        return torch.stack((x_dotdot, theta_dotdot), dim=-1)\n",
    "\n",
    "    def _euler_int(self, q, qdotdot):\n",
    "        \"\"\"\n",
    "        Performs Euler integration to calculate the new state given the current state and accelerations.\n",
    "\n",
    "        Parameters:\n",
    "        - q (torch.Tensor): The current state of the system, [x, theta, xdot, thetadot].\n",
    "        - qdotdot (torch.Tensor): The accelerations [x_dotdot, theta_dotdot] of the cart and the pole.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: The new state of the system after a single time step.\n",
    "        \"\"\"\n",
    "\n",
    "        qdot_new = q[...,2:] + qdotdot * self.dt\n",
    "        q_new = q[...,:2] + self.dt * qdot_new\n",
    "\n",
    "        return torch.cat((q_new, qdot_new), dim=-1)\n",
    "\n",
    "    def step(self, q, u):\n",
    "        \"\"\"\n",
    "        Performs a single step of simulation given the current state and control input.\n",
    "\n",
    "        Parameters:\n",
    "        - q (torch.Tensor or np.ndarray): The current state of the system.\n",
    "        - u (torch.Tensor or np.ndarray): The current control input.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: The new state of the system after the step.\n",
    "        \"\"\"\n",
    "\n",
    "        # Check for numpy array\n",
    "        if isinstance(q, np.ndarray):\n",
    "            q = torch.from_numpy(q)\n",
    "        if isinstance(u, np.ndarray):\n",
    "            u = torch.from_numpy(u)\n",
    "\n",
    "        scaled_u = u * float(self.u_range)\n",
    "\n",
    "        # Check for shape issues\n",
    "        if len(q.shape) == 2:\n",
    "            q_dotdot = self._qdotdot(q, scaled_u)\n",
    "        elif len(q.shape) == 1:\n",
    "            q_dotdot = self._qdotdot(q.reshape(1,-1), scaled_u)\n",
    "        else:\n",
    "            raise RuntimeError('Invalid q shape')\n",
    "\n",
    "        new_q = self._euler_int(q, q_dotdot)\n",
    "\n",
    "        if len(q.shape) == 1:\n",
    "            new_q = new_q[0]\n",
    "\n",
    "        return new_q\n",
    "\n",
    "    # given q [bs, q_shape] and u [bs, t, u_shape] run the trajectories\n",
    "    def run_batch_of_trajectories(self, q, u):\n",
    "        \"\"\"\n",
    "        Simulates a batch of trajectories given initial states and control inputs over time.\n",
    "\n",
    "        Parameters:\n",
    "        - q (torch.Tensor): Initial states for each trajectory in the batch.\n",
    "        - u (torch.Tensor): Control inputs for each trajectory over time.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: The states of the system at each time step for each trajectory.\n",
    "        \"\"\"\n",
    "        qs = [q]\n",
    "\n",
    "        for t in range(u.shape[1]):\n",
    "            qs.append(self.step(qs[-1], u[:,t]))\n",
    "\n",
    "        return torch.stack(qs, dim=1)\n",
    "\n",
    "    # given q [bs, t, q_shape] and u [bs, t, u_shape] calculate the rewards\n",
    "    def reward(self, q, u):\n",
    "        \"\"\"\n",
    "        Calculates the reward for given states and control inputs.\n",
    "\n",
    "        Parameters:\n",
    "        - q (torch.Tensor or np.ndarray): States of the system.\n",
    "        - u (torch.Tensor or np.ndarray): Control inputs applied.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: The calculated rewards for the states and inputs.\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(q, np.ndarray):\n",
    "            q = torch.from_numpy(q)\n",
    "        if isinstance(u, np.ndarray):\n",
    "            u = torch.from_numpy(u)\n",
    "\n",
    "        angle_term = 0.5*(1-torch.cos(q[...,1]))\n",
    "        pos_term = -0.5*torch.pow(q[...,0],2)\n",
    "        ctrl_cost = -0.001*(u**2).sum(dim=-1)\n",
    "\n",
    "        return angle_term + pos_term + ctrl_cost\n",
    "\n",
    "class CartpoleGym(gym.Env):\n",
    "    def __init__(self, timestep_limit=200):\n",
    "        \"\"\"\n",
    "        Initializes the Cartpole environment with a specified time step limit.\n",
    "\n",
    "        Parameters:\n",
    "        - timestep_limit (int): The maximum number of timesteps for each episode.\n",
    "\n",
    "        Sets up the dynamics model and initializes the simulation state.\n",
    "        \"\"\"\n",
    "        self.dynamics = CartpoleDynamics()\n",
    "\n",
    "        self.timestep_limit = timestep_limit\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the environment to the initial state.\n",
    "\n",
    "        Returns:\n",
    "        - np.ndarray: The initial state of the environment.\n",
    "        \"\"\"\n",
    "\n",
    "        self.q_sim = np.zeros(4)\n",
    "        self.timesteps = 0\n",
    "\n",
    "        self.traj = [self.get_observation()]\n",
    "\n",
    "        return self.traj[-1]\n",
    "\n",
    "    def get_observation(self):\n",
    "        \"\"\"\n",
    "        Retrieves the current state of the environment.\n",
    "\n",
    "        Returns:\n",
    "        - np.ndarray: The current state of the simulation.\n",
    "        \"\"\"\n",
    "\n",
    "        return self.q_sim\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Executes one time step within the environment using the given action.\n",
    "\n",
    "        Parameters:\n",
    "        - action (np.ndarray): The action to apply for this timestep.\n",
    "\n",
    "        Returns:\n",
    "        - Tuple[np.ndarray, float, bool, dict]: A tuple containing the new state, the reward received,\n",
    "          a boolean indicating whether the episode is done, and an info dictionary.\n",
    "        \"\"\"\n",
    "        action = np.clip(action, self.action_space.low, self.action_space.high)[0]\n",
    "\n",
    "        new_q = self.dynamics.step(\n",
    "            self.q_sim, action\n",
    "        )\n",
    "\n",
    "        if not isinstance(action, torch.Tensor):\n",
    "            action = torch.tensor(action)\n",
    "\n",
    "        reward = self.dynamics.reward(\n",
    "            new_q, action\n",
    "        ).numpy()\n",
    "\n",
    "        self.q_sim = new_q.numpy()\n",
    "        done = self.is_done()\n",
    "\n",
    "        self.timesteps += 1\n",
    "\n",
    "        self.traj.append(self.q_sim)\n",
    "\n",
    "        return self.q_sim, reward, done, {}\n",
    "\n",
    "    def is_done(self):\n",
    "        \"\"\"\n",
    "        Checks if the episode has finished based on the timestep limit.\n",
    "\n",
    "        Returns:\n",
    "        - bool: True if the episode is finished, False otherwise.\n",
    "        \"\"\"\n",
    "        # Kill trial when too much time has passed\n",
    "        if self.timesteps >= self.timestep_limit:\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def plot_func(self, to_plot, i=None):\n",
    "        \"\"\"\n",
    "        Plots the current state of the cartpole system for visualization.\n",
    "\n",
    "        Parameters:\n",
    "        - to_plot (matplotlib.axes.Axes or dict): Axes for plotting or a dictionary of plot elements to update.\n",
    "        - i (int, optional): The index of the current state in the trajectory to plot.\n",
    "        \"\"\"\n",
    "        def _square(center_x, center_y, shape, angle):\n",
    "            trans_points = np.array([\n",
    "                [shape[0], shape[1]],\n",
    "                [-shape[0], shape[1]],\n",
    "                [-shape[0], -shape[1]],\n",
    "                [shape[0], -shape[1]],\n",
    "                [shape[0], shape[1]]\n",
    "            ]) @ np.array([\n",
    "                [np.cos(angle), np.sin(angle)],\n",
    "                [-np.sin(angle), np.cos(angle)]\n",
    "            ]) + np.array([center_x, center_y])\n",
    "\n",
    "            return trans_points[:, 0], trans_points[:, 1]\n",
    "\n",
    "        if isinstance(to_plot, Axes):\n",
    "            imgs = dict(\n",
    "                cart=to_plot.plot([], [], c=\"k\")[0],\n",
    "                pole=to_plot.plot([], [], c=\"k\", linewidth=5)[0],\n",
    "                center=to_plot.plot([], [], marker=\"o\", c=\"k\",\n",
    "                                          markersize=10)[0]\n",
    "            )\n",
    "\n",
    "            x_width = max(1,max(np.abs(t[0]) for t in self.traj) * 1.3)\n",
    "\n",
    "            # centerline\n",
    "            to_plot.plot(np.linspace(-x_width, x_width, num=50), np.zeros(50),\n",
    "                         c=\"k\", linestyle=\"dashed\")\n",
    "\n",
    "            # set axis\n",
    "            to_plot.set_xlim([-x_width, x_width])\n",
    "            to_plot.set_ylim([-self.dynamics.l*1.2, self.dynamics.l*1.2])\n",
    "\n",
    "            return imgs\n",
    "\n",
    "        curr_x = self.traj[i]\n",
    "\n",
    "        cart_size = (0.15, 0.1)\n",
    "\n",
    "        cart_x, cart_y = _square(curr_x[0], 0.,\n",
    "                                cart_size, 0.)\n",
    "\n",
    "        pole_x = np.array([curr_x[0], curr_x[0] + self.dynamics.l\n",
    "                           * np.cos(curr_x[1]-np.pi/2)])\n",
    "        pole_y = np.array([0., self.dynamics.l\n",
    "                           * np.sin(curr_x[1]-np.pi/2)])\n",
    "\n",
    "        to_plot[\"cart\"].set_data(cart_x, cart_y)\n",
    "        to_plot[\"pole\"].set_data(pole_x, pole_y)\n",
    "        to_plot[\"center\"].set_data(self.traj[i][0], 0.)\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        \"\"\"\n",
    "        Renders the current state of the environment using a matplotlib animation.\n",
    "\n",
    "        This function creates a matplotlib figure and uses the plot_func method to update the figure with the current\n",
    "        state of the cartpole system at each timestep. The animation is created with the FuncAnimation class and is\n",
    "        configured to play at a specified frame rate.\n",
    "\n",
    "        Parameters:\n",
    "        - mode (str): The mode for rendering. Currently, only \"human\" mode is supported, which displays the animation\n",
    "          on screen.\n",
    "\n",
    "        Returns:\n",
    "        - matplotlib.animation.FuncAnimation: The animation object that can be displayed in a Jupyter notebook or\n",
    "          saved to file.\n",
    "        \"\"\"\n",
    "        self.anim_fig = plt.figure()\n",
    "\n",
    "        self.axis = self.anim_fig.add_subplot(111)\n",
    "        self.axis.set_aspect('equal', adjustable='box')\n",
    "\n",
    "        imgs = self.plot_func(self.axis)\n",
    "        _update_img = lambda i: self.plot_func(imgs, i)\n",
    "\n",
    "        Writer = animation.writers['ffmpeg']\n",
    "        writer = Writer(fps=15, metadata=dict(artist='Me'), bitrate=1800)\n",
    "\n",
    "        ani = FuncAnimation(\n",
    "            self.anim_fig, _update_img, interval=self.dynamics.dt*1000,\n",
    "            frames=len(self.traj)-1\n",
    "        )\n",
    "\n",
    "        plt.close()\n",
    "\n",
    "        return ani\n",
    "\n",
    "    @property\n",
    "    def action_space(self):\n",
    "        \"\"\"\n",
    "        Defines the action space of the environment using a Box space from OpenAI Gym.\n",
    "\n",
    "        The action space is defined based on the lower and upper bounds for the control input specified in the\n",
    "        dynamics model. This allows for a continuous range of actions that can be applied to the cartpole system.\n",
    "\n",
    "        Returns:\n",
    "        - gym.spaces.Box: The action space as a Box object, with low and high bounds derived from the dynamics model's\n",
    "          control input bounds.\n",
    "        \"\"\"\n",
    "        return gym.spaces.Box(low=self.dynamics.u_lb.numpy(), high=self.dynamics.u_ub.numpy())\n",
    "\n",
    "    @property\n",
    "    def observation_space(self):\n",
    "        \"\"\"\n",
    "        Defines the observation space of the environment using a Box space from OpenAI Gym.\n",
    "\n",
    "        The observation space is defined with no bounds on the values, representing the position and velocity of the\n",
    "        cart and the angle and angular velocity of the pole. This space allows for any real-valued vector of\n",
    "        positions and velocities to be a valid observation in the environment.\n",
    "\n",
    "        Returns:\n",
    "        - gym.spaces.Box: The observation space as a Box object, with low and high bounds set to negative and\n",
    "          positive infinity, respectively, for each dimension of the state vector.\n",
    "        \"\"\"\n",
    "        return gym.spaces.Box(\n",
    "            low= np.array([-np.inf, -np.inf, -np.inf, -np.inf]),\n",
    "            high=np.array([np.inf,   np.inf,  np.inf,  np.inf])\n",
    "        )\n",
    "\n",
    "env_name = 'CartpoleSwingUp-v0'\n",
    "if env_name in registry:\n",
    "    del registry[env_name]\n",
    "register(\n",
    "    id=env_name,\n",
    "    entry_point=f'{__name__}:CartpoleGym',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Requested MovieWriter (ffmpeg) not available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m      6\u001b[0m     q, r, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample())\n\u001b[0;32m----> 8\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl/lib/python3.11/site-packages/gym/wrappers/order_enforcing.py:51\u001b[0m, in \u001b[0;36mOrderEnforcing.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_disable_render_order_enforcing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call `env.render()` before calling `env.reset()`, if this is a intended action, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     50\u001b[0m     )\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl/lib/python3.11/site-packages/gym/wrappers/env_checker.py:53\u001b[0m, in \u001b[0;36mPassiveEnvChecker.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_render \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_render \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43menv_render_passive_checker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mrender(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/rl/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:316\u001b[0m, in \u001b[0;36menv_render_passive_checker\u001b[0;34m(env, *args, **kwargs)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    311\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m env\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m env\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;129;01min\u001b[39;00m render_modes, (\n\u001b[1;32m    312\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe environment was initialized successfully however with an unsupported render mode. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    313\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRender mode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv\u001b[38;5;241m.\u001b[39mrender_mode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, modes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrender_modes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    314\u001b[0m         )\n\u001b[0;32m--> 316\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;66;03m# TODO: Check that the result is correct\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "Cell \u001b[0;32mIn[3], line 327\u001b[0m, in \u001b[0;36mCartpoleGym.render\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    324\u001b[0m imgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplot_func(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis)\n\u001b[1;32m    325\u001b[0m _update_img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m i: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplot_func(imgs, i)\n\u001b[0;32m--> 327\u001b[0m Writer \u001b[38;5;241m=\u001b[39m \u001b[43manimation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mffmpeg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    328\u001b[0m writer \u001b[38;5;241m=\u001b[39m Writer(fps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m, metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(artist\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMe\u001b[39m\u001b[38;5;124m'\u001b[39m), bitrate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1800\u001b[39m)\n\u001b[1;32m    330\u001b[0m ani \u001b[38;5;241m=\u001b[39m FuncAnimation(\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manim_fig, _update_img, interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdynamics\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m1000\u001b[39m,\n\u001b[1;32m    332\u001b[0m     frames\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraj)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    333\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/rl/lib/python3.11/site-packages/matplotlib/animation.py:148\u001b[0m, in \u001b[0;36mMovieWriterRegistry.__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_available(name):\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_registered[name]\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequested MovieWriter (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) not available\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Requested MovieWriter (ffmpeg) not available"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make('CartpoleSwingUp-v0')\n",
    "\n",
    "q = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    q, r, done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(1000):\n",
    "    # Collect experiences\n",
    "    obs, logs = machine.collect_experiences(env)\n",
    "    \n",
    "    # Compute returns\n",
    "    returns = compute_returns(obs['rewards'], gamma=0.99)\n",
    "    \n",
    "    # Update parameters\n",
    "    policy_loss, value_loss = machine.update_parameters(obs, actions, returns, log_probs)\n",
    "    \n",
    "    print(f\"Episode {episode}: Policy Loss: {policy_loss}, Value Loss: {value_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nTODO: to do list\\n1. Implement cartpole environment CART\\n2. Implement PPO algorithm on CART\\n3. Use 1 arm bandit to autoadjust a hyperparameter for PPO and see the result\\n    3.1 See if we can introduce entropy on CART, because CART has continous action space\\n    3.2 A way to compute entropy of state s is to randomly pick n actions,\\n            then treat these n actions as the whole action space\\n            then use original formula to compute entropy\\n4. Apply the evolutionary 3 machines (or more machines) algorithm to CART\\n\\nAn alternative path:\\n1'. Use multi-armed bandit to select machines\\n    1'.1 Implement multi-armed bandit algorithm\\n    1'.2 Use the multi-armed bandit to select machines to perform action\\n    1'.3 Use smooth reward to represent the performance of each machine\\n    1'.4 Smooth reward is updated using the formula: smooth_r = (1 - tau) * smooth_r + tau * new_r_we_get\\n\\nAnother path:\\n1. Same as 1\\n2''. Recall that in HW6, we use the compromise policy\\n    (choosing the mpc with the worst performance to select best action)\\n    2''.1 That is, let (r_ij) be the matrix of reward,\\n        where r_ij is the reward of machine j when it performs action a_i\\n    2''.2 The compromise algorithm first compute r_i = max_j r_ij for all i,\\n        then choose i_0 = argmin_i r_i as the worst machine,\\n        then choose j_1, j_2, ..., j_{num_of_elites} according to the column i_0 of r_ij\\n        i.e., j_k are the top {num_of_elites} actions that machine i_0 performs\\n    2''.3 Then use {a_{j_k}}_k to update the distribution's mean\\n3''. Instead of always choosing the worst machine, we want to do some adjustments\\n    3''.1 As before, let r_i = max_j r_ij\\n    3''.2 Then we normalize the sequence {r_i}_i to have mean 0 and variance 1,\\n        here r_i is the performance of machine i\\n    3''.3 Then we give a weight to each machine based on its performance,\\n        w_i = exp(beta * r_i), where beta is a hyperparameter\\n        and get a performance score for each action based on the following formula:\\n        performance(a_j) = sum_i w_i * r_ij\\n    3''.4 Finally, we select the best {num_of_elites} actions according to their performance score\\n    3''.5 Note that if beta = -inf, then we get the same result as the compromise algorithm\\n4''. Now there are some things to do\\n    4''.1 Manually adjust beta to see if we can improve the performance (it should be!),\\n    4''.2 OR use 1-armed bandit to autoadjust beta (it should be better than baseline!)\\n    4''.3 OR use the 3 machine algorithm to do this task\\n        This may take some thoughts, since you may need to modify some part of task\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "TODO: to do list\n",
    "1. Implement cartpole environment CART\n",
    "2. Implement PPO algorithm on CART\n",
    "3. Use 1 arm bandit to autoadjust a hyperparameter for PPO and see the result\n",
    "    3.1 See if we can introduce entropy on CART, because CART has continous action space\n",
    "    3.2 A way to compute entropy of state s is to randomly pick n actions,\n",
    "            then treat these n actions as the whole action space\n",
    "            then use original formula to compute entropy\n",
    "4. Apply the evolutionary 3 machines (or more machines) algorithm to CART\n",
    "\n",
    "An alternative path:\n",
    "1'. Use multi-armed bandit to select machines\n",
    "    1'.1 Implement multi-armed bandit algorithm\n",
    "    1'.2 Use the multi-armed bandit to select machines to perform action\n",
    "    1'.3 Use smooth reward to represent the performance of each machine\n",
    "    1'.4 Smooth reward is updated using the formula: smooth_r = (1 - tau) * smooth_r + tau * new_r_we_get\n",
    "\n",
    "Another path:\n",
    "1. Same as 1\n",
    "2''. Recall that in HW6, we use the compromise policy\n",
    "    (choosing the mpc with the worst performance to select best action)\n",
    "    2''.1 That is, let (r_ij) be the matrix of reward,\n",
    "        where r_ij is the reward of machine i when it performs action a_j\n",
    "    2''.2 The compromise algorithm first compute r_i = max_j r_ij for all i,\n",
    "        then choose i_0 = argmin_i r_i as the worst machine,\n",
    "        then choose j_1, j_2, ..., j_{num_of_elites} according to the column i_0 of r_ij\n",
    "        i.e., j_k are the top {num_of_elites} actions that machine i_0 performs\n",
    "    2''.3 Then use {a_{j_k}}_k to update the distribution's mean\n",
    "3''. Instead of always choosing the worst machine, we want to do some adjustments\n",
    "    3''.1 As before, let r_i = max_j r_ij\n",
    "    3''.2 Then we normalize the sequence {r_i}_i to have mean 0 and variance 1,\n",
    "        here r_i is the performance of machine i\n",
    "    3''.3 Then we give a weight to each machine based on its performance,\n",
    "        w_i = exp(beta * r_i), where beta is a hyperparameter\n",
    "        and get a performance score for each action based on the following formula:\n",
    "        performance(a_j) = sum_i w_i * r_ij\n",
    "    3''.4 Finally, we select the best {num_of_elites} actions according to their performance score\n",
    "    3''.5 Note that if beta = -inf, then we get the same result as the compromise algorithm\n",
    "4''. Now there are some things to do\n",
    "    4''.1 Manually adjust beta to see if we can improve the performance (it should be!),\n",
    "    4''.2 OR use 1-armed bandit to autoadjust beta (it should be better than baseline!)\n",
    "    4''.3 OR use the 3 machine algorithm to do this task\n",
    "        This may take some thoughts, since you may need to modify some part of task\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
