{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/atticusw/opt/anaconda3/lib/python3.9/site-packages/gym/envs/registration.py:307: DeprecationWarning: The package name gym_minigrid has been deprecated in favor of minigrid. Please uninstall gym_minigrid and install minigrid with `pip install minigrid`. Future releases will be maintained under the new package name minigrid.\n",
      "  fn()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "import torch.nn.functional as F\n",
    "import gym_minigrid\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from tqdm.notebook import tqdm\n",
    "from gym_minigrid.envs.doorkey import DoorKeyEnv\n",
    "from gym.envs.registration import registry, register\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from typing_extensions import Self\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# rc('animation', html='jshtml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "# elif torch.backends.mps.is_available():\n",
    "#     DEVICE = torch.device(\"mps\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "print(DEVICE)\n",
    "\n",
    "class Config:\n",
    "    \"\"\"\n",
    "    Stores algorithmic hyperparameters.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                score_threshold=0.93,\n",
    "                discount=0.995,\n",
    "                lr=1e-3,\n",
    "                max_grad_norm=0.5,\n",
    "                log_interval=10,\n",
    "                max_episodes=2000,\n",
    "                gae_lambda=0.95,\n",
    "                use_critic=True,\n",
    "                clip_ratio=0.2,\n",
    "                target_kl=0.01,\n",
    "                fit_badly_coeff=0.05,\n",
    "                train_ac_iters=5,\n",
    "                use_discounted_reward=False,\n",
    "                entropy_coef=0.01,\n",
    "                use_gae=True,\n",
    "                my_var=0):\n",
    "\n",
    "        self.score_threshold = score_threshold # criterion for early stopping. If the rolling average reward (over the last 100 episodes) is greater than it, it ends.\n",
    "        self.discount = discount # discount factor\n",
    "        self.lr = lr # learning rate\n",
    "        self.max_grad_norm = max_grad_norm # the maximum gradient norm (https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html)\n",
    "        self.log_interval = log_interval # logging interval\n",
    "        self.max_episodes = max_episodes # the maximum number of episodes.\n",
    "        self.use_critic = use_critic # whether to use critic or not.\n",
    "        self.clip_ratio = clip_ratio # clip_ratio of PPO.\n",
    "        self.target_kl = target_kl # target KL divergence for early stoping train_ac_iters for PPO\n",
    "        self.fit_badly_coeff = fit_badly_coeff # the bound of approx. KL of when the model fits badly to the data.\n",
    "        self.train_ac_iters = train_ac_iters # how many time to train ac_model using current computed old_logps\n",
    "        self.gae_lambda=gae_lambda # lambda in Generalized Advantage Estimation (GAE)\n",
    "        self.use_discounted_reward=use_discounted_reward # whether use discounted reward or not.\n",
    "        self.entropy_coef = entropy_coef # entropy coefficient for PPO\n",
    "        self.use_gae = use_gae # whether to use GAE or not.\n",
    "        self.my_var = my_var # a variable for testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function from https://github.com/ikostrikov/pytorch-a2c-ppo-acktr/blob/master/model.py\n",
    "def init_params(m):\n",
    "    \"\"\"\n",
    "    Initialize parameters of the network.\n",
    "    m: torch.nn.Module\n",
    "    \"\"\"\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Linear\") != -1:\n",
    "        m.weight.data.normal_(0, 1)\n",
    "        m.weight.data *= 1 / torch.sqrt(m.weight.data.pow(2).sum(1, keepdim=True))\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0)\n",
    "\n",
    "def preprocess_obss(obss, device=None):\n",
    "    \"\"\"\n",
    "    Convert observation into Torch.Tensor\n",
    "\n",
    "    Parameters\n",
    "    ----\n",
    "    obss: dictionary or np.ndarray\n",
    "    device: target device of torch.Tensor ('cpu', 'cuda')\n",
    "\n",
    "    Return\n",
    "    ----\n",
    "    Torch Tensor\n",
    "    \"\"\"\n",
    "    if isinstance(obss, dict):\n",
    "        images = np.array([obss[\"image\"]])\n",
    "    else:\n",
    "        images = np.array([o[\"image\"] for o in obss])\n",
    "\n",
    "    return torch.tensor(images, device=device, dtype=torch.float)\n",
    "\n",
    "class DoorKeyEnv5x5(DoorKeyEnv):\n",
    "    def __init__(self):\n",
    "        self.render_mode = \"rgb_array\"\n",
    "        super().__init__(size=5)\n",
    "\n",
    "    def _reward(self):\n",
    "        \"\"\"\n",
    "        Compute the reward to be given upon success\n",
    "        \"\"\"\n",
    "        return 1\n",
    "    \n",
    "class DoorKeyEnv6x6(DoorKeyEnv):\n",
    "    def __init__(self):\n",
    "        self.render_mode = \"rgb_array\"\n",
    "        super().__init__(size=6)\n",
    "\n",
    "    def _reward(self):\n",
    "        \"\"\"\n",
    "        Compute the reward to be given upon success\n",
    "        \"\"\"\n",
    "        return 1\n",
    "    \n",
    "class DoorKeyEnv8x8(DoorKeyEnv):\n",
    "    def __init__(self):\n",
    "        self.render_mode = \"rgb_array\"\n",
    "        super().__init__(size=8)\n",
    "\n",
    "    def _reward(self):\n",
    "        \"\"\"\n",
    "        Compute the reward to be given upon success\n",
    "        \"\"\"\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fbc89c167c0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = DoorKeyEnv6x6() # define environment.\n",
    "env.reset()\n",
    "action = env.action_space.sample()\n",
    "obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "# get an RGB image corresponding to the whole environment or the agent's point of view (https://github.com/Farama-Foundation/Minigrid/blob/master/minigrid/minigrid_env.py#L716)\n",
    "#            highlight (bool): If true, the agent's field of view or point of view is highlighted with a lighter gray color.\n",
    "#            tile_size (int): How many pixels will form a tile from the NxM grid.\n",
    "#            agent_pov (bool): If true, the rendered frame will only contain the point of view of the agent.\n",
    "frame = env.get_frame(highlight=env.highlight, tile_size=env.tile_size, agent_pov=env.agent_pov)\n",
    "# show an image to the notebook.\n",
    "plt.imshow(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACModel(nn.Module):\n",
    "    def __init__(self, num_actions, use_critic=True):\n",
    "        \"\"\"\n",
    "        Represents an Actor Crictic model that takes a 2d, multi-channeled\n",
    "        image as input.\n",
    "\n",
    "        Parameters\n",
    "        ----\n",
    "        num_actions : int\n",
    "\n",
    "                      The action space of the environment.\n",
    "                      The action space for DoorKey5x5 is 7-dimensional:\n",
    "                      0: turn left,\n",
    "                      1: turn right,\n",
    "                      2: forward,\n",
    "                      3: pickup an object,\n",
    "                      4: drop an object,\n",
    "                      5: activate an object,\n",
    "                      6: done completing task\n",
    "\n",
    "        use_critics : bool\n",
    "\n",
    "                      Critic network will be used in forward pass if flag is set\n",
    "                      to true.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.use_critic = use_critic\n",
    "\n",
    "        # Define actor's model\n",
    "        self.image_conv_actor = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, (2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 2)),\n",
    "            nn.Conv2d(16, 32, (2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, (2, 2)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, num_actions)\n",
    "        )\n",
    "\n",
    "        # Define critic's model\n",
    "        if self.use_critic:\n",
    "            self.image_conv_critic = nn.Sequential(\n",
    "                nn.Conv2d(3, 16, (2, 2)),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d((2, 2)),\n",
    "                nn.Conv2d(16, 32, (2, 2)),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(32, 64, (2, 2)),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "            self.critic = nn.Sequential(\n",
    "                nn.Linear(64, 64),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(64, 1)\n",
    "            )\n",
    "\n",
    "        # Initialize parameters correctly\n",
    "        self.apply(init_params)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        \"\"\"\n",
    "        Performs a forward pass through the actor-critic network\n",
    "\n",
    "        Parameters\n",
    "        ----\n",
    "        obs : int tensor. Shape [Batch size, ImWidth, ImHeight, Channels]\n",
    "\n",
    "              input to the network.\n",
    "        ----\n",
    "\n",
    "        returns:\n",
    "\n",
    "        dist : torch.distribution\n",
    "            The distribution of actions from policy. A Categorical distribution\n",
    "            for discreet action spaces.\n",
    "        value : torch.Tensor (Batch size, 1)\n",
    "            value output by critic network\n",
    "        \"\"\"\n",
    "        conv_in = obs.transpose(1, 3).transpose(2, 3) # reshape into [b, c, h, w]\n",
    "\n",
    "        dist, value = None, None\n",
    "\n",
    "        x = self.image_conv_actor(conv_in)\n",
    "        embedding = x.reshape(x.shape[0], -1)\n",
    "\n",
    "        x = self.actor(embedding)\n",
    "        dist = Categorical(logits=F.log_softmax(x, dim=1))\n",
    "\n",
    "        if self.use_critic:\n",
    "            y = self.image_conv_critic(conv_in)\n",
    "            embedding = y.reshape(y.shape[0], -1)\n",
    "\n",
    "            value = self.critic(embedding).squeeze(1)\n",
    "        else:\n",
    "            value = torch.zeros((x.shape[0], 1), device=x.device)\n",
    "        \n",
    "        return dist, value\n",
    "\n",
    "def compute_discounted_return(rewards, discount, device=DEVICE):\n",
    "    \"\"\"\n",
    "\t\trewards: reward obtained at timestep.  Shape: (T,)\n",
    "\t\tdiscount: discount factor. float\n",
    "\n",
    "    ----\n",
    "    returns: sum of discounted rewards. Shape: (T,)\n",
    "\t\t\"\"\"\n",
    "    returns = torch.zeros(*rewards.shape, device=device)\n",
    "\n",
    "    R = 0\n",
    "    for t in reversed(range((rewards.shape[0]))):\n",
    "        R = rewards[t] + discount * R\n",
    "        returns[t] = R\n",
    "    return returns\n",
    "\n",
    "def compute_advantage_gae(values, rewards, T, gae_lambda, discount):\n",
    "    \"\"\"\n",
    "    Compute Adavantage with GAE. See Section 4.4.2 in the lecture notes.\n",
    "\n",
    "    values: value at each timestep (T,)\n",
    "    rewards: reward obtained at each timestep.  Shape: (T,)\n",
    "    T: the number of frames, float\n",
    "    gae_lambda: hyperparameter, float\n",
    "    discount: discount factor, float\n",
    "\n",
    "    -----\n",
    "\n",
    "    returns:\n",
    "\n",
    "    advantages : tensor.float. Shape [T,]\n",
    "\n",
    "                 gae advantage term for timesteps 0 to T\n",
    "\n",
    "    \"\"\"\n",
    "    advantages = torch.zeros_like(values)\n",
    "    for i in reversed(range(T)):\n",
    "        next_value = values[i+1]\n",
    "        next_advantage = advantages[i+1]\n",
    "\n",
    "        delta = rewards[i] + discount * next_value  - values[i]\n",
    "        advantages[i] = delta + discount * gae_lambda * next_advantage\n",
    "    return advantages[:T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Categorical(logits: torch.Size([1, 7])),\n",
       " tensor([0.1054], grad_fn=<SqueezeBackward1>))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TESTS\n",
    "\n",
    "model = ACModel(num_actions=7, use_critic=True).to(DEVICE)\n",
    "obss = env.reset()\n",
    "obs = preprocess_obss(obss[0], DEVICE)\n",
    "model(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Machine:\n",
    "    def __init__(self, num_actions=7, args=Config()):\n",
    "        \"\"\"\n",
    "        A Machine object consists of a ACModel, its hyperparameters, and its history of rewards.\n",
    "\n",
    "        Args:\n",
    "            num_actions: size of action space\n",
    "            args: Model hyperparameters\n",
    "        \"\"\"\n",
    "        self.args = args\n",
    "        self.model = ACModel(num_actions, use_critic=True).to(DEVICE)\n",
    "        self.optim = torch.optim.Adam(self.model.parameters(), lr=self.args.lr)\n",
    "        self.rewards = []\n",
    "\n",
    "    def copy_machine(self, other: Self) -> None:\n",
    "        \"\"\"\n",
    "        Copy state dict from 'other'. Reset rs.\n",
    "        \"\"\"\n",
    "        state_dict = other.model.state_dict()\n",
    "        for key, v in state_dict.items():\n",
    "            self.model.state_dict()[key].copy_(v)\n",
    "        self.rewards = []\n",
    "\n",
    "    def avg_r(self) -> float:\n",
    "        assert len(self.rewards) > 0, 'No rewards yet'\n",
    "        return sum(self.rewards) / len(self.rewards)\n",
    "    \n",
    "    def collect_experiences(self, env:gym.Env):\n",
    "        \"\"\"\n",
    "        Collects rollouts and computes advantages.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        exps : dict\n",
    "            Contains actions, rewards, advantages etc as attributes.\n",
    "            Each attribute, e.g. `exps['reward']` has a shape\n",
    "            (self.num_frames, ...).\n",
    "        logs : dict\n",
    "            Useful stats about the training process, including the average\n",
    "            reward, policy loss, value loss, etc.\n",
    "        \"\"\"\n",
    "        acmodel = self.model\n",
    "\n",
    "        MAX_FRAMES_PER_EP = 300 # TODO: change this with different environments\n",
    "        shape = (MAX_FRAMES_PER_EP, )\n",
    "\n",
    "        actions = torch.zeros(*shape, device=DEVICE, dtype=torch.int)\n",
    "        values = torch.zeros(*shape, device=DEVICE)\n",
    "        rewards = torch.zeros(*shape, device=DEVICE)\n",
    "        log_probs = torch.zeros(*shape, device=DEVICE)\n",
    "        obss = [None]*MAX_FRAMES_PER_EP\n",
    "\n",
    "        obs, _ = env.reset()\n",
    "\n",
    "        total_return = 0\n",
    "\n",
    "        T = 0\n",
    "\n",
    "        while True:\n",
    "            # Do one agent-environment interaction\n",
    "\n",
    "            preprocessed_obs = preprocess_obss(obs, device=DEVICE)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                dist, value = acmodel(preprocessed_obs)\n",
    "\n",
    "            dist: Categorical\n",
    "            action = dist.sample()[0]\n",
    "\n",
    "            obss[T] = obs\n",
    "            obs, reward, done, _, _ = env.step(action.item())\n",
    "\n",
    "\n",
    "            # Update experiences values\n",
    "            actions[T] = action\n",
    "            values[T] = value\n",
    "            rewards[T] = reward\n",
    "            log_probs[T] = dist.log_prob(action)\n",
    "\n",
    "\n",
    "            total_return += reward\n",
    "            T += 1\n",
    "\n",
    "            if done or T>=MAX_FRAMES_PER_EP-1:\n",
    "                break\n",
    "\n",
    "        discounted_reward = compute_discounted_return(rewards[:T], self.args.discount, DEVICE)\n",
    "        exps = dict(\n",
    "            obs = preprocess_obss([\n",
    "                obss[i]\n",
    "                for i in range(T)\n",
    "            ], device=DEVICE),\n",
    "            action = actions[:T],\n",
    "            value  = values[:T],\n",
    "            reward = rewards[:T],\n",
    "            log_prob = log_probs[:T],\n",
    "            discounted_reward = discounted_reward,\n",
    "            T = T\n",
    "        )\n",
    "\n",
    "        logs = {\n",
    "            \"return_per_episode\": total_return,\n",
    "            \"num_frames\": T\n",
    "        }\n",
    "\n",
    "        return exps, logs\n",
    "\n",
    "    def _compute_policy_loss_ppo(self, obs, init_logp, old_logp, actions, advantages):\n",
    "        \"\"\"\n",
    "        Computes the policy loss for PPO.\n",
    "\n",
    "        obs: observeration to pass into acmodel. shape: (T,)\n",
    "        init_logp: log probabilities we get from the agent performing the action. shape: (T,)\n",
    "        old_logp: log probabilities from previous timestep. shape: (T,)\n",
    "        actions: action at this timestep. shape: (T,ImWidth,ImHeight,Channels)\n",
    "        advantages: the computed advantages. shape: (T,)\n",
    "\n",
    "        ---\n",
    "        Returns:\n",
    "\n",
    "        policy_loss: ppo policy loss as shown in line 6 of PPO alg. tensor.float. Shape (,1)\n",
    "        fit_badly: an approximation of kl divergence between init_logp and old_logp. tensor.float. Shape (,1)\n",
    "        approx_kl: an approximation of kl divergence between old_logp and new_logp. tensor.float. Shape (,1)\n",
    "        \"\"\"\n",
    "        policy_loss, approx_kl = 0, 0\n",
    "        dist, _ = self.model(obs)\n",
    "        dist: Categorical\n",
    "\n",
    "        entropy = dist.entropy()\n",
    "        logps = dist.log_prob(actions)\n",
    "        r_terms = torch.exp(logps - old_logp)\n",
    "        ppo_loss = torch.min(r_terms * advantages, torch.clamp(r_terms, 1 - self.args.clip_ratio, 1 + self.args.clip_ratio) * advantages)\n",
    "        \n",
    "        # Importance sampling factor\n",
    "        factors = torch.exp(old_logp - init_logp)\n",
    "        \n",
    "        indices = factors < 100\n",
    "\n",
    "        policy_loss_tensor = factors * ppo_loss + self.args.entropy_coef * entropy\n",
    "        policy_loss = - torch.mean(policy_loss_tensor[indices])\n",
    "\n",
    "        # approx_kl = torch.sum(torch.exp(old_logp) * (old_logp - logps)) / torch.sum(torch.exp(old_logp))\n",
    "        approx_kl = torch.mean((old_logp - logps) ** 2) / 2\n",
    "        fit_badly = torch.mean((old_logp - init_logp) ** 2)\n",
    "\n",
    "        return policy_loss, fit_badly, approx_kl\n",
    "    \n",
    "    def _compute_value_loss(self, obs, returns):\n",
    "        _, value = self.model(obs)\n",
    "        value_loss = torch.mean((value - returns) ** 2)\n",
    "        return value_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MachineCluster:\n",
    "    def __init__(self, hps:list[str], num_machines:int=3, num_survivors:int=1):\n",
    "        \"\"\"\n",
    "        hps: A list of hyperparameters to optimize. Must be a subset of keys in Machine.args.\n",
    "        num_machines: The number of machines in the cluster.\n",
    "        num_survivors: The number of survivors to select from the cluster.\n",
    "        \"\"\"\n",
    "        self.num_machines = num_machines\n",
    "        self.num_survivors = num_survivors\n",
    "        self.hps = hps\n",
    "        self.machines = []\n",
    "        for _ in range(self.num_machines):\n",
    "            self.machines.append(Machine())\n",
    "\n",
    "        # now let's say hps = ['entropy_coef']\n",
    "        for hp in hps:\n",
    "            # sample num_models values from a uniform distribution\n",
    "            values = np.random.uniform(0, 1, size=num_machines)\n",
    "            # TODO: Should it be other distributions?\n",
    "    \n",
    "            # set the hyperparameters of the models\n",
    "            self.update_hp(hp, values)\n",
    "\n",
    "    def update_hp(self, hp:str, targets:list[float]) -> None:\n",
    "        \"\"\"\n",
    "        Update the hyperparameters of the models in the cluster.\n",
    "\n",
    "        Args:\n",
    "            hp: The hyperparameter to update.\n",
    "            targets: The target hyperparameter values. Has length self.num_models.\n",
    "        \"\"\"\n",
    "        for i, target in enumerate(targets):\n",
    "            setattr(self.machines[i].args, hp, target)\n",
    "\n",
    "    def generate_data(self, num_trajs:int=32):\n",
    "        \"\"\"\n",
    "        Generate batches of data for training the models.\n",
    "\n",
    "        Return:\n",
    "            trajs: list of exps.  \n",
    "        \"\"\"\n",
    "\n",
    "        trajs = []\n",
    "\n",
    "        for i in range(self.num_machines):\n",
    "            for _ in range(num_trajs):\n",
    "                exps, _ = self.machines[i].collect_experiences(env)\n",
    "                trajs.append(exps)\n",
    "            \n",
    "            print(f'Generated data for machine {i}.')\n",
    "        \n",
    "        return trajs\n",
    "    \n",
    "    def train_one_step(self, data, batch_size=8):\n",
    "        \"\"\"\n",
    "        Do at most self.args.train_ac_iters step of optimization on each model in the cluster.\n",
    "\n",
    "        Args:\n",
    "            data: list of exps.\n",
    "            batch_size: number of trajectories used in each gradient step.\n",
    "\n",
    "        Return:\n",
    "            logs: list of length self.num_machines. Each element is a list of dictionaries containing policy_loss and value_loss.\n",
    "        \"\"\"\n",
    "        # TODO: actually do this\n",
    "\n",
    "        MAX_FRAMES_PER_EP = 300\n",
    "\n",
    "        logs = [[]] * self.num_machines\n",
    "        fit_badly_counter = False\n",
    "        \n",
    "        for machine_idx in range(self.num_machines):\n",
    "\n",
    "            machine = self.machines[machine_idx]\n",
    "\n",
    "            total_policy_loss = 0\n",
    "            total_value_loss = 0\n",
    "            fit_badly_count = 0\n",
    "\n",
    "            # select random subset of data\n",
    "            idx = np.random.choice(len(data), batch_size, replace=False)\n",
    "\n",
    "            for i in idx:\n",
    "                sb = data[i]\n",
    "                T = sb['T']\n",
    "                obs = sb['obs']\n",
    "                dist, values = machine.model(obs)\n",
    "                init_logp = sb['log_prob']\n",
    "                old_logp = dist.log_prob(sb['action']).detach()\n",
    "\n",
    "                # add 0 to end of values until it has length MAX_FRAMES_PER_EP\n",
    "                values_extended = torch.cat([values, torch.zeros((MAX_FRAMES_PER_EP - len(values), ), device=DEVICE)], dim=0)\n",
    "                full_reward = torch.cat([sb['reward'], torch.zeros((MAX_FRAMES_PER_EP - len(sb['reward']), ), device=DEVICE)], dim=0)\n",
    "                assert len(values_extended) == MAX_FRAMES_PER_EP\n",
    "                assert len(full_reward) == MAX_FRAMES_PER_EP\n",
    "                \n",
    "                # compute advantage\n",
    "                if machine.args.use_gae:\n",
    "                    advantage = compute_advantage_gae(values_extended, full_reward, T, machine.args.gae_lambda, machine.args.discount)\n",
    "                else:\n",
    "                    advantage = sb['discounted_reward'] - values.reshape(-1)\n",
    "                \n",
    "                # compute policy and value loss\n",
    "                policy_loss, fit_badly, approx_kl = machine._compute_policy_loss_ppo(obs, init_logp, old_logp, sb['action'], advantage)\n",
    "\n",
    "                value_loss = machine._compute_value_loss(obs, sb['discounted_reward'])\n",
    "                \n",
    "                if fit_badly < machine.args.fit_badly_coeff:\n",
    "                    total_policy_loss += policy_loss\n",
    "                    total_value_loss += value_loss\n",
    "                else:\n",
    "                    fit_badly_count += 1\n",
    "            \n",
    "            if random.random() < 0.1:\n",
    "                print(fit_badly_count / (batch_size * self.num_machines))\n",
    "            \n",
    "            machine.optim.zero_grad()\n",
    "            loss = torch.tensor(total_policy_loss + total_value_loss, device=DEVICE).requires_grad_()\n",
    "            loss.backward()\n",
    "            machine.optim.step()\n",
    "            \n",
    "            logs[machine_idx].append({\n",
    "            \"policy_loss\": total_policy_loss,\n",
    "            \"value_loss\": total_value_loss\n",
    "            })\n",
    "\n",
    "            if fit_badly_count / (batch_size * self.num_machines) > 0.3:\n",
    "                fit_badly_counter = True\n",
    "        \n",
    "        return logs, fit_badly_counter\n",
    "\n",
    "    def train(self, data, batch_size=8, max_steps=25):\n",
    "        \"\"\"\n",
    "        Train each model in the cluster until data fits poorly, or until reaches max_steps.\n",
    "\n",
    "        Return:\n",
    "        logs: list of length self.num_machines. Each element is a list of dictionaries containing policy_loss and value_loss.\n",
    "        \"\"\" \n",
    "\n",
    "        logs = [[]] * self.num_machines\n",
    "        steps = 0\n",
    "        fit_badly_counter = False\n",
    "    \n",
    "        while not fit_badly_counter:\n",
    "            if steps >= max_steps:\n",
    "                break\n",
    "            new_logs, fit_badly_counter = self.train_one_step(data, batch_size)\n",
    "            logs = [[*i, *j] for i, j in zip(logs, new_logs)]\n",
    "            steps += 1\n",
    "        \n",
    "        return logs\n",
    "        \n",
    "    def evaluate(self) -> list[float]:\n",
    "        \"\"\"\n",
    "        Evaluate the performance of the models in the cluster.\n",
    "\n",
    "        Args:\n",
    "        - trained_models: list of Machines.\n",
    "\n",
    "        Returns:\n",
    "        - performances: list of floats. The average return of each model.\n",
    "        \"\"\"\n",
    "\n",
    "        performances = []\n",
    "\n",
    "        for machine in self.machines:\n",
    "            total_rewards = 0\n",
    "            for i in range(10):\n",
    "                _, logs = machine.collect_experiences(env)\n",
    "                total_rewards += logs['return_per_episode']\n",
    "            machine.rewards.append(total_rewards / 10)\n",
    "            performances.append(total_rewards / 10)\n",
    "\n",
    "        return performances\n",
    "        \n",
    "    def reproduce(self, performances) -> None:\n",
    "        \"\"\"\n",
    "        Select and reproduce the survivors.\n",
    "        Also shift hyperparams.\n",
    "        \"\"\"\n",
    "        # TODO: shift hyperparams\n",
    "\n",
    "        survivors = torch.topk(torch.tensor(performances, device=DEVICE), self.num_survivors).indices\n",
    "        logits = torch.tensor([performances[x] if x in survivors else float('-inf') for x in range(self.num_machines)])\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "\n",
    "        for i in range(self.num_machines):\n",
    "            if i not in survivors:\n",
    "                self.machines[i].copy_machine(self.machines[dist.sample().item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cluster_experiment(hps:list[str], \n",
    "                           num_machines:int=3, \n",
    "                           num_survivors:int=1, \n",
    "                           num_trajs:int=32, \n",
    "                           num_epochs:int=10,\n",
    "                           batch_size:int=8,\n",
    "                           max_steps:int=25, \n",
    "                           seed=42):\n",
    "    \"\"\"\n",
    "    Run the cluster experiment.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    cluster = MachineCluster(hps = hps, num_machines = num_machines, num_survivors = num_survivors)\n",
    "\n",
    "    pbar = tqdm(range(num_epochs))\n",
    "    logs = [[]] * num_machines\n",
    "\n",
    "    for epoch in pbar:\n",
    "        data = cluster.generate_data(num_trajs=num_trajs)\n",
    "        _ = cluster.train(data=data, batch_size=batch_size, max_steps=max_steps)\n",
    "        performances = cluster.evaluate()\n",
    "        assert len(performances) == num_machines\n",
    "        logs = [[*i, j] for i, j in zip(logs, performances)]\n",
    "        cluster.reproduce(performances)\n",
    "\n",
    "        print(f'Epoch {epoch} done!')\n",
    "\n",
    "    # return pd.DataFrame(pdlogs).set_index('epoch')\n",
    "    return logs, cluster.machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated data for machine 0.\n",
      "Generated data for machine 1.\n",
      "Generated data for machine 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8m/r3kmm3xs1lngf9bhwg216mlc0000gn/T/ipykernel_6169/360898798.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss = torch.tensor(total_policy_loss + total_value_loss, device=DEVICE).requires_grad_()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n",
      "0.20833333333333334\n",
      "0.16666666666666666\n",
      "0.0\n",
      "0.125\n",
      "0.20833333333333334\n",
      "0.16666666666666666\n",
      "0.16666666666666666\n",
      "0.20833333333333334\n",
      "0.16666666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [01:18<11:43, 78.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 done!\n",
      "Generated data for machine 0.\n",
      "Generated data for machine 1.\n",
      "Generated data for machine 2.\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [02:39<10:40, 80.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 done!\n",
      "Generated data for machine 0.\n",
      "Generated data for machine 1.\n",
      "Generated data for machine 2.\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [03:59<09:20, 80.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 done!\n",
      "Generated data for machine 0.\n",
      "Generated data for machine 1.\n",
      "Generated data for machine 2.\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [05:25<08:14, 82.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 done!\n",
      "Generated data for machine 0.\n",
      "Generated data for machine 1.\n",
      "Generated data for machine 2.\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [07:01<07:17, 87.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 done!\n",
      "Generated data for machine 0.\n",
      "Generated data for machine 1.\n",
      "Generated data for machine 2.\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [08:41<06:06, 91.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 done!\n",
      "Generated data for machine 0.\n",
      "Generated data for machine 1.\n",
      "Generated data for machine 2.\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [10:13<04:34, 91.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 done!\n",
      "Generated data for machine 0.\n",
      "Generated data for machine 1.\n",
      "Generated data for machine 2.\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [11:41<03:01, 90.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 done!\n",
      "Generated data for machine 0.\n",
      "Generated data for machine 1.\n",
      "Generated data for machine 2.\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [13:09<01:29, 89.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 done!\n",
      "Generated data for machine 0.\n",
      "Generated data for machine 1.\n",
      "Generated data for machine 2.\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [14:35<00:00, 87.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df, trained_machines = run_cluster_experiment(hps=['entropy_coef'])\n",
    "\n",
    "# df.plot(x='num_frames', y=['reward', 'smooth_reward'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.1, 0.2, 0.0, 0.2], [0.0, 0.0, 0.1, 0.1, 0.1, 0.0, 0.0, 0.0, 0.1, 0.1], [0.0, 0.0, 0.2, 0.1, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0]]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(coefs:list[float], max_steps:int, env=DoorKeyEnv6x6(), args=Config(), seed=0):\n",
    "    \"\"\"\n",
    "    Upper level function for running experiments to analyze reinforce and\n",
    "    policy gradient methods. Instantiates a model, collects epxeriences, and\n",
    "    then updates the neccessary parameters.\n",
    "\n",
    "    Args:\n",
    "        coefs: a list of coefs each machine has\n",
    "        max_steps: maximum number of steps to run the experiment for.\n",
    "        env: the environment to use. \n",
    "        args: Config object with hyperparameters.\n",
    "        seed: random seed. int\n",
    "\n",
    "    Returns:\n",
    "        DataFrame indexed by episode\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    device = DEVICE\n",
    "\n",
    "    machines = [Machine(coef, label=idx, args=args) for idx, coef in enumerate(coefs)]\n",
    "    winners = []\n",
    "    full_rs = []\n",
    "\n",
    "    is_solved = False\n",
    "\n",
    "    SMOOTH_REWARD_WINDOW = 50\n",
    "\n",
    "    pd_logs, rewards = [], [0]*SMOOTH_REWARD_WINDOW\n",
    "\n",
    "    num_frames = 0\n",
    "\n",
    "    pbar = tqdm(range(max_steps))\n",
    "    for update in pbar:\n",
    "\n",
    "        label = update % len(machines)\n",
    "\n",
    "        machine = machines[label]\n",
    "        \n",
    "        exps, logs1 = machine.collect_experiences(env)\n",
    "\n",
    "        for m in machines:\n",
    "            if m is machine:\n",
    "                logs2 = m.update_parameters(exps)\n",
    "            else:\n",
    "                m.update_parameters(exps)\n",
    "\n",
    "        logs = {**logs1, **logs2}\n",
    "\n",
    "        num_frames += logs[\"num_frames\"]\n",
    "\n",
    "        rewards.append(logs[\"return_per_episode\"])\n",
    "\n",
    "        smooth_reward = np.mean(rewards[-SMOOTH_REWARD_WINDOW:])\n",
    "\n",
    "        data = {'episode':update, 'num_frames':num_frames, 'smooth_reward':smooth_reward,\n",
    "               'reward':logs[\"return_per_episode\"], 'policy_loss':logs[\"policy_loss\"]}\n",
    "\n",
    "        if args.use_critic:\n",
    "            data['value_loss'] = logs[\"value_loss\"]\n",
    "\n",
    "        pd_logs.append(data)\n",
    "\n",
    "        pbar.set_postfix(data)\n",
    "\n",
    "        machine.add_r(rewards[-1])\n",
    "        if update % (20 * len(machines)) == 0 and update > 0:\n",
    "            full_rs.append([m.avg_r() for m in machines])\n",
    "            winner = survival_fittest(machines)\n",
    "            winners.append(winner)\n",
    "\n",
    "        # # Early terminate\n",
    "        # if smooth_reward >= args.score_threshold:\n",
    "        #     is_solved = True\n",
    "        #     break\n",
    "\n",
    "    if is_solved:\n",
    "        print('Solved!')\n",
    "\n",
    "    for m in machines:\n",
    "        print(m.avg_r())\n",
    "    print(winners)\n",
    "    print(full_rs)\n",
    "\n",
    "    return pd.DataFrame(pd_logs).set_index('episode')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'registry' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 372\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m gym\u001b[38;5;241m.\u001b[39mspaces\u001b[38;5;241m.\u001b[39mBox(\n\u001b[1;32m    367\u001b[0m             low\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf, \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf, \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf, \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf]),\n\u001b[1;32m    368\u001b[0m             high\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray([np\u001b[38;5;241m.\u001b[39minf,   np\u001b[38;5;241m.\u001b[39minf,  np\u001b[38;5;241m.\u001b[39minf,  np\u001b[38;5;241m.\u001b[39minf])\n\u001b[1;32m    369\u001b[0m         )\n\u001b[1;32m    371\u001b[0m env_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCartpoleSwingUp-v0\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m env_name \u001b[38;5;129;01min\u001b[39;00m \u001b[43mregistry\u001b[49m\u001b[38;5;241m.\u001b[39menv_specs:\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m registry\u001b[38;5;241m.\u001b[39menv_specs[env_name]\n\u001b[1;32m    374\u001b[0m register(\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39menv_name,\n\u001b[1;32m    376\u001b[0m     entry_point\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:CartpoleGym\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    377\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'registry' is not defined"
     ]
    }
   ],
   "source": [
    "class CartpoleDynamics:\n",
    "    def __init__(self,\n",
    "                 timestep=0.02,\n",
    "                 m_p=0.5,\n",
    "                 m_c=0.5,\n",
    "                 l=0.6,\n",
    "                 g=-9.81,\n",
    "                 u_range=15):\n",
    "        \"\"\"\n",
    "        Initializes the Cartpole Dynamics model with given parameters.\n",
    "\n",
    "        Parameters:\n",
    "        - timestep (float): The time step for the simulation.\n",
    "        - m_p (float): Mass of the pole.\n",
    "        - m_c (float): Mass of the cart.\n",
    "        - l (float): Length of the pole.\n",
    "        - g (float): Acceleration due to gravity. Negative values indicate direction.\n",
    "        - u_range (float): Range of the control input.\n",
    "        \"\"\"\n",
    "\n",
    "        self.m_p  = m_p\n",
    "        self.m_c  = m_c\n",
    "        self.l    = l\n",
    "        self.g    = -g\n",
    "        self.dt   = timestep\n",
    "\n",
    "        self.u_range = u_range\n",
    "\n",
    "        self.u_lb = torch.tensor([-1]).float()\n",
    "        self.u_ub = torch.tensor([1]).float()\n",
    "        self.q_shape = 4\n",
    "        self.u_shape = 1\n",
    "\n",
    "    def _qdotdot(self, q, u):\n",
    "        \"\"\"\n",
    "        Calculates the acceleration of both cart and pole as a function of the current state and control input.\n",
    "\n",
    "        Parameters:\n",
    "        - q (torch.Tensor): The current state of the system, [x, theta, xdot, thetadot].\n",
    "        - u (torch.Tensor): The current control input.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: Accelerations [x_dotdot, theta_dotdot] of the cart and the pole.\n",
    "        \"\"\"\n",
    "        x, theta, xdot, thetadot = q.T\n",
    "\n",
    "        if len(u.shape) == 2:\n",
    "            u = torch.flatten(u)\n",
    "\n",
    "        x_dotdot = (\n",
    "            u + self.m_p * torch.sin(theta) * (\n",
    "                self.l * torch.pow(thetadot,2) + self.g * torch.cos(theta)\n",
    "            )\n",
    "        ) / (self.m_c + self.m_p * torch.sin(theta)**2)\n",
    "\n",
    "        theta_dotdot = (\n",
    "            -u*torch.cos(theta) -\n",
    "            self.m_p * self.l * torch.pow(thetadot,2) * torch.cos(theta) * torch.sin(theta) -\n",
    "            (self.m_c + self.m_p) * self.g * torch.sin(theta)\n",
    "        ) / (self.l * (self.m_c + self.m_p * torch.sin(theta)**2))\n",
    "\n",
    "        return torch.stack((x_dotdot, theta_dotdot), dim=-1)\n",
    "\n",
    "    def _euler_int(self, q, qdotdot):\n",
    "        \"\"\"\n",
    "        Performs Euler integration to calculate the new state given the current state and accelerations.\n",
    "\n",
    "        Parameters:\n",
    "        - q (torch.Tensor): The current state of the system, [x, theta, xdot, thetadot].\n",
    "        - qdotdot (torch.Tensor): The accelerations [x_dotdot, theta_dotdot] of the cart and the pole.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: The new state of the system after a single time step.\n",
    "        \"\"\"\n",
    "\n",
    "        qdot_new = q[...,2:] + qdotdot * self.dt\n",
    "        q_new = q[...,:2] + self.dt * qdot_new\n",
    "\n",
    "        return torch.cat((q_new, qdot_new), dim=-1)\n",
    "\n",
    "    def step(self, q, u):\n",
    "        \"\"\"\n",
    "        Performs a single step of simulation given the current state and control input.\n",
    "\n",
    "        Parameters:\n",
    "        - q (torch.Tensor or np.ndarray): The current state of the system.\n",
    "        - u (torch.Tensor or np.ndarray): The current control input.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: The new state of the system after the step.\n",
    "        \"\"\"\n",
    "\n",
    "        # Check for numpy array\n",
    "        if isinstance(q, np.ndarray):\n",
    "            q = torch.from_numpy(q)\n",
    "        if isinstance(u, np.ndarray):\n",
    "            u = torch.from_numpy(u)\n",
    "\n",
    "        scaled_u = u * float(self.u_range)\n",
    "\n",
    "        # Check for shape issues\n",
    "        if len(q.shape) == 2:\n",
    "            q_dotdot = self._qdotdot(q, scaled_u)\n",
    "        elif len(q.shape) == 1:\n",
    "            q_dotdot = self._qdotdot(q.reshape(1,-1), scaled_u)\n",
    "        else:\n",
    "            raise RuntimeError('Invalid q shape')\n",
    "\n",
    "        new_q = self._euler_int(q, q_dotdot)\n",
    "\n",
    "        if len(q.shape) == 1:\n",
    "            new_q = new_q[0]\n",
    "\n",
    "        return new_q\n",
    "\n",
    "    # given q [bs, q_shape] and u [bs, t, u_shape] run the trajectories\n",
    "    def run_batch_of_trajectories(self, q, u):\n",
    "        \"\"\"\n",
    "        Simulates a batch of trajectories given initial states and control inputs over time.\n",
    "\n",
    "        Parameters:\n",
    "        - q (torch.Tensor): Initial states for each trajectory in the batch.\n",
    "        - u (torch.Tensor): Control inputs for each trajectory over time.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: The states of the system at each time step for each trajectory.\n",
    "        \"\"\"\n",
    "        qs = [q]\n",
    "\n",
    "        for t in range(u.shape[1]):\n",
    "            qs.append(self.step(qs[-1], u[:,t]))\n",
    "\n",
    "        return torch.stack(qs, dim=1)\n",
    "\n",
    "    # given q [bs, t, q_shape] and u [bs, t, u_shape] calculate the rewards\n",
    "    def reward(self, q, u):\n",
    "        \"\"\"\n",
    "        Calculates the reward for given states and control inputs.\n",
    "\n",
    "        Parameters:\n",
    "        - q (torch.Tensor or np.ndarray): States of the system.\n",
    "        - u (torch.Tensor or np.ndarray): Control inputs applied.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: The calculated rewards for the states and inputs.\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(q, np.ndarray):\n",
    "            q = torch.from_numpy(q)\n",
    "        if isinstance(u, np.ndarray):\n",
    "            u = torch.from_numpy(u)\n",
    "\n",
    "        angle_term = 0.5*(1-torch.cos(q[...,1]))\n",
    "        pos_term = -0.5*torch.pow(q[...,0],2)\n",
    "        ctrl_cost = -0.001*(u**2).sum(dim=-1)\n",
    "\n",
    "        return angle_term + pos_term + ctrl_cost\n",
    "\n",
    "class CartpoleGym(gym.Env):\n",
    "    def __init__(self, timestep_limit=200):\n",
    "        \"\"\"\n",
    "        Initializes the Cartpole environment with a specified time step limit.\n",
    "\n",
    "        Parameters:\n",
    "        - timestep_limit (int): The maximum number of timesteps for each episode.\n",
    "\n",
    "        Sets up the dynamics model and initializes the simulation state.\n",
    "        \"\"\"\n",
    "        self.dynamics = CartpoleDynamics()\n",
    "\n",
    "        self.timestep_limit = timestep_limit\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the environment to the initial state.\n",
    "\n",
    "        Returns:\n",
    "        - np.ndarray: The initial state of the environment.\n",
    "        \"\"\"\n",
    "\n",
    "        self.q_sim = np.zeros(4)\n",
    "        self.timesteps = 0\n",
    "\n",
    "        self.traj = [self.get_observation()]\n",
    "\n",
    "        return self.traj[-1]\n",
    "\n",
    "    def get_observation(self):\n",
    "        \"\"\"\n",
    "        Retrieves the current state of the environment.\n",
    "\n",
    "        Returns:\n",
    "        - np.ndarray: The current state of the simulation.\n",
    "        \"\"\"\n",
    "\n",
    "        return self.q_sim\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Executes one time step within the environment using the given action.\n",
    "\n",
    "        Parameters:\n",
    "        - action (np.ndarray): The action to apply for this timestep.\n",
    "\n",
    "        Returns:\n",
    "        - Tuple[np.ndarray, float, bool, dict]: A tuple containing the new state, the reward received,\n",
    "          a boolean indicating whether the episode is done, and an info dictionary.\n",
    "        \"\"\"\n",
    "        action = np.clip(action, self.action_space.low, self.action_space.high)[0]\n",
    "\n",
    "        new_q = self.dynamics.step(\n",
    "            self.q_sim, action\n",
    "        )\n",
    "\n",
    "        if not isinstance(action, torch.Tensor):\n",
    "            action = torch.tensor(action)\n",
    "\n",
    "        reward = self.dynamics.reward(\n",
    "            new_q, action\n",
    "        ).numpy()\n",
    "\n",
    "        self.q_sim = new_q.numpy()\n",
    "        done = self.is_done()\n",
    "\n",
    "        self.timesteps += 1\n",
    "\n",
    "        self.traj.append(self.q_sim)\n",
    "\n",
    "        return self.q_sim, reward, done, {}\n",
    "\n",
    "    def is_done(self):\n",
    "        \"\"\"\n",
    "        Checks if the episode has finished based on the timestep limit.\n",
    "\n",
    "        Returns:\n",
    "        - bool: True if the episode is finished, False otherwise.\n",
    "        \"\"\"\n",
    "        # Kill trial when too much time has passed\n",
    "        if self.timesteps >= self.timestep_limit:\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def plot_func(self, to_plot, i=None):\n",
    "        \"\"\"\n",
    "        Plots the current state of the cartpole system for visualization.\n",
    "\n",
    "        Parameters:\n",
    "        - to_plot (matplotlib.axes.Axes or dict): Axes for plotting or a dictionary of plot elements to update.\n",
    "        - i (int, optional): The index of the current state in the trajectory to plot.\n",
    "        \"\"\"\n",
    "        def _square(center_x, center_y, shape, angle):\n",
    "            trans_points = np.array([\n",
    "                [shape[0], shape[1]],\n",
    "                [-shape[0], shape[1]],\n",
    "                [-shape[0], -shape[1]],\n",
    "                [shape[0], -shape[1]],\n",
    "                [shape[0], shape[1]]\n",
    "            ]) @ np.array([\n",
    "                [np.cos(angle), np.sin(angle)],\n",
    "                [-np.sin(angle), np.cos(angle)]\n",
    "            ]) + np.array([center_x, center_y])\n",
    "\n",
    "            return trans_points[:, 0], trans_points[:, 1]\n",
    "\n",
    "        if isinstance(to_plot, Axes):\n",
    "            imgs = dict(\n",
    "                cart=to_plot.plot([], [], c=\"k\")[0],\n",
    "                pole=to_plot.plot([], [], c=\"k\", linewidth=5)[0],\n",
    "                center=to_plot.plot([], [], marker=\"o\", c=\"k\",\n",
    "                                          markersize=10)[0]\n",
    "            )\n",
    "\n",
    "            x_width = max(1,max(np.abs(t[0]) for t in self.traj) * 1.3)\n",
    "\n",
    "            # centerline\n",
    "            to_plot.plot(np.linspace(-x_width, x_width, num=50), np.zeros(50),\n",
    "                         c=\"k\", linestyle=\"dashed\")\n",
    "\n",
    "            # set axis\n",
    "            to_plot.set_xlim([-x_width, x_width])\n",
    "            to_plot.set_ylim([-self.dynamics.l*1.2, self.dynamics.l*1.2])\n",
    "\n",
    "            return imgs\n",
    "\n",
    "        curr_x = self.traj[i]\n",
    "\n",
    "        cart_size = (0.15, 0.1)\n",
    "\n",
    "        cart_x, cart_y = _square(curr_x[0], 0.,\n",
    "                                cart_size, 0.)\n",
    "\n",
    "        pole_x = np.array([curr_x[0], curr_x[0] + self.dynamics.l\n",
    "                           * np.cos(curr_x[1]-np.pi/2)])\n",
    "        pole_y = np.array([0., self.dynamics.l\n",
    "                           * np.sin(curr_x[1]-np.pi/2)])\n",
    "\n",
    "        to_plot[\"cart\"].set_data(cart_x, cart_y)\n",
    "        to_plot[\"pole\"].set_data(pole_x, pole_y)\n",
    "        to_plot[\"center\"].set_data(self.traj[i][0], 0.)\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        \"\"\"\n",
    "        Renders the current state of the environment using a matplotlib animation.\n",
    "\n",
    "        This function creates a matplotlib figure and uses the plot_func method to update the figure with the current\n",
    "        state of the cartpole system at each timestep. The animation is created with the FuncAnimation class and is\n",
    "        configured to play at a specified frame rate.\n",
    "\n",
    "        Parameters:\n",
    "        - mode (str): The mode for rendering. Currently, only \"human\" mode is supported, which displays the animation\n",
    "          on screen.\n",
    "\n",
    "        Returns:\n",
    "        - matplotlib.animation.FuncAnimation: The animation object that can be displayed in a Jupyter notebook or\n",
    "          saved to file.\n",
    "        \"\"\"\n",
    "        self.anim_fig = plt.figure()\n",
    "\n",
    "        self.axis = self.anim_fig.add_subplot(111)\n",
    "        self.axis.set_aspect('equal', adjustable='box')\n",
    "\n",
    "        imgs = self.plot_func(self.axis)\n",
    "        _update_img = lambda i: self.plot_func(imgs, i)\n",
    "\n",
    "        Writer = animation.writers['ffmpeg']\n",
    "        writer = Writer(fps=15, metadata=dict(artist='Me'), bitrate=1800)\n",
    "\n",
    "        ani = FuncAnimation(\n",
    "            self.anim_fig, _update_img, interval=self.dynamics.dt*1000,\n",
    "            frames=len(self.traj)-1\n",
    "        )\n",
    "\n",
    "        plt.close()\n",
    "\n",
    "        return ani\n",
    "\n",
    "    @property\n",
    "    def action_space(self):\n",
    "        \"\"\"\n",
    "        Defines the action space of the environment using a Box space from OpenAI Gym.\n",
    "\n",
    "        The action space is defined based on the lower and upper bounds for the control input specified in the\n",
    "        dynamics model. This allows for a continuous range of actions that can be applied to the cartpole system.\n",
    "\n",
    "        Returns:\n",
    "        - gym.spaces.Box: The action space as a Box object, with low and high bounds derived from the dynamics model's\n",
    "          control input bounds.\n",
    "        \"\"\"\n",
    "        return gym.spaces.Box(low=self.dynamics.u_lb.numpy(), high=self.dynamics.u_ub.numpy())\n",
    "\n",
    "    @property\n",
    "    def observation_space(self):\n",
    "        \"\"\"\n",
    "        Defines the observation space of the environment using a Box space from OpenAI Gym.\n",
    "\n",
    "        The observation space is defined with no bounds on the values, representing the position and velocity of the\n",
    "        cart and the angle and angular velocity of the pole. This space allows for any real-valued vector of\n",
    "        positions and velocities to be a valid observation in the environment.\n",
    "\n",
    "        Returns:\n",
    "        - gym.spaces.Box: The observation space as a Box object, with low and high bounds set to negative and\n",
    "          positive infinity, respectively, for each dimension of the state vector.\n",
    "        \"\"\"\n",
    "        return gym.spaces.Box(\n",
    "            low= np.array([-np.inf, -np.inf, -np.inf, -np.inf]),\n",
    "            high=np.array([np.inf,   np.inf,  np.inf,  np.inf])\n",
    "        )\n",
    "\n",
    "env_name = 'CartpoleSwingUp-v0'\n",
    "if env_name in registry.env_specs:\n",
    "    del registry.env_specs[env_name]\n",
    "register(\n",
    "    id=env_name,\n",
    "    entry_point=f'{__name__}:CartpoleGym',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameNotFound",
     "evalue": "Environment CartpoleSwingUp doesn't exist. Did you mean: `CartPole`?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameNotFound\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCartpoleSwingUp-v0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m q \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m      4\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl/lib/python3.11/site-packages/gym/envs/registration.py:569\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    563\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    564\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the latest versioned environment `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_env_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    565\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead of the unversioned environment `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mid\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    566\u001b[0m         )\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spec_ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 569\u001b[0m         \u001b[43m_check_version_exists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    570\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo registered env with id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mid\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    572\u001b[0m _kwargs \u001b[38;5;241m=\u001b[39m spec_\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m~/miniconda3/envs/rl/lib/python3.11/site-packages/gym/envs/registration.py:219\u001b[0m, in \u001b[0;36m_check_version_exists\u001b[0;34m(ns, name, version)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m get_env_id(ns, name, version) \u001b[38;5;129;01min\u001b[39;00m registry:\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 219\u001b[0m \u001b[43m_check_name_exists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl/lib/python3.11/site-packages/gym/envs/registration.py:197\u001b[0m, in \u001b[0;36m_check_name_exists\u001b[0;34m(ns, name)\u001b[0m\n\u001b[1;32m    194\u001b[0m namespace_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in namespace \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mns\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ns \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    195\u001b[0m suggestion_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDid you mean: `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuggestion[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`?\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m suggestion \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 197\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mNameNotFound(\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnvironment \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt exist\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnamespace_msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuggestion_msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    199\u001b[0m )\n",
      "\u001b[0;31mNameNotFound\u001b[0m: Environment CartpoleSwingUp doesn't exist. Did you mean: `CartPole`?"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartpoleSwingUp-v0')\n",
    "\n",
    "q = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    q, r, done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "env.render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
