{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/atticusw/opt/anaconda3/lib/python3.9/site-packages/gym/envs/registration.py:307: DeprecationWarning: The package name gym_minigrid has been deprecated in favor of minigrid. Please uninstall gym_minigrid and install minigrid with `pip install minigrid`. Future releases will be maintained under the new package name minigrid.\n",
      "  fn()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "import torch.nn.functional as F\n",
    "import gym_minigrid\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from tqdm.notebook import tqdm\n",
    "from gym_minigrid.envs.doorkey import DoorKeyEnv\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from typing_extensions import Self\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Config:\n",
    "    \"\"\"\n",
    "    Stores algorithmic hyperparameters.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                score_threshold=0.93,\n",
    "                discount=0.995,\n",
    "                lr=1e-3,\n",
    "                max_grad_norm=0.5,\n",
    "                log_interval=10,\n",
    "                max_episodes=2000,\n",
    "                gae_lambda=0.95,\n",
    "                use_critic=True,\n",
    "                clip_ratio=0.2,\n",
    "                target_kl=0.01,\n",
    "                train_ac_iters=5,\n",
    "                use_discounted_reward=False,\n",
    "                entropy_coef=0.01,\n",
    "                use_gae=True,\n",
    "                my_var=0):\n",
    "\n",
    "        self.score_threshold = score_threshold # criterion for early stopping. If the rolling average reward (over the last 100 episodes) is greater than it, it ends.\n",
    "        self.discount = discount # discount factor\n",
    "        self.lr = lr # learning rate\n",
    "        self.max_grad_norm = max_grad_norm # the maximum gradient norm (https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html)\n",
    "        self.log_interval = log_interval # logging interval\n",
    "        self.max_episodes = max_episodes # the maximum number of episodes.\n",
    "        self.use_critic = use_critic # whether to use critic or not.\n",
    "        self.clip_ratio = clip_ratio # clip_ratio of PPO.\n",
    "        self.target_kl = target_kl # target KL divergence for early stoping train_ac_iters for PPO\n",
    "        self.train_ac_iters = train_ac_iters # how many time to train ac_model using current computed old_logps\n",
    "        self.gae_lambda=gae_lambda # lambda in Generalized Advantage Estimation (GAE)\n",
    "        self.use_discounted_reward=use_discounted_reward # whether use discounted reward or not.\n",
    "        self.entropy_coef = entropy_coef # entropy coefficient for PPO\n",
    "        self.use_gae = use_gae # whether to use GAE or not.\n",
    "        self.my_var = my_var # a variable for testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function from https://github.com/ikostrikov/pytorch-a2c-ppo-acktr/blob/master/model.py\n",
    "def init_params(m):\n",
    "    \"\"\"\n",
    "    Initialize parameters of the network.\n",
    "    m: torch.nn.Module\n",
    "    \"\"\"\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Linear\") != -1:\n",
    "        m.weight.data.normal_(0, 1)\n",
    "        m.weight.data *= 1 / torch.sqrt(m.weight.data.pow(2).sum(1, keepdim=True))\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0)\n",
    "\n",
    "def preprocess_obss(obss, device=None):\n",
    "    \"\"\"\n",
    "    Convert observation into Torch.Tensor\n",
    "\n",
    "    Parameters\n",
    "    ----\n",
    "    obss: dictionary or np.ndarray\n",
    "    device: target device of torch.Tensor ('cpu', 'cuda')\n",
    "\n",
    "    Return\n",
    "    ----\n",
    "    Torch Tensor\n",
    "    \"\"\"\n",
    "    if isinstance(obss, dict):\n",
    "        images = np.array([obss[\"image\"]])\n",
    "    else:\n",
    "        images = np.array([o[\"image\"] for o in obss])\n",
    "\n",
    "    return torch.tensor(images, device=device, dtype=torch.float)\n",
    "\n",
    "class DoorKeyEnv5x5(DoorKeyEnv):\n",
    "    def __init__(self):\n",
    "        self.render_mode = \"rgb_array\"\n",
    "        super().__init__(size=5)\n",
    "\n",
    "    def _reward(self):\n",
    "        \"\"\"\n",
    "        Compute the reward to be given upon success\n",
    "        \"\"\"\n",
    "        return 1\n",
    "    \n",
    "class DoorKeyEnv6x6(DoorKeyEnv):\n",
    "    def __init__(self):\n",
    "        self.render_mode = \"rgb_array\"\n",
    "        super().__init__(size=6)\n",
    "\n",
    "    def _reward(self):\n",
    "        \"\"\"\n",
    "        Compute the reward to be given upon success\n",
    "        \"\"\"\n",
    "        return 1\n",
    "    \n",
    "class DoorKeyEnv8x8(DoorKeyEnv):\n",
    "    def __init__(self):\n",
    "        self.render_mode = \"rgb_array\"\n",
    "        super().__init__(size=8)\n",
    "\n",
    "    def _reward(self):\n",
    "        \"\"\"\n",
    "        Compute the reward to be given upon success\n",
    "        \"\"\"\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fba2a35c940>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = DoorKeyEnv6x6() # define environment.\n",
    "env.reset()\n",
    "action = env.action_space.sample()\n",
    "obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "# get an RGB image corresponding to the whole environment or the agent's point of view (https://github.com/Farama-Foundation/Minigrid/blob/master/minigrid/minigrid_env.py#L716)\n",
    "#            highlight (bool): If true, the agent's field of view or point of view is highlighted with a lighter gray color.\n",
    "#            tile_size (int): How many pixels will form a tile from the NxM grid.\n",
    "#            agent_pov (bool): If true, the rendered frame will only contain the point of view of the agent.\n",
    "frame = env.get_frame(highlight=env.highlight, tile_size=env.tile_size, agent_pov=env.agent_pov)\n",
    "# show an image to the notebook.\n",
    "plt.imshow(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACModel(nn.Module):\n",
    "    def __init__(self, num_actions, use_critic=True):\n",
    "        \"\"\"\n",
    "        Represents an Actor Crictic model that takes a 2d, multi-channeled\n",
    "        image as input.\n",
    "\n",
    "        Parameters\n",
    "        ----\n",
    "        num_actions : int\n",
    "\n",
    "                      The action space of the environment.\n",
    "                      The action space for DoorKey5x5 is 7-dimensional:\n",
    "                      0: turn left,\n",
    "                      1: turn right,\n",
    "                      2: forward,\n",
    "                      3: pickup an object,\n",
    "                      4: drop an object,\n",
    "                      5: activate an object,\n",
    "                      6: done completing task\n",
    "\n",
    "        use_critics : bool\n",
    "\n",
    "                      Critic network will be used in forward pass if flag is set\n",
    "                      to true.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.use_critic = use_critic\n",
    "\n",
    "        # Define actor's model\n",
    "        self.image_conv_actor = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, (2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 2)),\n",
    "            nn.Conv2d(16, 32, (2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, (2, 2)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, num_actions)\n",
    "        )\n",
    "\n",
    "        # Define critic's model\n",
    "        if self.use_critic:\n",
    "            self.image_conv_critic = nn.Sequential(\n",
    "                nn.Conv2d(3, 16, (2, 2)),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d((2, 2)),\n",
    "                nn.Conv2d(16, 32, (2, 2)),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(32, 64, (2, 2)),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "            self.critic = nn.Sequential(\n",
    "                nn.Linear(64, 64),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(64, 1)\n",
    "            )\n",
    "\n",
    "        # Initialize parameters correctly\n",
    "        self.apply(init_params)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        \"\"\"\n",
    "        Performs a forward pass through the actor-critic network\n",
    "\n",
    "        Parameters\n",
    "        ----\n",
    "        obs : int tensor. Shape [Batch size, ImWidth, ImHeight, Channels]\n",
    "\n",
    "              input to the network.\n",
    "        ----\n",
    "\n",
    "        returns:\n",
    "\n",
    "        dist : torch.distribution\n",
    "            The distribution of actions from policy. A Categorical distribution\n",
    "            for discreet action spaces.\n",
    "        value : torch.Tensor (Batch size, 1)\n",
    "            value output by critic network\n",
    "        \"\"\"\n",
    "        conv_in = obs.transpose(1, 3).transpose(2, 3) # reshape into [b, c, h, w]\n",
    "\n",
    "        dist, value = None, None\n",
    "\n",
    "        x = self.image_conv_actor(conv_in)\n",
    "        embedding = x.reshape(x.shape[0], -1)\n",
    "\n",
    "        x = self.actor(embedding)\n",
    "        dist = Categorical(logits=F.log_softmax(x, dim=1))\n",
    "\n",
    "        if self.use_critic:\n",
    "            y = self.image_conv_critic(conv_in)\n",
    "            embedding = y.reshape(y.shape[0], -1)\n",
    "\n",
    "            value = self.critic(embedding).squeeze(1)\n",
    "        else:\n",
    "            value = torch.zeros((x.shape[0], 1), device=x.device)\n",
    "        \n",
    "        return dist, value\n",
    "\n",
    "def compute_discounted_return(rewards, discount, device=DEVICE):\n",
    "    \"\"\"\n",
    "\t\trewards: reward obtained at timestep.  Shape: (T,)\n",
    "\t\tdiscount: discount factor. float\n",
    "\n",
    "    ----\n",
    "    returns: sum of discounted rewards. Shape: (T,)\n",
    "\t\t\"\"\"\n",
    "    returns = torch.zeros(*rewards.shape, device=device)\n",
    "\n",
    "    R = 0\n",
    "    for t in reversed(range((rewards.shape[0]))):\n",
    "        R = rewards[t] + discount * R\n",
    "        returns[t] = R\n",
    "    return returns\n",
    "\n",
    "def compute_advantage_gae(values, rewards, T, gae_lambda, discount):\n",
    "    \"\"\"\n",
    "    Compute Adavantage with GAE. See Section 4.4.2 in the lecture notes.\n",
    "\n",
    "    values: value at each timestep (T,)\n",
    "    rewards: reward obtained at each timestep.  Shape: (T,)\n",
    "    T: the number of frames, float\n",
    "    gae_lambda: hyperparameter, float\n",
    "    discount: discount factor, float\n",
    "\n",
    "    -----\n",
    "\n",
    "    returns:\n",
    "\n",
    "    advantages : tensor.float. Shape [T,]\n",
    "\n",
    "                 gae advantage term for timesteps 0 to T\n",
    "\n",
    "    \"\"\"\n",
    "    advantages = torch.zeros_like(values)\n",
    "    for i in reversed(range(T)):\n",
    "        next_value = values[i+1]\n",
    "        next_advantage = advantages[i+1]\n",
    "\n",
    "        delta = rewards[i] + discount * next_value  - values[i]\n",
    "        advantages[i] = delta + discount * gae_lambda * next_advantage\n",
    "    return advantages[:T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:64] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Categorical(logits: torch.Size([1, 7])),\n",
       " tensor([-0.2645], grad_fn=<SqueezeBackward1>))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TESTS\n",
    "\n",
    "model = ACModel(num_actions=7, use_critic=True).to(DEVICE)\n",
    "obss = env.reset()\n",
    "obs = preprocess_obss(obss[0], DEVICE)\n",
    "model(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Machine:\n",
    "    def __init__(self, num_actions=7, args=Config()):\n",
    "        \"\"\"\n",
    "        A Machine object consists of a ACModel, its hyperparameters, and its history of rewards.\n",
    "\n",
    "        Args:\n",
    "            num_actions: size of action space\n",
    "            args: Model hyperparameters\n",
    "        \"\"\"\n",
    "        self.args = args\n",
    "        self.model = ACModel(num_actions, use_critic=True).to(DEVICE)\n",
    "        self.optim = torch.optim.Adam(self.model.parameters(), lr=self.args.lr)\n",
    "        self.rewards = []\n",
    "\n",
    "    def reset_rs(self) -> None:\n",
    "        self.rewards = []\n",
    "\n",
    "    def copy_machine(self, other: Self) -> None:\n",
    "        \"\"\"\n",
    "        Copy state dict from 'other'. Reset rs.\n",
    "        \"\"\"\n",
    "        state_dict = other.model.state_dict()\n",
    "        for key, v in state_dict.items():\n",
    "            self.model.state_dict()[key].copy_(v)\n",
    "        \n",
    "        self.reset_rs()\n",
    "\n",
    "    def add_r(self, r: float) -> None:\n",
    "        self.rewards.append(r)\n",
    "\n",
    "    def avg_r(self) -> float:\n",
    "        assert len(self.rewards) > 0, 'No rewards yet'\n",
    "        return sum(self.rewards) / len(self.rewards)\n",
    "    \n",
    "    def collect_experiences(self, env:gym.Env):\n",
    "        \"\"\"\n",
    "        Collects rollouts and computes advantages.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        exps : dict\n",
    "            Contains actions, rewards, advantages etc as attributes.\n",
    "            Each attribute, e.g. `exps['reward']` has a shape\n",
    "            (self.num_frames, ...).\n",
    "        logs : dict\n",
    "            Useful stats about the training process, including the average\n",
    "            reward, policy loss, value loss, etc.\n",
    "        \"\"\"\n",
    "        acmodel = self.model\n",
    "\n",
    "        MAX_FRAMES_PER_EP = 300\n",
    "        shape = (MAX_FRAMES_PER_EP, )\n",
    "\n",
    "        actions = torch.zeros(*shape, device=DEVICE, dtype=torch.int)\n",
    "        values = torch.zeros(*shape, device=DEVICE)\n",
    "        rewards = torch.zeros(*shape, device=DEVICE)\n",
    "        log_probs = torch.zeros(*shape, device=DEVICE)\n",
    "        obss = [None]*MAX_FRAMES_PER_EP\n",
    "\n",
    "        obs, _ = env.reset()\n",
    "\n",
    "        total_return = 0\n",
    "\n",
    "        T = 0\n",
    "\n",
    "        while True:\n",
    "            # Do one agent-environment interaction\n",
    "\n",
    "            preprocessed_obs = preprocess_obss(obs, device=device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                dist, value = acmodel(preprocessed_obs)\n",
    "\n",
    "            dist: Categorical\n",
    "            action = dist.sample()[0]\n",
    "\n",
    "            obss[T] = obs\n",
    "            obs, reward, done, _, _ = env.step(action.item())\n",
    "\n",
    "\n",
    "            # Update experiences values\n",
    "            actions[T] = action\n",
    "            values[T] = value\n",
    "            rewards[T] = reward\n",
    "            log_probs[T] = dist.log_prob(action)\n",
    "\n",
    "\n",
    "            total_return += reward\n",
    "            T += 1\n",
    "\n",
    "            if done or T>=MAX_FRAMES_PER_EP-1:\n",
    "                break\n",
    "\n",
    "        discounted_reward = compute_discounted_return(rewards[:T], self.args.discount, device)\n",
    "        exps = dict(\n",
    "            obs = preprocess_obss([\n",
    "                obss[i]\n",
    "                for i in range(T)\n",
    "            ], device=DEVICE),\n",
    "            action = actions[:T],\n",
    "            value  = values[:T],\n",
    "            reward = rewards[:T],\n",
    "            log_prob = log_probs[:T],\n",
    "            discounted_reward = discounted_reward,\n",
    "            label = self.label,\n",
    "            T = T\n",
    "        )\n",
    "\n",
    "        logs = {\n",
    "            \"return_per_episode\": total_return,\n",
    "            \"num_frames\": T\n",
    "        }\n",
    "\n",
    "        return exps, logs\n",
    "\n",
    "    def _compute_policy_loss_ppo(self, obs, init_logp, old_logp, actions, advantages):\n",
    "        \"\"\"\n",
    "        Computes the policy loss for PPO.\n",
    "\n",
    "        obs: observeration to pass into acmodel. shape: (T,)\n",
    "        init_logp: log probabilities we get from the agent performing the action. shape: (T,)\n",
    "        old_logp: log probabilities from previous timestep. shape: (T,)\n",
    "        actions: action at this timestep. shape: (T,ImWidth,ImHeight,Channels)\n",
    "        advantages: the computed advantages. shape: (T,)\n",
    "\n",
    "        ---\n",
    "        returns\n",
    "\n",
    "        policy_loss : ppo policy loss as shown in line 6 of PPO alg. tensor.float. Shape (,1)\n",
    "        approx_kl: an appoximation of the kl_divergence. tensor.float. Shape (,1)\n",
    "        \"\"\"\n",
    "        policy_loss, approx_kl = 0, 0\n",
    "\n",
    "        dist, _ = self.model(obs)\n",
    "        dist: Categorical\n",
    "\n",
    "        coef = self.coef\n",
    "\n",
    "        entropy = dist.entropy()\n",
    "        logps = dist.log_prob(actions)\n",
    "        r_terms = torch.exp(logps - old_logp)\n",
    "        ppo_loss = torch.min(r_terms * advantages, torch.clamp(r_terms, 1 - self.args.clip_ratio, 1 + self.args.clip_ratio) * advantages)\n",
    "        \n",
    "        # Importance sampling factor\n",
    "        factors = torch.exp(old_logp - init_logp)\n",
    "        \n",
    "        indices = factors < 100\n",
    "        policy_loss_tensor = factors * ppo_loss + coef * entropy\n",
    "\n",
    "        policy_loss = - torch.mean(policy_loss_tensor[indices])\n",
    "\n",
    "        # approx_kl = torch.sum(torch.exp(old_logp) * (old_logp - logps)) / torch.sum(torch.exp(old_logp))\n",
    "        approx_kl = torch.mean((old_logp - logps) ** 2) / 2\n",
    "\n",
    "        return policy_loss, approx_kl\n",
    "    \n",
    "    def _compute_value_loss(self, obs, returns):\n",
    "        _, value = self.model(obs)\n",
    "\n",
    "        value_loss = torch.mean((value - returns) ** 2)\n",
    "\n",
    "        return value_loss\n",
    "\n",
    "\n",
    "    def update_parameters(self, sb):\n",
    "        \n",
    "        MAX_FRAMES_PER_EP = 300\n",
    "        T = sb['T']\n",
    "        dist, values = self.model(sb['obs'])\n",
    "        values = values.reshape(-1)\n",
    "        dist: Categorical\n",
    "        old_logp = dist.log_prob(sb['action']).detach()\n",
    "        init_logp = sb['log_prob']\n",
    "\n",
    "        # add 0 to end of values until it has length MAX_FRAMES_PER_EP\n",
    "        values_extended = torch.cat([values, torch.zeros((MAX_FRAMES_PER_EP - len(values), ), device=DEVICE)], dim=0)\n",
    "        full_reward = torch.cat([sb['reward'], torch.zeros((MAX_FRAMES_PER_EP - len(sb['reward']), ), device=DEVICE)], dim=0)\n",
    "\n",
    "        if self.args.use_gae:\n",
    "            advantage = compute_advantage_gae(values_extended, full_reward, T, self.args.gae_lambda, self.args.discount)\n",
    "        else:\n",
    "            advantage = sb['discounted_reward'] - values.reshape(-1)\n",
    "\n",
    "        policy_loss, _ = self._compute_policy_loss_ppo(sb['obs'], init_logp, old_logp, sb['action'], advantage)\n",
    "        value_loss = self._compute_value_loss(sb['obs'], sb['discounted_reward'])\n",
    "\n",
    "        for i in range(self.args.train_ac_iters):\n",
    "            self.optim.zero_grad()\n",
    "            loss_pi, approx_kl = self._compute_policy_loss_ppo(sb['obs'], init_logp, old_logp, sb['action'], advantage)\n",
    "            if sb['label'] == self.label:\n",
    "                loss_v = self._compute_value_loss(sb['obs'], sb['discounted_reward'])\n",
    "            else:\n",
    "                loss_v = 0.0\n",
    "\n",
    "            loss = loss_v + loss_pi\n",
    "            if approx_kl > 1.5 * self.args.target_kl:\n",
    "                break\n",
    "\n",
    "            loss.backward(retain_graph=True)\n",
    "            self.optim.step()\n",
    "\n",
    "        update_policy_loss = policy_loss.item()\n",
    "        update_value_loss = value_loss.item()\n",
    "\n",
    "        logs = {\n",
    "            \"policy_loss\": update_policy_loss,\n",
    "            \"value_loss\": update_value_loss,\n",
    "        }\n",
    "\n",
    "        return logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1887660741.py, line 57)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/8m/r3kmm3xs1lngf9bhwg216mlc0000gn/T/ipykernel_16324/1887660741.py\"\u001b[0;36m, line \u001b[0;32m57\u001b[0m\n\u001b[0;31m    advantage =\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class MachineCluster:\n",
    "    def __init__(self, hps:list[str], num_machines:int=5, num_survivors:int=2, num_trajs:int=40):\n",
    "        \"\"\"\n",
    "        The cluster of machines.\n",
    "\n",
    "        hps: A list of hyperparameters to optimize. Must be a subset of keys in Machine.args.\n",
    "        num_machines: The number of machines in the cluster.\n",
    "        num_survivors: The number of survivors to select from the cluster.\n",
    "        num_trajs: The number of trajectories collected by each machine in each training epoch.\n",
    "        \"\"\"\n",
    "        self.num_machines = num_machines\n",
    "        self.num_survivors = num_survivors\n",
    "        self.hps = hps\n",
    "        self.num_trajs = num_trajs\n",
    "        self.machines = []\n",
    "        for _ in range(self.num_machines):\n",
    "            self.models.append(Machine())\n",
    "\n",
    "        # now let's say hps = ['entropy_coef']\n",
    "        for hp in hps:\n",
    "            # sample num_models values from a uniform distribution\n",
    "            values = np.random.uniform(0, 1, size=num_machines)\n",
    "            # set the hyperparameters of the models\n",
    "            self.update_hp(hp, values)\n",
    "\n",
    "    def generate_data(self):\n",
    "        \"\"\"\n",
    "        Generate batches of data for training the models.\n",
    "\n",
    "        Return:\n",
    "            trajs: list of exps.  \n",
    "        \"\"\"\n",
    "\n",
    "        trajs = []\n",
    "\n",
    "        for i, _ in range(self.num_machines), range(self.num_trajs):\n",
    "            exps, _ = self.machines[i].collect_experiences(env)\n",
    "            trajs.append(exps)\n",
    "        \n",
    "        print(f'Finished generating {len(trajs)} trajectories')\n",
    "        return trajs\n",
    "\n",
    "    def train(self, data):\n",
    "        \"\"\"\n",
    "        Train the models in the cluster.\n",
    "\n",
    "        Args:\n",
    "            data: list of exps.\n",
    "        \"\"\"\n",
    "\n",
    "        MAX_FRAMES_PER_EP = 300\n",
    "        \n",
    "        for machine in self.machines:\n",
    "\n",
    "            total_policy_loss = 0\n",
    "            total_value_loss = 0\n",
    "\n",
    "            for sb in data:\n",
    "                T = sb['T']\n",
    "                obs = sb['obs']\n",
    "                dist, values = machine(obs)\n",
    "                init_logp = sb['log_prob']\n",
    "                old_logp = dist.log_prob(sb['action']).detach()\n",
    "\n",
    "                # add 0 to end of values until it has length MAX_FRAMES_PER_EP\n",
    "                values_extended = torch.cat([values, torch.zeros((MAX_FRAMES_PER_EP - len(values), ), device=DEVICE)], dim=0)\n",
    "                full_reward = torch.cat([sb['reward'], torch.zeros((MAX_FRAMES_PER_EP - len(sb['reward']), ), device=DEVICE)], dim=0)\n",
    "                assert len(values_extended) == MAX_FRAMES_PER_EP\n",
    "                assert len(full_reward) == MAX_FRAMES_PER_EP\n",
    "                \n",
    "                # compute advantage\n",
    "                if machine.args.use_gae:\n",
    "                    advantage = compute_advantage_gae(values_extended, full_reward, T, machine.args.gae_lambda, machine.args.discount)\n",
    "                else:\n",
    "                    advantage = sb['discounted_reward'] - values.reshape(-1)\n",
    "                \n",
    "                # compute policy and value loss\n",
    "                policy_loss, _ = machine._compute_policy_loss_ppo(obs, init_logp, old_logp, sb['action'], advantage)\n",
    "\n",
    "                value_loss = machine._compute_value_loss(obs, sb['discounted_reward'])\n",
    "\n",
    "                total_policy_loss += policy_loss\n",
    "                total_value_loss += value_loss\n",
    "            \n",
    "            machine.optim.zero_grad()\n",
    "\n",
    "\n",
    "            loss = total_policy_loss + total_value_loss\n",
    "\n",
    "            loss.backward()\n",
    "            machine.optim.step()\n",
    "        \n",
    "    def evaluate(self, trained_models):\n",
    "        \"\"\"\n",
    "        Evaluate the performance of the models in the cluster.\n",
    "\n",
    "        Args:\n",
    "        - trained_models: list of ExtendedModels.\n",
    "\n",
    "        Returns:\n",
    "        - performances:\n",
    "        \"\"\"\n",
    "\n",
    "        return performances\n",
    "\n",
    "\n",
    "    def update_hp(self, hp:str, targets:list[float]) -> None:\n",
    "        \"\"\"\n",
    "        Update the hyperparameters of the models in the cluster.\n",
    "\n",
    "        Args:\n",
    "            hp: The hyperparameter to update.\n",
    "            targets: The target hyperparameter values.\n",
    "                    Has length self.num_models.\n",
    "        \"\"\"\n",
    "        for i, target in enumerate(targets):\n",
    "            self.models[i].update_arg(hp, target)\n",
    "\n",
    "    def eliminate(self, trained_models, performances):\n",
    "        \"\"\"\n",
    "        The elimination process to select the survivors.\n",
    "        Also shift hyperparams.\n",
    "        \"\"\"\n",
    "\n",
    "        return elites\n",
    "    \n",
    "    def reproduce(self):\n",
    "        \"\"\"\n",
    "        Reproduce models.\n",
    "        \"\"\"\n",
    "\n",
    "        return NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cluster_experiment(hps:list[str], num_models:int=5, num_survivors:int=2, num_trajs:int=40, num_epochs:int=10, seed=42):\n",
    "    \"\"\"\n",
    "    Run the cluster experiment.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    cluster = MachineCluster(hps = hps, num_models = num_models, num_survivors = num_survivors, num_trajs = num_trajs)\n",
    "\n",
    "    pbar = tqdm(range(num_epochs))\n",
    "    logs = dict(\n",
    "        rewards = [],\n",
    "        smooth_rewards = []\n",
    "    )\n",
    "    pdlogs = []\n",
    "\n",
    "    for epoch in pbar:\n",
    "        # Generate data\n",
    "        obs = cluster.generate_data()\n",
    "\n",
    "        # Train until data fits poorly\n",
    "        cluster.train(data=obs)\n",
    "        trained_models = cluster.models\n",
    "\n",
    "        # Evaluate performances\n",
    "        performances = cluster.evaluate(trained_models)\n",
    "        logs.append(performances) # what is in logs?\n",
    "\n",
    "        # Survival of the fittest\n",
    "        survivors = cluster.eliminate(trained_models, performances)\n",
    "        cluster.reproduce(survivors)\n",
    "\n",
    "    return pd.DataFrame(pdlogs).set_index('epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = run_cluster_experiment(hps=['entropy_coef'])\n",
    "\n",
    "# df.plot(x='num_frames', y=['reward', 'smooth_reward'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(coefs:list[float], max_steps:int, env=DoorKeyEnv6x6(), args=Config(), seed=0):\n",
    "    \"\"\"\n",
    "    Upper level function for running experiments to analyze reinforce and\n",
    "    policy gradient methods. Instantiates a model, collects epxeriences, and\n",
    "    then updates the neccessary parameters.\n",
    "\n",
    "    Args:\n",
    "        coefs: a list of coefs each machine has\n",
    "        max_steps: maximum number of steps to run the experiment for.\n",
    "        env: the environment to use. \n",
    "        args: Config object with hyperparameters.\n",
    "        seed: random seed. int\n",
    "\n",
    "    Returns:\n",
    "        DataFrame indexed by episode\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    device = DEVICE\n",
    "\n",
    "    machines = [Machine(coef, label=idx, args=args) for idx, coef in enumerate(coefs)]\n",
    "    winners = []\n",
    "    full_rs = []\n",
    "\n",
    "    is_solved = False\n",
    "\n",
    "    SMOOTH_REWARD_WINDOW = 50\n",
    "\n",
    "    pd_logs, rewards = [], [0]*SMOOTH_REWARD_WINDOW\n",
    "\n",
    "    num_frames = 0\n",
    "\n",
    "    pbar = tqdm(range(max_steps))\n",
    "    for update in pbar:\n",
    "\n",
    "        label = update % len(machines)\n",
    "\n",
    "        machine = machines[label]\n",
    "        \n",
    "        exps, logs1 = machine.collect_experiences(env)\n",
    "\n",
    "        for m in machines:\n",
    "            if m is machine:\n",
    "                logs2 = m.update_parameters(exps)\n",
    "            else:\n",
    "                m.update_parameters(exps)\n",
    "\n",
    "        logs = {**logs1, **logs2}\n",
    "\n",
    "        num_frames += logs[\"num_frames\"]\n",
    "\n",
    "        rewards.append(logs[\"return_per_episode\"])\n",
    "\n",
    "        smooth_reward = np.mean(rewards[-SMOOTH_REWARD_WINDOW:])\n",
    "\n",
    "        data = {'episode':update, 'num_frames':num_frames, 'smooth_reward':smooth_reward,\n",
    "               'reward':logs[\"return_per_episode\"], 'policy_loss':logs[\"policy_loss\"]}\n",
    "\n",
    "        if args.use_critic:\n",
    "            data['value_loss'] = logs[\"value_loss\"]\n",
    "\n",
    "        pd_logs.append(data)\n",
    "\n",
    "        pbar.set_postfix(data)\n",
    "\n",
    "        machine.add_r(rewards[-1])\n",
    "        if update % (20 * len(machines)) == 0 and update > 0:\n",
    "            full_rs.append([m.avg_r() for m in machines])\n",
    "            winner = survival_fittest(machines)\n",
    "            winners.append(winner)\n",
    "\n",
    "        # # Early terminate\n",
    "        # if smooth_reward >= args.score_threshold:\n",
    "        #     is_solved = True\n",
    "        #     break\n",
    "\n",
    "    if is_solved:\n",
    "        print('Solved!')\n",
    "\n",
    "    for m in machines:\n",
    "        print(m.avg_r())\n",
    "    print(winners)\n",
    "    print(full_rs)\n",
    "\n",
    "    return pd.DataFrame(pd_logs).set_index('episode')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
