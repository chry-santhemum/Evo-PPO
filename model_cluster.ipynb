{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/atticusw/opt/anaconda3/lib/python3.9/site-packages/gym/envs/registration.py:307: DeprecationWarning: The package name gym_minigrid has been deprecated in favor of minigrid. Please uninstall gym_minigrid and install minigrid with `pip install minigrid`. Future releases will be maintained under the new package name minigrid.\n",
      "  fn()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "import torch.nn.functional as F\n",
    "import gym_minigrid\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from tqdm.notebook import tqdm\n",
    "from gym_minigrid.envs.doorkey import DoorKeyEnv\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from typing_extensions import Self\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Config:\n",
    "    \"\"\"\n",
    "    Stores algorithmic hyperparameters.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                score_threshold=0.93,\n",
    "                discount=0.995,\n",
    "                lr=1e-3,\n",
    "                max_grad_norm=0.5,\n",
    "                log_interval=10,\n",
    "                max_episodes=2000,\n",
    "                gae_lambda=0.95,\n",
    "                use_critic=True,\n",
    "                clip_ratio=0.2,\n",
    "                target_kl=0.01,\n",
    "                fit_badly_coeff=0.05,\n",
    "                train_ac_iters=5,\n",
    "                use_discounted_reward=False,\n",
    "                entropy_coef=0.01,\n",
    "                use_gae=True,\n",
    "                my_var=0):\n",
    "\n",
    "        self.score_threshold = score_threshold # criterion for early stopping. If the rolling average reward (over the last 100 episodes) is greater than it, it ends.\n",
    "        self.discount = discount # discount factor\n",
    "        self.lr = lr # learning rate\n",
    "        self.max_grad_norm = max_grad_norm # the maximum gradient norm (https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html)\n",
    "        self.log_interval = log_interval # logging interval\n",
    "        self.max_episodes = max_episodes # the maximum number of episodes.\n",
    "        self.use_critic = use_critic # whether to use critic or not.\n",
    "        self.clip_ratio = clip_ratio # clip_ratio of PPO.\n",
    "        self.target_kl = target_kl # target KL divergence for early stoping train_ac_iters for PPO\n",
    "        self.fit_badly_coeff = fit_badly_coeff # the bound of approx. KL of when the model fits badly to the data.\n",
    "        self.train_ac_iters = train_ac_iters # how many time to train ac_model using current computed old_logps\n",
    "        self.gae_lambda=gae_lambda # lambda in Generalized Advantage Estimation (GAE)\n",
    "        self.use_discounted_reward=use_discounted_reward # whether use discounted reward or not.\n",
    "        self.entropy_coef = entropy_coef # entropy coefficient for PPO\n",
    "        self.use_gae = use_gae # whether to use GAE or not.\n",
    "        self.my_var = my_var # a variable for testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function from https://github.com/ikostrikov/pytorch-a2c-ppo-acktr/blob/master/model.py\n",
    "def init_params(m):\n",
    "    \"\"\"\n",
    "    Initialize parameters of the network.\n",
    "    m: torch.nn.Module\n",
    "    \"\"\"\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Linear\") != -1:\n",
    "        m.weight.data.normal_(0, 1)\n",
    "        m.weight.data *= 1 / torch.sqrt(m.weight.data.pow(2).sum(1, keepdim=True))\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0)\n",
    "\n",
    "def preprocess_obss(obss, device=None):\n",
    "    \"\"\"\n",
    "    Convert observation into Torch.Tensor\n",
    "\n",
    "    Parameters\n",
    "    ----\n",
    "    obss: dictionary or np.ndarray\n",
    "    device: target device of torch.Tensor ('cpu', 'cuda')\n",
    "\n",
    "    Return\n",
    "    ----\n",
    "    Torch Tensor\n",
    "    \"\"\"\n",
    "    if isinstance(obss, dict):\n",
    "        images = np.array([obss[\"image\"]])\n",
    "    else:\n",
    "        images = np.array([o[\"image\"] for o in obss])\n",
    "\n",
    "    return torch.tensor(images, device=device, dtype=torch.float)\n",
    "\n",
    "class DoorKeyEnv5x5(DoorKeyEnv):\n",
    "    def __init__(self):\n",
    "        self.render_mode = \"rgb_array\"\n",
    "        super().__init__(size=5)\n",
    "\n",
    "    def _reward(self):\n",
    "        \"\"\"\n",
    "        Compute the reward to be given upon success\n",
    "        \"\"\"\n",
    "        return 1\n",
    "    \n",
    "class DoorKeyEnv6x6(DoorKeyEnv):\n",
    "    def __init__(self):\n",
    "        self.render_mode = \"rgb_array\"\n",
    "        super().__init__(size=6)\n",
    "\n",
    "    def _reward(self):\n",
    "        \"\"\"\n",
    "        Compute the reward to be given upon success\n",
    "        \"\"\"\n",
    "        return 1\n",
    "    \n",
    "class DoorKeyEnv8x8(DoorKeyEnv):\n",
    "    def __init__(self):\n",
    "        self.render_mode = \"rgb_array\"\n",
    "        super().__init__(size=8)\n",
    "\n",
    "    def _reward(self):\n",
    "        \"\"\"\n",
    "        Compute the reward to be given upon success\n",
    "        \"\"\"\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fdea1778820>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = DoorKeyEnv6x6() # define environment.\n",
    "env.reset()\n",
    "action = env.action_space.sample()\n",
    "obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "# get an RGB image corresponding to the whole environment or the agent's point of view (https://github.com/Farama-Foundation/Minigrid/blob/master/minigrid/minigrid_env.py#L716)\n",
    "#            highlight (bool): If true, the agent's field of view or point of view is highlighted with a lighter gray color.\n",
    "#            tile_size (int): How many pixels will form a tile from the NxM grid.\n",
    "#            agent_pov (bool): If true, the rendered frame will only contain the point of view of the agent.\n",
    "frame = env.get_frame(highlight=env.highlight, tile_size=env.tile_size, agent_pov=env.agent_pov)\n",
    "# show an image to the notebook.\n",
    "plt.imshow(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACModel(nn.Module):\n",
    "    def __init__(self, num_actions, use_critic=True):\n",
    "        \"\"\"\n",
    "        Represents an Actor Crictic model that takes a 2d, multi-channeled\n",
    "        image as input.\n",
    "\n",
    "        Parameters\n",
    "        ----\n",
    "        num_actions : int\n",
    "\n",
    "                      The action space of the environment.\n",
    "                      The action space for DoorKey5x5 is 7-dimensional:\n",
    "                      0: turn left,\n",
    "                      1: turn right,\n",
    "                      2: forward,\n",
    "                      3: pickup an object,\n",
    "                      4: drop an object,\n",
    "                      5: activate an object,\n",
    "                      6: done completing task\n",
    "\n",
    "        use_critics : bool\n",
    "\n",
    "                      Critic network will be used in forward pass if flag is set\n",
    "                      to true.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.use_critic = use_critic\n",
    "\n",
    "        # Define actor's model\n",
    "        self.image_conv_actor = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, (2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 2)),\n",
    "            nn.Conv2d(16, 32, (2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, (2, 2)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, num_actions)\n",
    "        )\n",
    "\n",
    "        # Define critic's model\n",
    "        if self.use_critic:\n",
    "            self.image_conv_critic = nn.Sequential(\n",
    "                nn.Conv2d(3, 16, (2, 2)),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d((2, 2)),\n",
    "                nn.Conv2d(16, 32, (2, 2)),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(32, 64, (2, 2)),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "            self.critic = nn.Sequential(\n",
    "                nn.Linear(64, 64),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(64, 1)\n",
    "            )\n",
    "\n",
    "        # Initialize parameters correctly\n",
    "        self.apply(init_params)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        \"\"\"\n",
    "        Performs a forward pass through the actor-critic network\n",
    "\n",
    "        Parameters\n",
    "        ----\n",
    "        obs : int tensor. Shape [Batch size, ImWidth, ImHeight, Channels]\n",
    "\n",
    "              input to the network.\n",
    "        ----\n",
    "\n",
    "        returns:\n",
    "\n",
    "        dist : torch.distribution\n",
    "            The distribution of actions from policy. A Categorical distribution\n",
    "            for discreet action spaces.\n",
    "        value : torch.Tensor (Batch size, 1)\n",
    "            value output by critic network\n",
    "        \"\"\"\n",
    "        conv_in = obs.transpose(1, 3).transpose(2, 3) # reshape into [b, c, h, w]\n",
    "\n",
    "        dist, value = None, None\n",
    "\n",
    "        x = self.image_conv_actor(conv_in)\n",
    "        embedding = x.reshape(x.shape[0], -1)\n",
    "\n",
    "        x = self.actor(embedding)\n",
    "        dist = Categorical(logits=F.log_softmax(x, dim=1))\n",
    "\n",
    "        if self.use_critic:\n",
    "            y = self.image_conv_critic(conv_in)\n",
    "            embedding = y.reshape(y.shape[0], -1)\n",
    "\n",
    "            value = self.critic(embedding).squeeze(1)\n",
    "        else:\n",
    "            value = torch.zeros((x.shape[0], 1), device=x.device)\n",
    "        \n",
    "        return dist, value\n",
    "\n",
    "def compute_discounted_return(rewards, discount, device=DEVICE):\n",
    "    \"\"\"\n",
    "\t\trewards: reward obtained at timestep.  Shape: (T,)\n",
    "\t\tdiscount: discount factor. float\n",
    "\n",
    "    ----\n",
    "    returns: sum of discounted rewards. Shape: (T,)\n",
    "\t\t\"\"\"\n",
    "    returns = torch.zeros(*rewards.shape, device=device)\n",
    "\n",
    "    R = 0\n",
    "    for t in reversed(range((rewards.shape[0]))):\n",
    "        R = rewards[t] + discount * R\n",
    "        returns[t] = R\n",
    "    return returns\n",
    "\n",
    "def compute_advantage_gae(values, rewards, T, gae_lambda, discount):\n",
    "    \"\"\"\n",
    "    Compute Adavantage with GAE. See Section 4.4.2 in the lecture notes.\n",
    "\n",
    "    values: value at each timestep (T,)\n",
    "    rewards: reward obtained at each timestep.  Shape: (T,)\n",
    "    T: the number of frames, float\n",
    "    gae_lambda: hyperparameter, float\n",
    "    discount: discount factor, float\n",
    "\n",
    "    -----\n",
    "\n",
    "    returns:\n",
    "\n",
    "    advantages : tensor.float. Shape [T,]\n",
    "\n",
    "                 gae advantage term for timesteps 0 to T\n",
    "\n",
    "    \"\"\"\n",
    "    advantages = torch.zeros_like(values)\n",
    "    for i in reversed(range(T)):\n",
    "        next_value = values[i+1]\n",
    "        next_advantage = advantages[i+1]\n",
    "\n",
    "        delta = rewards[i] + discount * next_value  - values[i]\n",
    "        advantages[i] = delta + discount * gae_lambda * next_advantage\n",
    "    return advantages[:T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:64] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Categorical(logits: torch.Size([1, 7])),\n",
       " tensor([0.0451], grad_fn=<SqueezeBackward1>))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TESTS\n",
    "\n",
    "model = ACModel(num_actions=7, use_critic=True).to(DEVICE)\n",
    "obss = env.reset()\n",
    "obs = preprocess_obss(obss[0], DEVICE)\n",
    "model(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Machine:\n",
    "    def __init__(self, num_actions=7, args=Config()):\n",
    "        \"\"\"\n",
    "        A Machine object consists of a ACModel, its hyperparameters, and its history of rewards.\n",
    "\n",
    "        Args:\n",
    "            num_actions: size of action space\n",
    "            args: Model hyperparameters\n",
    "        \"\"\"\n",
    "        self.args = args\n",
    "        self.model = ACModel(num_actions, use_critic=True).to(DEVICE)\n",
    "        self.optim = torch.optim.Adam(self.model.parameters(), lr=self.args.lr)\n",
    "        self.rewards = []\n",
    "\n",
    "    def reset_rs(self) -> None:\n",
    "        self.rewards = []\n",
    "\n",
    "    def copy_machine(self, other: Self) -> None:\n",
    "        \"\"\"\n",
    "        Copy state dict from 'other'. Reset rs.\n",
    "        \"\"\"\n",
    "        state_dict = other.model.state_dict()\n",
    "        for key, v in state_dict.items():\n",
    "            self.model.state_dict()[key].copy_(v)\n",
    "        \n",
    "        self.reset_rs()\n",
    "\n",
    "    def add_r(self, r: float) -> None:\n",
    "        self.rewards.append(r)\n",
    "\n",
    "    def avg_r(self) -> float:\n",
    "        assert len(self.rewards) > 0, 'No rewards yet'\n",
    "        return sum(self.rewards) / len(self.rewards)\n",
    "    \n",
    "    def collect_experiences(self, env:gym.Env):\n",
    "        \"\"\"\n",
    "        Collects rollouts and computes advantages.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        exps : dict\n",
    "            Contains actions, rewards, advantages etc as attributes.\n",
    "            Each attribute, e.g. `exps['reward']` has a shape\n",
    "            (self.num_frames, ...).\n",
    "        logs : dict\n",
    "            Useful stats about the training process, including the average\n",
    "            reward, policy loss, value loss, etc.\n",
    "        \"\"\"\n",
    "        acmodel = self.model\n",
    "\n",
    "        MAX_FRAMES_PER_EP = 300\n",
    "        shape = (MAX_FRAMES_PER_EP, )\n",
    "\n",
    "        actions = torch.zeros(*shape, device=DEVICE, dtype=torch.int)\n",
    "        values = torch.zeros(*shape, device=DEVICE)\n",
    "        rewards = torch.zeros(*shape, device=DEVICE)\n",
    "        log_probs = torch.zeros(*shape, device=DEVICE)\n",
    "        obss = [None]*MAX_FRAMES_PER_EP\n",
    "\n",
    "        obs, _ = env.reset()\n",
    "\n",
    "        total_return = 0\n",
    "\n",
    "        T = 0\n",
    "\n",
    "        while True:\n",
    "            # Do one agent-environment interaction\n",
    "\n",
    "            preprocessed_obs = preprocess_obss(obs, device=DEVICE)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                dist, value = acmodel(preprocessed_obs)\n",
    "\n",
    "            dist: Categorical\n",
    "            action = dist.sample()[0]\n",
    "\n",
    "            obss[T] = obs\n",
    "            obs, reward, done, _, _ = env.step(action.item())\n",
    "\n",
    "\n",
    "            # Update experiences values\n",
    "            actions[T] = action\n",
    "            values[T] = value\n",
    "            rewards[T] = reward\n",
    "            log_probs[T] = dist.log_prob(action)\n",
    "\n",
    "\n",
    "            total_return += reward\n",
    "            T += 1\n",
    "\n",
    "            if done or T>=MAX_FRAMES_PER_EP-1:\n",
    "                break\n",
    "\n",
    "        discounted_reward = compute_discounted_return(rewards[:T], self.args.discount, DEVICE)\n",
    "        exps = dict(\n",
    "            obs = preprocess_obss([\n",
    "                obss[i]\n",
    "                for i in range(T)\n",
    "            ], device=DEVICE),\n",
    "            action = actions[:T],\n",
    "            value  = values[:T],\n",
    "            reward = rewards[:T],\n",
    "            log_prob = log_probs[:T],\n",
    "            discounted_reward = discounted_reward,\n",
    "            T = T\n",
    "        )\n",
    "\n",
    "        logs = {\n",
    "            \"return_per_episode\": total_return,\n",
    "            \"num_frames\": T\n",
    "        }\n",
    "\n",
    "        return exps, logs\n",
    "\n",
    "    def _compute_policy_loss_ppo(self, obs, init_logp, old_logp, actions, advantages):\n",
    "        \"\"\"\n",
    "        Computes the policy loss for PPO.\n",
    "\n",
    "        obs: observeration to pass into acmodel. shape: (T,)\n",
    "        init_logp: log probabilities we get from the agent performing the action. shape: (T,)\n",
    "        old_logp: log probabilities from previous timestep. shape: (T,)\n",
    "        actions: action at this timestep. shape: (T,ImWidth,ImHeight,Channels)\n",
    "        advantages: the computed advantages. shape: (T,)\n",
    "\n",
    "        ---\n",
    "        returns\n",
    "\n",
    "        policy_loss : ppo policy loss as shown in line 6 of PPO alg. tensor.float. Shape (,1)\n",
    "        approx_kl: an appoximation of the kl_divergence. tensor.float. Shape (,1)\n",
    "        \"\"\"\n",
    "        policy_loss, approx_kl = 0, 0\n",
    "\n",
    "        dist, _ = self.model(obs)\n",
    "        dist: Categorical\n",
    "\n",
    "        entropy = dist.entropy()\n",
    "        logps = dist.log_prob(actions)\n",
    "        r_terms = torch.exp(logps - old_logp)\n",
    "        ppo_loss = torch.min(r_terms * advantages, torch.clamp(r_terms, 1 - self.args.clip_ratio, 1 + self.args.clip_ratio) * advantages)\n",
    "        \n",
    "        # Importance sampling factor\n",
    "        factors = torch.exp(old_logp - init_logp)\n",
    "        \n",
    "        indices = factors < 100\n",
    "\n",
    "        policy_loss_tensor = factors * ppo_loss + self.args.entropy_coef * entropy\n",
    "\n",
    "        policy_loss = - torch.mean(policy_loss_tensor[indices])\n",
    "\n",
    "        # approx_kl = torch.sum(torch.exp(old_logp) * (old_logp - logps)) / torch.sum(torch.exp(old_logp))\n",
    "        # approx_kl = torch.mean((old_logp - logps) ** 2) / 2\n",
    "        \n",
    "        fit_badly = torch.mean((old_logp - init_logp) ** 2)\n",
    "\n",
    "        return policy_loss, fit_badly\n",
    "    \n",
    "    def _compute_value_loss(self, obs, returns):\n",
    "        _, value = self.model(obs)\n",
    "\n",
    "        value_loss = torch.mean((value - returns) ** 2)\n",
    "\n",
    "        return value_loss\n",
    "\n",
    "\n",
    "    def update_parameters(self, sb):\n",
    "        \n",
    "        MAX_FRAMES_PER_EP = 300\n",
    "        T = sb['T']\n",
    "        dist, values = self.model(sb['obs'])\n",
    "        values = values.reshape(-1)\n",
    "        dist: Categorical\n",
    "        old_logp = dist.log_prob(sb['action']).detach()\n",
    "        init_logp = sb['log_prob']\n",
    "\n",
    "        # add 0 to end of values until it has length MAX_FRAMES_PER_EP\n",
    "        values_extended = torch.cat([values, torch.zeros((MAX_FRAMES_PER_EP - len(values), ), device=DEVICE)], dim=0)\n",
    "        full_reward = torch.cat([sb['reward'], torch.zeros((MAX_FRAMES_PER_EP - len(sb['reward']), ), device=DEVICE)], dim=0)\n",
    "\n",
    "        if self.args.use_gae:\n",
    "            advantage = compute_advantage_gae(values_extended, full_reward, T, self.args.gae_lambda, self.args.discount)\n",
    "        else:\n",
    "            advantage = sb['discounted_reward'] - values.reshape(-1)\n",
    "\n",
    "        policy_loss, _ = self._compute_policy_loss_ppo(sb['obs'], init_logp, old_logp, sb['action'], advantage)\n",
    "        value_loss = self._compute_value_loss(sb['obs'], sb['discounted_reward'])\n",
    "\n",
    "        for i in range(self.args.train_ac_iters):\n",
    "            self.optim.zero_grad()\n",
    "            loss_pi, approx_kl = self._compute_policy_loss_ppo(sb['obs'], init_logp, old_logp, sb['action'], advantage)\n",
    "            if sb['label'] == self.label:\n",
    "                loss_v = self._compute_value_loss(sb['obs'], sb['discounted_reward'])\n",
    "            else:\n",
    "                loss_v = 0.0\n",
    "\n",
    "            loss = loss_v + loss_pi\n",
    "            if approx_kl > 1.5 * self.args.target_kl:\n",
    "                break\n",
    "\n",
    "            loss.backward(retain_graph=True)\n",
    "            self.optim.step()\n",
    "\n",
    "        update_policy_loss = policy_loss.item()\n",
    "        update_value_loss = value_loss.item()\n",
    "\n",
    "        logs = {\n",
    "            \"policy_loss\": update_policy_loss,\n",
    "            \"value_loss\": update_value_loss,\n",
    "        }\n",
    "\n",
    "        return logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MachineCluster:\n",
    "    def __init__(self, hps:list[str], num_machines:int=5, num_survivors:int=2, num_trajs:int=16):\n",
    "        \"\"\"\n",
    "        The cluster of machines.\n",
    "\n",
    "        hps: A list of hyperparameters to optimize. Must be a subset of keys in Machine.args.\n",
    "        num_machines: The number of machines in the cluster.\n",
    "        num_survivors: The number of survivors to select from the cluster.\n",
    "        num_trajs: The number of trajectories collected by each machine in each training epoch.\n",
    "        batch_size: The number of trajectories used in each gradient step.\n",
    "        \"\"\"\n",
    "        self.num_machines = num_machines\n",
    "        self.num_survivors = num_survivors\n",
    "        self.hps = hps\n",
    "        self.num_trajs = num_trajs\n",
    "        self.machines = []\n",
    "        for _ in range(self.num_machines):\n",
    "            self.machines.append(Machine())\n",
    "\n",
    "        # now let's say hps = ['entropy_coef']\n",
    "        for hp in hps:\n",
    "            # sample num_models values from a uniform distribution\n",
    "            values = np.random.uniform(0, 1, size=num_machines)\n",
    "            # set the hyperparameters of the models\n",
    "            self.update_hp(hp, values)\n",
    "\n",
    "    def generate_data(self):\n",
    "        \"\"\"\n",
    "        Generate batches of data for training the models.\n",
    "\n",
    "        Return:\n",
    "            trajs: list of exps.  \n",
    "        \"\"\"\n",
    "\n",
    "        trajs = []\n",
    "\n",
    "        for i in range(self.num_machines):\n",
    "            for _ in range(self.num_trajs):\n",
    "                exps, _ = self.machines[i].collect_experiences(env)\n",
    "                trajs.append(exps)\n",
    "            \n",
    "            print(f'Generated data for machine {i}.')\n",
    "        \n",
    "        return trajs\n",
    "\n",
    "    def train_one_step(self, data, batch_size):\n",
    "        \"\"\"\n",
    "        Do one step of optimization on each model in the cluster.\n",
    "\n",
    "        Args:\n",
    "            data: list of exps.\n",
    "            batch_size: number of trajectories used in each gradient step.\n",
    "\n",
    "        Return:\n",
    "            logs\n",
    "        \"\"\"\n",
    "\n",
    "        MAX_FRAMES_PER_EP = 300\n",
    "\n",
    "        logs = [[]] * self.num_machines\n",
    "        fit_badly_counter = False\n",
    "        \n",
    "        for machine_idx in range(self.num_machines):\n",
    "\n",
    "            machine = self.machines[machine_idx]\n",
    "\n",
    "            total_policy_loss = 0\n",
    "            total_value_loss = 0\n",
    "            fit_badly_count = 0\n",
    "\n",
    "            # select random subset of data\n",
    "            idx = np.random.choice(len(data), batch_size, replace=False)\n",
    "\n",
    "            for i in idx:\n",
    "                sb = data[i]\n",
    "                T = sb['T']\n",
    "                obs = sb['obs']\n",
    "                dist, values = machine.model(obs)\n",
    "                init_logp = sb['log_prob']\n",
    "                old_logp = dist.log_prob(sb['action']).detach()\n",
    "\n",
    "                # add 0 to end of values until it has length MAX_FRAMES_PER_EP\n",
    "                values_extended = torch.cat([values, torch.zeros((MAX_FRAMES_PER_EP - len(values), ), device=DEVICE)], dim=0)\n",
    "                full_reward = torch.cat([sb['reward'], torch.zeros((MAX_FRAMES_PER_EP - len(sb['reward']), ), device=DEVICE)], dim=0)\n",
    "                assert len(values_extended) == MAX_FRAMES_PER_EP\n",
    "                assert len(full_reward) == MAX_FRAMES_PER_EP\n",
    "                \n",
    "                # compute advantage\n",
    "                if machine.args.use_gae:\n",
    "                    advantage = compute_advantage_gae(values_extended, full_reward, T, machine.args.gae_lambda, machine.args.discount)\n",
    "                else:\n",
    "                    advantage = sb['discounted_reward'] - values.reshape(-1)\n",
    "                \n",
    "                # compute policy and value loss\n",
    "                policy_loss, fit_badly = machine._compute_policy_loss_ppo(obs, init_logp, old_logp, sb['action'], advantage)\n",
    "\n",
    "                value_loss = machine._compute_value_loss(obs, sb['discounted_reward'])\n",
    "                \n",
    "                if fit_badly < machine.args.fit_badly_coeff:\n",
    "                    total_policy_loss += policy_loss\n",
    "                    total_value_loss += value_loss\n",
    "                else:\n",
    "                    fit_badly_count += 1\n",
    "            \n",
    "            print(fit_badly_count)\n",
    "            \n",
    "            machine.optim.zero_grad()\n",
    "            loss = total_policy_loss + total_value_loss\n",
    "            loss.backward()\n",
    "            machine.optim.step()\n",
    "            \n",
    "            logs[machine_idx].append({\n",
    "            \"policy_loss\": total_policy_loss,\n",
    "            \"value_loss\": total_value_loss\n",
    "            })\n",
    "\n",
    "            if fit_badly_count / (batch_size * self.num_machines) > 0.5:\n",
    "                fit_badly_counter = True\n",
    "        \n",
    "        return logs, fit_badly_counter\n",
    "\n",
    "    def train(self, data, batch_size, max_steps=50):\n",
    "        \"\"\"\n",
    "        Train each model in the cluster until data fits poorly, or until reaches max_steps.\n",
    "        \"\"\" \n",
    "\n",
    "        logs = []\n",
    "        steps = 0\n",
    "        fit_badly_counter = False\n",
    "    \n",
    "        while not fit_badly_counter:\n",
    "            if steps >= max_steps:\n",
    "                break\n",
    "            new_logs, fit_badly_counter = self.train_one_step(data, batch_size)\n",
    "            logs = [[*i, *j] for i, j in zip(logs, new_logs)]\n",
    "            steps += 1\n",
    "        \n",
    "        return logs\n",
    "            \n",
    "        \n",
    "    def evaluate(self, trained_models):\n",
    "        \"\"\"\n",
    "        Evaluate the performance of the models in the cluster.\n",
    "\n",
    "        Args:\n",
    "        - trained_models: list of ExtendedModels.\n",
    "\n",
    "        Returns:\n",
    "        - performances:\n",
    "        \"\"\"\n",
    "\n",
    "        pass\n",
    "\n",
    "\n",
    "    def update_hp(self, hp:str, targets:list[float]) -> None:\n",
    "        \"\"\"\n",
    "        Update the hyperparameters of the models in the cluster.\n",
    "\n",
    "        Args:\n",
    "            hp: The hyperparameter to update.\n",
    "            targets: The target hyperparameter values.\n",
    "                    Has length self.num_models.\n",
    "        \"\"\"\n",
    "        for i, target in enumerate(targets):\n",
    "            setattr(self.machines[i].args, hp, target)\n",
    "\n",
    "    def eliminate(self, trained_models, performances):\n",
    "        \"\"\"\n",
    "        The elimination process to select the survivors.\n",
    "        Also shift hyperparams.\n",
    "        \"\"\"\n",
    "\n",
    "        pass\n",
    "    \n",
    "    def reproduce(self):\n",
    "        \"\"\"\n",
    "        Reproduce models.\n",
    "        \"\"\"\n",
    "\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cluster_experiment(hps:list[str], num_machines:int=5, num_survivors:int=2, num_trajs:int=16, num_epochs:int=10, seed=42):\n",
    "    \"\"\"\n",
    "    Run the cluster experiment.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    cluster = MachineCluster(hps = hps, num_machines = num_machines, num_survivors = num_survivors, num_trajs = num_trajs)\n",
    "\n",
    "    pbar = tqdm(range(num_epochs))\n",
    "    logs = dict(\n",
    "        rewards = [],\n",
    "        smooth_rewards = []\n",
    "    )\n",
    "    pdlogs = []\n",
    "\n",
    "    for epoch in pbar:\n",
    "        # Generate data\n",
    "        obs = cluster.generate_data()\n",
    "\n",
    "        # Train until data fits poorly\n",
    "        logs = cluster.train(data=obs, batch_size=8)\n",
    "        trained_machines = cluster.machines\n",
    "\n",
    "        # # Evaluate performances\n",
    "        # performances = cluster.evaluate(trained_models)\n",
    "        # logs.append(performances) # what is in logs?\n",
    "\n",
    "        # # Survival of the fittest\n",
    "        # survivors = cluster.eliminate(trained_models, performances)\n",
    "        # cluster.reproduce(survivors)\n",
    "\n",
    "        print(f'Epoch {epoch} done!')\n",
    "\n",
    "    # return pd.DataFrame(pdlogs).set_index('epoch')\n",
    "    return logs, trained_machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, trained_machines = run_cluster_experiment(hps=['entropy_coef'])\n",
    "\n",
    "# df.plot(x='num_frames', y=['reward', 'smooth_reward'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(coefs:list[float], max_steps:int, env=DoorKeyEnv6x6(), args=Config(), seed=0):\n",
    "    \"\"\"\n",
    "    Upper level function for running experiments to analyze reinforce and\n",
    "    policy gradient methods. Instantiates a model, collects epxeriences, and\n",
    "    then updates the neccessary parameters.\n",
    "\n",
    "    Args:\n",
    "        coefs: a list of coefs each machine has\n",
    "        max_steps: maximum number of steps to run the experiment for.\n",
    "        env: the environment to use. \n",
    "        args: Config object with hyperparameters.\n",
    "        seed: random seed. int\n",
    "\n",
    "    Returns:\n",
    "        DataFrame indexed by episode\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    device = DEVICE\n",
    "\n",
    "    machines = [Machine(coef, label=idx, args=args) for idx, coef in enumerate(coefs)]\n",
    "    winners = []\n",
    "    full_rs = []\n",
    "\n",
    "    is_solved = False\n",
    "\n",
    "    SMOOTH_REWARD_WINDOW = 50\n",
    "\n",
    "    pd_logs, rewards = [], [0]*SMOOTH_REWARD_WINDOW\n",
    "\n",
    "    num_frames = 0\n",
    "\n",
    "    pbar = tqdm(range(max_steps))\n",
    "    for update in pbar:\n",
    "\n",
    "        label = update % len(machines)\n",
    "\n",
    "        machine = machines[label]\n",
    "        \n",
    "        exps, logs1 = machine.collect_experiences(env)\n",
    "\n",
    "        for m in machines:\n",
    "            if m is machine:\n",
    "                logs2 = m.update_parameters(exps)\n",
    "            else:\n",
    "                m.update_parameters(exps)\n",
    "\n",
    "        logs = {**logs1, **logs2}\n",
    "\n",
    "        num_frames += logs[\"num_frames\"]\n",
    "\n",
    "        rewards.append(logs[\"return_per_episode\"])\n",
    "\n",
    "        smooth_reward = np.mean(rewards[-SMOOTH_REWARD_WINDOW:])\n",
    "\n",
    "        data = {'episode':update, 'num_frames':num_frames, 'smooth_reward':smooth_reward,\n",
    "               'reward':logs[\"return_per_episode\"], 'policy_loss':logs[\"policy_loss\"]}\n",
    "\n",
    "        if args.use_critic:\n",
    "            data['value_loss'] = logs[\"value_loss\"]\n",
    "\n",
    "        pd_logs.append(data)\n",
    "\n",
    "        pbar.set_postfix(data)\n",
    "\n",
    "        machine.add_r(rewards[-1])\n",
    "        if update % (20 * len(machines)) == 0 and update > 0:\n",
    "            full_rs.append([m.avg_r() for m in machines])\n",
    "            winner = survival_fittest(machines)\n",
    "            winners.append(winner)\n",
    "\n",
    "        # # Early terminate\n",
    "        # if smooth_reward >= args.score_threshold:\n",
    "        #     is_solved = True\n",
    "        #     break\n",
    "\n",
    "    if is_solved:\n",
    "        print('Solved!')\n",
    "\n",
    "    for m in machines:\n",
    "        print(m.avg_r())\n",
    "    print(winners)\n",
    "    print(full_rs)\n",
    "\n",
    "    return pd.DataFrame(pd_logs).set_index('episode')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
