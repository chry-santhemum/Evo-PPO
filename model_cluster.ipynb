{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ningshanma/miniconda3/envs/rl/lib/python3.11/site-packages/gym/envs/registration.py:307: DeprecationWarning: The package name gym_minigrid has been deprecated in favor of minigrid. Please uninstall gym_minigrid and install minigrid with `pip install minigrid`. Future releases will be maintained under the new package name minigrid.\n",
      "  fn()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "import torch.nn.functional as F\n",
    "import gym_minigrid\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from tqdm.notebook import tqdm\n",
    "from gym_minigrid.envs.doorkey import DoorKeyEnv\n",
    "from gym.envs.registration import registry, register\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from typing_extensions import Self\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rc('animation', html='jshtml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Config:\n",
    "    \"\"\"\n",
    "    Stores algorithmic hyperparameters.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                score_threshold=0.93,\n",
    "                discount=0.995,\n",
    "                lr=1e-3,\n",
    "                max_grad_norm=0.5,\n",
    "                log_interval=10,\n",
    "                max_episodes=2000,\n",
    "                gae_lambda=0.95,\n",
    "                use_critic=True,\n",
    "                clip_ratio=0.2,\n",
    "                target_kl=0.01,\n",
    "                fit_badly_coeff=0.05,\n",
    "                train_ac_iters=5,\n",
    "                use_discounted_reward=False,\n",
    "                entropy_coef=0.01,\n",
    "                use_gae=True,\n",
    "                my_var=0):\n",
    "\n",
    "        self.score_threshold = score_threshold # criterion for early stopping. If the rolling average reward (over the last 100 episodes) is greater than it, it ends.\n",
    "        self.discount = discount # discount factor\n",
    "        self.lr = lr # learning rate\n",
    "        self.max_grad_norm = max_grad_norm # the maximum gradient norm (https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html)\n",
    "        self.log_interval = log_interval # logging interval\n",
    "        self.max_episodes = max_episodes # the maximum number of episodes.\n",
    "        self.use_critic = use_critic # whether to use critic or not.\n",
    "        self.clip_ratio = clip_ratio # clip_ratio of PPO.\n",
    "        self.target_kl = target_kl # target KL divergence for early stoping train_ac_iters for PPO\n",
    "        self.fit_badly_coeff = fit_badly_coeff # the bound of approx. KL of when the model fits badly to the data.\n",
    "        self.train_ac_iters = train_ac_iters # how many time to train ac_model using current computed old_logps\n",
    "        self.gae_lambda=gae_lambda # lambda in Generalized Advantage Estimation (GAE)\n",
    "        self.use_discounted_reward=use_discounted_reward # whether use discounted reward or not.\n",
    "        self.entropy_coef = entropy_coef # entropy coefficient for PPO\n",
    "        self.use_gae = use_gae # whether to use GAE or not.\n",
    "        self.my_var = my_var # a variable for testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function from https://github.com/ikostrikov/pytorch-a2c-ppo-acktr/blob/master/model.py\n",
    "def init_params(m):\n",
    "    \"\"\"\n",
    "    Initialize parameters of the network.\n",
    "    m: torch.nn.Module\n",
    "    \"\"\"\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Linear\") != -1:\n",
    "        m.weight.data.normal_(0, 1)\n",
    "        m.weight.data *= 1 / torch.sqrt(m.weight.data.pow(2).sum(1, keepdim=True))\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0)\n",
    "\n",
    "def preprocess_obss(obss, device=None):\n",
    "    \"\"\"\n",
    "    Convert observation into Torch.Tensor\n",
    "\n",
    "    Parameters\n",
    "    ----\n",
    "    obss: dictionary or np.ndarray\n",
    "    device: target device of torch.Tensor ('cpu', 'cuda')\n",
    "\n",
    "    Return\n",
    "    ----\n",
    "    Torch Tensor\n",
    "    \"\"\"\n",
    "    if isinstance(obss, dict):\n",
    "        images = np.array([obss[\"image\"]])\n",
    "    else:\n",
    "        images = np.array([o[\"image\"] for o in obss])\n",
    "\n",
    "    return torch.tensor(images, device=device, dtype=torch.float)\n",
    "\n",
    "class DoorKeyEnv5x5(DoorKeyEnv):\n",
    "    def __init__(self):\n",
    "        self.render_mode = \"rgb_array\"\n",
    "        super().__init__(size=5)\n",
    "\n",
    "    def _reward(self):\n",
    "        \"\"\"\n",
    "        Compute the reward to be given upon success\n",
    "        \"\"\"\n",
    "        return 1\n",
    "    \n",
    "class DoorKeyEnv6x6(DoorKeyEnv):\n",
    "    def __init__(self):\n",
    "        self.render_mode = \"rgb_array\"\n",
    "        super().__init__(size=6)\n",
    "\n",
    "    def _reward(self):\n",
    "        \"\"\"\n",
    "        Compute the reward to be given upon success\n",
    "        \"\"\"\n",
    "        return 1\n",
    "    \n",
    "class DoorKeyEnv8x8(DoorKeyEnv):\n",
    "    def __init__(self):\n",
    "        self.render_mode = \"rgb_array\"\n",
    "        super().__init__(size=8)\n",
    "\n",
    "    def _reward(self):\n",
    "        \"\"\"\n",
    "        Compute the reward to be given upon success\n",
    "        \"\"\"\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2a2927950>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = DoorKeyEnv6x6() # define environment.\n",
    "env.reset()\n",
    "action = env.action_space.sample()\n",
    "obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "# get an RGB image corresponding to the whole environment or the agent's point of view (https://github.com/Farama-Foundation/Minigrid/blob/master/minigrid/minigrid_env.py#L716)\n",
    "#            highlight (bool): If true, the agent's field of view or point of view is highlighted with a lighter gray color.\n",
    "#            tile_size (int): How many pixels will form a tile from the NxM grid.\n",
    "#            agent_pov (bool): If true, the rendered frame will only contain the point of view of the agent.\n",
    "frame = env.get_frame(highlight=env.highlight, tile_size=env.tile_size, agent_pov=env.agent_pov)\n",
    "# show an image to the notebook.\n",
    "plt.imshow(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACModel(nn.Module):\n",
    "    def __init__(self, num_actions, use_critic=True):\n",
    "        \"\"\"\n",
    "        Represents an Actor Crictic model that takes a 2d, multi-channeled\n",
    "        image as input.\n",
    "\n",
    "        Parameters\n",
    "        ----\n",
    "        num_actions : int\n",
    "\n",
    "                      The action space of the environment.\n",
    "                      The action space for DoorKey5x5 is 7-dimensional:\n",
    "                      0: turn left,\n",
    "                      1: turn right,\n",
    "                      2: forward,\n",
    "                      3: pickup an object,\n",
    "                      4: drop an object,\n",
    "                      5: activate an object,\n",
    "                      6: done completing task\n",
    "\n",
    "        use_critics : bool\n",
    "\n",
    "                      Critic network will be used in forward pass if flag is set\n",
    "                      to true.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.use_critic = use_critic\n",
    "\n",
    "        # Define actor's model\n",
    "        self.image_conv_actor = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, (2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 2)),\n",
    "            nn.Conv2d(16, 32, (2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, (2, 2)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, num_actions)\n",
    "        )\n",
    "\n",
    "        # Define critic's model\n",
    "        if self.use_critic:\n",
    "            self.image_conv_critic = nn.Sequential(\n",
    "                nn.Conv2d(3, 16, (2, 2)),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d((2, 2)),\n",
    "                nn.Conv2d(16, 32, (2, 2)),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(32, 64, (2, 2)),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "            self.critic = nn.Sequential(\n",
    "                nn.Linear(64, 64),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(64, 1)\n",
    "            )\n",
    "\n",
    "        # Initialize parameters correctly\n",
    "        self.apply(init_params)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        \"\"\"\n",
    "        Performs a forward pass through the actor-critic network\n",
    "\n",
    "        Parameters\n",
    "        ----\n",
    "        obs : int tensor. Shape [Batch size, ImWidth, ImHeight, Channels]\n",
    "\n",
    "              input to the network.\n",
    "        ----\n",
    "\n",
    "        returns:\n",
    "\n",
    "        dist : torch.distribution\n",
    "            The distribution of actions from policy. A Categorical distribution\n",
    "            for discreet action spaces.\n",
    "        value : torch.Tensor (Batch size, 1)\n",
    "            value output by critic network\n",
    "        \"\"\"\n",
    "        conv_in = obs.transpose(1, 3).transpose(2, 3) # reshape into [b, c, h, w]\n",
    "\n",
    "        dist, value = None, None\n",
    "\n",
    "        x = self.image_conv_actor(conv_in)\n",
    "        embedding = x.reshape(x.shape[0], -1)\n",
    "\n",
    "        x = self.actor(embedding)\n",
    "        dist = Categorical(logits=F.log_softmax(x, dim=1))\n",
    "\n",
    "        if self.use_critic:\n",
    "            y = self.image_conv_critic(conv_in)\n",
    "            embedding = y.reshape(y.shape[0], -1)\n",
    "\n",
    "            value = self.critic(embedding).squeeze(1)\n",
    "        else:\n",
    "            value = torch.zeros((x.shape[0], 1), device=x.device)\n",
    "        \n",
    "        return dist, value\n",
    "\n",
    "def compute_discounted_return(rewards, discount, device=DEVICE):\n",
    "    \"\"\"\n",
    "\t\trewards: reward obtained at timestep.  Shape: (T,)\n",
    "\t\tdiscount: discount factor. float\n",
    "\n",
    "    ----\n",
    "    returns: sum of discounted rewards. Shape: (T,)\n",
    "\t\t\"\"\"\n",
    "    returns = torch.zeros(*rewards.shape, device=device)\n",
    "\n",
    "    R = 0\n",
    "    for t in reversed(range((rewards.shape[0]))):\n",
    "        R = rewards[t] + discount * R\n",
    "        returns[t] = R\n",
    "    return returns\n",
    "\n",
    "def compute_advantage_gae(values, rewards, T, gae_lambda, discount):\n",
    "    \"\"\"\n",
    "    Compute Adavantage with GAE. See Section 4.4.2 in the lecture notes.\n",
    "\n",
    "    values: value at each timestep (T,)\n",
    "    rewards: reward obtained at each timestep.  Shape: (T,)\n",
    "    T: the number of frames, float\n",
    "    gae_lambda: hyperparameter, float\n",
    "    discount: discount factor, float\n",
    "\n",
    "    -----\n",
    "\n",
    "    returns:\n",
    "\n",
    "    advantages : tensor.float. Shape [T,]\n",
    "\n",
    "                 gae advantage term for timesteps 0 to T\n",
    "\n",
    "    \"\"\"\n",
    "    advantages = torch.zeros_like(values)\n",
    "    for i in reversed(range(T)):\n",
    "        next_value = values[i+1]\n",
    "        next_advantage = advantages[i+1]\n",
    "\n",
    "        delta = rewards[i] + discount * next_value  - values[i]\n",
    "        advantages[i] = delta + discount * gae_lambda * next_advantage\n",
    "    return advantages[:T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:64] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Categorical(logits: torch.Size([1, 7])),\n",
       " tensor([0.0451], grad_fn=<SqueezeBackward1>))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TESTS\n",
    "\n",
    "model = ACModel(num_actions=7, use_critic=True).to(DEVICE)\n",
    "obss = env.reset()\n",
    "obs = preprocess_obss(obss[0], DEVICE)\n",
    "model(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Machine:\n",
    "    def __init__(self, num_actions=7, args=Config()):\n",
    "        \"\"\"\n",
    "        A Machine object consists of a ACModel, its hyperparameters, and its history of rewards.\n",
    "\n",
    "        Args:\n",
    "            num_actions: size of action space\n",
    "            args: Model hyperparameters\n",
    "        \"\"\"\n",
    "        self.args = args\n",
    "        self.model = ACModel(num_actions, use_critic=True).to(DEVICE)\n",
    "        self.optim = torch.optim.Adam(self.model.parameters(), lr=self.args.lr)\n",
    "        self.rewards = []\n",
    "\n",
    "    def reset_rs(self) -> None:\n",
    "        self.rewards = []\n",
    "\n",
    "    def copy_machine(self, other: Self) -> None:\n",
    "        \"\"\"\n",
    "        Copy state dict from 'other'. Reset rs.\n",
    "        \"\"\"\n",
    "        state_dict = other.model.state_dict()\n",
    "        for key, v in state_dict.items():\n",
    "            self.model.state_dict()[key].copy_(v)\n",
    "        \n",
    "        self.reset_rs()\n",
    "\n",
    "    def add_r(self, r: float) -> None:\n",
    "        self.rewards.append(r)\n",
    "\n",
    "    def avg_r(self) -> float:\n",
    "        assert len(self.rewards) > 0, 'No rewards yet'\n",
    "        return sum(self.rewards) / len(self.rewards)\n",
    "    \n",
    "    def collect_experiences(self, env:gym.Env):\n",
    "        \"\"\"\n",
    "        Collects rollouts and computes advantages.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        exps : dict\n",
    "            Contains actions, rewards, advantages etc as attributes.\n",
    "            Each attribute, e.g. `exps['reward']` has a shape\n",
    "            (self.num_frames, ...).\n",
    "        logs : dict\n",
    "            Useful stats about the training process, including the average\n",
    "            reward, policy loss, value loss, etc.\n",
    "        \"\"\"\n",
    "        acmodel = self.model\n",
    "\n",
    "        MAX_FRAMES_PER_EP = 300\n",
    "        shape = (MAX_FRAMES_PER_EP, )\n",
    "\n",
    "        actions = torch.zeros(*shape, device=DEVICE, dtype=torch.int)\n",
    "        values = torch.zeros(*shape, device=DEVICE)\n",
    "        rewards = torch.zeros(*shape, device=DEVICE)\n",
    "        log_probs = torch.zeros(*shape, device=DEVICE)\n",
    "        obss = [None]*MAX_FRAMES_PER_EP\n",
    "\n",
    "        obs, _ = env.reset()\n",
    "\n",
    "        total_return = 0\n",
    "\n",
    "        T = 0\n",
    "\n",
    "        while True:\n",
    "            # Do one agent-environment interaction\n",
    "\n",
    "            preprocessed_obs = preprocess_obss(obs, device=DEVICE)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                dist, value = acmodel(preprocessed_obs)\n",
    "\n",
    "            dist: Categorical\n",
    "            action = dist.sample()[0]\n",
    "\n",
    "            obss[T] = obs\n",
    "            obs, reward, done, _, _ = env.step(action.item())\n",
    "\n",
    "\n",
    "            # Update experiences values\n",
    "            actions[T] = action\n",
    "            values[T] = value\n",
    "            rewards[T] = reward\n",
    "            log_probs[T] = dist.log_prob(action)\n",
    "\n",
    "\n",
    "            total_return += reward\n",
    "            T += 1\n",
    "\n",
    "            if done or T>=MAX_FRAMES_PER_EP-1:\n",
    "                break\n",
    "\n",
    "        discounted_reward = compute_discounted_return(rewards[:T], self.args.discount, DEVICE)\n",
    "        exps = dict(\n",
    "            obs = preprocess_obss([\n",
    "                obss[i]\n",
    "                for i in range(T)\n",
    "            ], device=DEVICE),\n",
    "            action = actions[:T],\n",
    "            value  = values[:T],\n",
    "            reward = rewards[:T],\n",
    "            log_prob = log_probs[:T],\n",
    "            discounted_reward = discounted_reward,\n",
    "            T = T\n",
    "        )\n",
    "\n",
    "        logs = {\n",
    "            \"return_per_episode\": total_return,\n",
    "            \"num_frames\": T\n",
    "        }\n",
    "\n",
    "        return exps, logs\n",
    "\n",
    "    def _compute_policy_loss_ppo(self, obs, init_logp, old_logp, actions, advantages):\n",
    "        \"\"\"\n",
    "        Computes the policy loss for PPO.\n",
    "\n",
    "        obs: observeration to pass into acmodel. shape: (T,)\n",
    "        init_logp: log probabilities we get from the agent performing the action. shape: (T,)\n",
    "        old_logp: log probabilities from previous timestep. shape: (T,)\n",
    "        actions: action at this timestep. shape: (T,ImWidth,ImHeight,Channels)\n",
    "        advantages: the computed advantages. shape: (T,)\n",
    "\n",
    "        ---\n",
    "        returns\n",
    "\n",
    "        policy_loss : ppo policy loss as shown in line 6 of PPO alg. tensor.float. Shape (,1)\n",
    "        approx_kl: an appoximation of the kl_divergence. tensor.float. Shape (,1)\n",
    "        \"\"\"\n",
    "        policy_loss, approx_kl = 0, 0\n",
    "\n",
    "        dist, _ = self.model(obs)\n",
    "        dist: Categorical\n",
    "\n",
    "        entropy = dist.entropy()\n",
    "        logps = dist.log_prob(actions)\n",
    "        r_terms = torch.exp(logps - old_logp)\n",
    "        ppo_loss = torch.min(r_terms * advantages, torch.clamp(r_terms, 1 - self.args.clip_ratio, 1 + self.args.clip_ratio) * advantages)\n",
    "        \n",
    "        # Importance sampling factor\n",
    "        factors = torch.exp(old_logp - init_logp)\n",
    "        \n",
    "        indices = factors < 100\n",
    "\n",
    "        policy_loss_tensor = factors * ppo_loss + self.args.entropy_coef * entropy\n",
    "\n",
    "        policy_loss = - torch.mean(policy_loss_tensor[indices])\n",
    "\n",
    "        # approx_kl = torch.sum(torch.exp(old_logp) * (old_logp - logps)) / torch.sum(torch.exp(old_logp))\n",
    "        # approx_kl = torch.mean((old_logp - logps) ** 2) / 2\n",
    "        \n",
    "        fit_badly = torch.mean((old_logp - init_logp) ** 2)\n",
    "\n",
    "        return policy_loss, fit_badly\n",
    "    \n",
    "    def _compute_value_loss(self, obs, returns):\n",
    "        _, value = self.model(obs)\n",
    "\n",
    "        value_loss = torch.mean((value - returns) ** 2)\n",
    "\n",
    "        return value_loss\n",
    "\n",
    "\n",
    "    def update_parameters(self, sb):\n",
    "        \n",
    "        MAX_FRAMES_PER_EP = 300\n",
    "        T = sb['T']\n",
    "        dist, values = self.model(sb['obs'])\n",
    "        values = values.reshape(-1)\n",
    "        dist: Categorical\n",
    "        old_logp = dist.log_prob(sb['action']).detach()\n",
    "        init_logp = sb['log_prob']\n",
    "\n",
    "        # add 0 to end of values until it has length MAX_FRAMES_PER_EP\n",
    "        values_extended = torch.cat([values, torch.zeros((MAX_FRAMES_PER_EP - len(values), ), device=DEVICE)], dim=0)\n",
    "        full_reward = torch.cat([sb['reward'], torch.zeros((MAX_FRAMES_PER_EP - len(sb['reward']), ), device=DEVICE)], dim=0)\n",
    "\n",
    "        if self.args.use_gae:\n",
    "            advantage = compute_advantage_gae(values_extended, full_reward, T, self.args.gae_lambda, self.args.discount)\n",
    "        else:\n",
    "            advantage = sb['discounted_reward'] - values.reshape(-1)\n",
    "\n",
    "        policy_loss, _ = self._compute_policy_loss_ppo(sb['obs'], init_logp, old_logp, sb['action'], advantage)\n",
    "        value_loss = self._compute_value_loss(sb['obs'], sb['discounted_reward'])\n",
    "\n",
    "        for i in range(self.args.train_ac_iters):\n",
    "            self.optim.zero_grad()\n",
    "            loss_pi, approx_kl = self._compute_policy_loss_ppo(sb['obs'], init_logp, old_logp, sb['action'], advantage)\n",
    "            if sb['label'] == self.label:\n",
    "                loss_v = self._compute_value_loss(sb['obs'], sb['discounted_reward'])\n",
    "            else:\n",
    "                loss_v = 0.0\n",
    "\n",
    "            loss = loss_v + loss_pi\n",
    "            if approx_kl > 1.5 * self.args.target_kl:\n",
    "                break\n",
    "\n",
    "            loss.backward(retain_graph=True)\n",
    "            self.optim.step()\n",
    "\n",
    "        update_policy_loss = policy_loss.item()\n",
    "        update_value_loss = value_loss.item()\n",
    "\n",
    "        logs = {\n",
    "            \"policy_loss\": update_policy_loss,\n",
    "            \"value_loss\": update_value_loss,\n",
    "        }\n",
    "\n",
    "        return logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MachineCluster:\n",
    "    def __init__(self, hps:list[str], num_machines:int=5, num_survivors:int=2, num_trajs:int=16):\n",
    "        \"\"\"\n",
    "        The cluster of machines.\n",
    "\n",
    "        hps: A list of hyperparameters to optimize. Must be a subset of keys in Machine.args.\n",
    "        num_machines: The number of machines in the cluster.\n",
    "        num_survivors: The number of survivors to select from the cluster.\n",
    "        num_trajs: The number of trajectories collected by each machine in each training epoch.\n",
    "        batch_size: The number of trajectories used in each gradient step.\n",
    "        \"\"\"\n",
    "        self.num_machines = num_machines\n",
    "        self.num_survivors = num_survivors\n",
    "        self.hps = hps\n",
    "        self.num_trajs = num_trajs\n",
    "        self.machines = []\n",
    "        for _ in range(self.num_machines):\n",
    "            self.machines.append(Machine())\n",
    "\n",
    "        # now let's say hps = ['entropy_coef']\n",
    "        for hp in hps:\n",
    "            # sample num_models values from a uniform distribution\n",
    "            values = np.random.uniform(0, 1, size=num_machines)\n",
    "            # set the hyperparameters of the models\n",
    "            self.update_hp(hp, values)\n",
    "\n",
    "    def generate_data(self):\n",
    "        \"\"\"\n",
    "        Generate batches of data for training the models.\n",
    "\n",
    "        Return:\n",
    "            trajs: list of exps.  \n",
    "        \"\"\"\n",
    "\n",
    "        trajs = []\n",
    "\n",
    "        for i in range(self.num_machines):\n",
    "            for _ in range(self.num_trajs):\n",
    "                exps, _ = self.machines[i].collect_experiences(env)\n",
    "                trajs.append(exps)\n",
    "            \n",
    "            print(f'Generated data for machine {i}.')\n",
    "        \n",
    "        return trajs\n",
    "\n",
    "    def train_one_step(self, data, batch_size):\n",
    "        \"\"\"\n",
    "        Do one step of optimization on each model in the cluster.\n",
    "\n",
    "        Args:\n",
    "            data: list of exps.\n",
    "            batch_size: number of trajectories used in each gradient step.\n",
    "\n",
    "        Return:\n",
    "            logs\n",
    "        \"\"\"\n",
    "\n",
    "        MAX_FRAMES_PER_EP = 300\n",
    "\n",
    "        logs = [[]] * self.num_machines\n",
    "        fit_badly_counter = False\n",
    "        \n",
    "        for machine_idx in range(self.num_machines):\n",
    "\n",
    "            machine = self.machines[machine_idx]\n",
    "\n",
    "            total_policy_loss = 0\n",
    "            total_value_loss = 0\n",
    "            fit_badly_count = 0\n",
    "\n",
    "            # select random subset of data\n",
    "            idx = np.random.choice(len(data), batch_size, replace=False)\n",
    "\n",
    "            for i in idx:\n",
    "                sb = data[i]\n",
    "                T = sb['T']\n",
    "                obs = sb['obs']\n",
    "                dist, values = machine.model(obs)\n",
    "                init_logp = sb['log_prob']\n",
    "                old_logp = dist.log_prob(sb['action']).detach()\n",
    "\n",
    "                # add 0 to end of values until it has length MAX_FRAMES_PER_EP\n",
    "                values_extended = torch.cat([values, torch.zeros((MAX_FRAMES_PER_EP - len(values), ), device=DEVICE)], dim=0)\n",
    "                full_reward = torch.cat([sb['reward'], torch.zeros((MAX_FRAMES_PER_EP - len(sb['reward']), ), device=DEVICE)], dim=0)\n",
    "                assert len(values_extended) == MAX_FRAMES_PER_EP\n",
    "                assert len(full_reward) == MAX_FRAMES_PER_EP\n",
    "                \n",
    "                # compute advantage\n",
    "                if machine.args.use_gae:\n",
    "                    advantage = compute_advantage_gae(values_extended, full_reward, T, machine.args.gae_lambda, machine.args.discount)\n",
    "                else:\n",
    "                    advantage = sb['discounted_reward'] - values.reshape(-1)\n",
    "                \n",
    "                # compute policy and value loss\n",
    "                policy_loss, fit_badly = machine._compute_policy_loss_ppo(obs, init_logp, old_logp, sb['action'], advantage)\n",
    "\n",
    "                value_loss = machine._compute_value_loss(obs, sb['discounted_reward'])\n",
    "                \n",
    "                if fit_badly < machine.args.fit_badly_coeff:\n",
    "                    total_policy_loss += policy_loss\n",
    "                    total_value_loss += value_loss\n",
    "                else:\n",
    "                    fit_badly_count += 1\n",
    "            \n",
    "            print(fit_badly_count)\n",
    "            \n",
    "            machine.optim.zero_grad()\n",
    "            loss = total_policy_loss + total_value_loss\n",
    "            loss.backward()\n",
    "            machine.optim.step()\n",
    "            \n",
    "            logs[machine_idx].append({\n",
    "            \"policy_loss\": total_policy_loss,\n",
    "            \"value_loss\": total_value_loss\n",
    "            })\n",
    "\n",
    "            if fit_badly_count / (batch_size * self.num_machines) > 0.5:\n",
    "                fit_badly_counter = True\n",
    "        \n",
    "        return logs, fit_badly_counter\n",
    "\n",
    "    def train(self, data, batch_size, max_steps=50):\n",
    "        \"\"\"\n",
    "        Train each model in the cluster until data fits poorly, or until reaches max_steps.\n",
    "        \"\"\" \n",
    "\n",
    "        logs = []\n",
    "        steps = 0\n",
    "        fit_badly_counter = False\n",
    "    \n",
    "        while not fit_badly_counter:\n",
    "            if steps >= max_steps:\n",
    "                break\n",
    "            new_logs, fit_badly_counter = self.train_one_step(data, batch_size)\n",
    "            logs = [[*i, *j] for i, j in zip(logs, new_logs)]\n",
    "            steps += 1\n",
    "        \n",
    "        return logs\n",
    "            \n",
    "        \n",
    "    def evaluate(self, trained_models):\n",
    "        \"\"\"\n",
    "        Evaluate the performance of the models in the cluster.\n",
    "\n",
    "        Args:\n",
    "        - trained_models: list of ExtendedModels.\n",
    "\n",
    "        Returns:\n",
    "        - performances:\n",
    "        \"\"\"\n",
    "\n",
    "        pass\n",
    "\n",
    "\n",
    "    def update_hp(self, hp:str, targets:list[float]) -> None:\n",
    "        \"\"\"\n",
    "        Update the hyperparameters of the models in the cluster.\n",
    "\n",
    "        Args:\n",
    "            hp: The hyperparameter to update.\n",
    "            targets: The target hyperparameter values.\n",
    "                    Has length self.num_models.\n",
    "        \"\"\"\n",
    "        for i, target in enumerate(targets):\n",
    "            setattr(self.machines[i].args, hp, target)\n",
    "\n",
    "    def eliminate(self, trained_models, performances):\n",
    "        \"\"\"\n",
    "        The elimination process to select the survivors.\n",
    "        Also shift hyperparams.\n",
    "        \"\"\"\n",
    "\n",
    "        pass\n",
    "    \n",
    "    def reproduce(self):\n",
    "        \"\"\"\n",
    "        Reproduce models.\n",
    "        \"\"\"\n",
    "\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cluster_experiment(hps:list[str], num_machines:int=5, num_survivors:int=2, num_trajs:int=16, num_epochs:int=10, seed=42):\n",
    "    \"\"\"\n",
    "    Run the cluster experiment.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    cluster = MachineCluster(hps = hps, num_machines = num_machines, num_survivors = num_survivors, num_trajs = num_trajs)\n",
    "\n",
    "    pbar = tqdm(range(num_epochs))\n",
    "    logs = dict(\n",
    "        rewards = [],\n",
    "        smooth_rewards = []\n",
    "    )\n",
    "    pdlogs = []\n",
    "\n",
    "    for epoch in pbar:\n",
    "        # Generate data\n",
    "        obs = cluster.generate_data()\n",
    "\n",
    "        # Train until data fits poorly\n",
    "        logs = cluster.train(data=obs, batch_size=8)\n",
    "        trained_machines = cluster.machines\n",
    "\n",
    "        # # Evaluate performances\n",
    "        # performances = cluster.evaluate(trained_models)\n",
    "        # logs.append(performances) # what is in logs?\n",
    "\n",
    "        # # Survival of the fittest\n",
    "        # survivors = cluster.eliminate(trained_models, performances)\n",
    "        # cluster.reproduce(survivors)\n",
    "\n",
    "        print(f'Epoch {epoch} done!')\n",
    "\n",
    "    # return pd.DataFrame(pdlogs).set_index('epoch')\n",
    "    return logs, trained_machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, trained_machines = run_cluster_experiment(hps=['entropy_coef'])\n",
    "\n",
    "# df.plot(x='num_frames', y=['reward', 'smooth_reward'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(coefs:list[float], max_steps:int, env=DoorKeyEnv6x6(), args=Config(), seed=0):\n",
    "    \"\"\"\n",
    "    Upper level function for running experiments to analyze reinforce and\n",
    "    policy gradient methods. Instantiates a model, collects epxeriences, and\n",
    "    then updates the neccessary parameters.\n",
    "\n",
    "    Args:\n",
    "        coefs: a list of coefs each machine has\n",
    "        max_steps: maximum number of steps to run the experiment for.\n",
    "        env: the environment to use. \n",
    "        args: Config object with hyperparameters.\n",
    "        seed: random seed. int\n",
    "\n",
    "    Returns:\n",
    "        DataFrame indexed by episode\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    device = DEVICE\n",
    "\n",
    "    machines = [Machine(coef, label=idx, args=args) for idx, coef in enumerate(coefs)]\n",
    "    winners = []\n",
    "    full_rs = []\n",
    "\n",
    "    is_solved = False\n",
    "\n",
    "    SMOOTH_REWARD_WINDOW = 50\n",
    "\n",
    "    pd_logs, rewards = [], [0]*SMOOTH_REWARD_WINDOW\n",
    "\n",
    "    num_frames = 0\n",
    "\n",
    "    pbar = tqdm(range(max_steps))\n",
    "    for update in pbar:\n",
    "\n",
    "        label = update % len(machines)\n",
    "\n",
    "        machine = machines[label]\n",
    "        \n",
    "        exps, logs1 = machine.collect_experiences(env)\n",
    "\n",
    "        for m in machines:\n",
    "            if m is machine:\n",
    "                logs2 = m.update_parameters(exps)\n",
    "            else:\n",
    "                m.update_parameters(exps)\n",
    "\n",
    "        logs = {**logs1, **logs2}\n",
    "\n",
    "        num_frames += logs[\"num_frames\"]\n",
    "\n",
    "        rewards.append(logs[\"return_per_episode\"])\n",
    "\n",
    "        smooth_reward = np.mean(rewards[-SMOOTH_REWARD_WINDOW:])\n",
    "\n",
    "        data = {'episode':update, 'num_frames':num_frames, 'smooth_reward':smooth_reward,\n",
    "               'reward':logs[\"return_per_episode\"], 'policy_loss':logs[\"policy_loss\"]}\n",
    "\n",
    "        if args.use_critic:\n",
    "            data['value_loss'] = logs[\"value_loss\"]\n",
    "\n",
    "        pd_logs.append(data)\n",
    "\n",
    "        pbar.set_postfix(data)\n",
    "\n",
    "        machine.add_r(rewards[-1])\n",
    "        if update % (20 * len(machines)) == 0 and update > 0:\n",
    "            full_rs.append([m.avg_r() for m in machines])\n",
    "            winner = survival_fittest(machines)\n",
    "            winners.append(winner)\n",
    "\n",
    "        # # Early terminate\n",
    "        # if smooth_reward >= args.score_threshold:\n",
    "        #     is_solved = True\n",
    "        #     break\n",
    "\n",
    "    if is_solved:\n",
    "        print('Solved!')\n",
    "\n",
    "    for m in machines:\n",
    "        print(m.avg_r())\n",
    "    print(winners)\n",
    "    print(full_rs)\n",
    "\n",
    "    return pd.DataFrame(pd_logs).set_index('episode')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'registry' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 372\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m gym\u001b[38;5;241m.\u001b[39mspaces\u001b[38;5;241m.\u001b[39mBox(\n\u001b[1;32m    367\u001b[0m             low\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf, \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf, \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf, \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf]),\n\u001b[1;32m    368\u001b[0m             high\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray([np\u001b[38;5;241m.\u001b[39minf,   np\u001b[38;5;241m.\u001b[39minf,  np\u001b[38;5;241m.\u001b[39minf,  np\u001b[38;5;241m.\u001b[39minf])\n\u001b[1;32m    369\u001b[0m         )\n\u001b[1;32m    371\u001b[0m env_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCartpoleSwingUp-v0\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m env_name \u001b[38;5;129;01min\u001b[39;00m \u001b[43mregistry\u001b[49m\u001b[38;5;241m.\u001b[39menv_specs:\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m registry\u001b[38;5;241m.\u001b[39menv_specs[env_name]\n\u001b[1;32m    374\u001b[0m register(\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39menv_name,\n\u001b[1;32m    376\u001b[0m     entry_point\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:CartpoleGym\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    377\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'registry' is not defined"
     ]
    }
   ],
   "source": [
    "class CartpoleDynamics:\n",
    "    def __init__(self,\n",
    "                 timestep=0.02,\n",
    "                 m_p=0.5,\n",
    "                 m_c=0.5,\n",
    "                 l=0.6,\n",
    "                 g=-9.81,\n",
    "                 u_range=15):\n",
    "        \"\"\"\n",
    "        Initializes the Cartpole Dynamics model with given parameters.\n",
    "\n",
    "        Parameters:\n",
    "        - timestep (float): The time step for the simulation.\n",
    "        - m_p (float): Mass of the pole.\n",
    "        - m_c (float): Mass of the cart.\n",
    "        - l (float): Length of the pole.\n",
    "        - g (float): Acceleration due to gravity. Negative values indicate direction.\n",
    "        - u_range (float): Range of the control input.\n",
    "        \"\"\"\n",
    "\n",
    "        self.m_p  = m_p\n",
    "        self.m_c  = m_c\n",
    "        self.l    = l\n",
    "        self.g    = -g\n",
    "        self.dt   = timestep\n",
    "\n",
    "        self.u_range = u_range\n",
    "\n",
    "        self.u_lb = torch.tensor([-1]).float()\n",
    "        self.u_ub = torch.tensor([1]).float()\n",
    "        self.q_shape = 4\n",
    "        self.u_shape = 1\n",
    "\n",
    "    def _qdotdot(self, q, u):\n",
    "        \"\"\"\n",
    "        Calculates the acceleration of both cart and pole as a function of the current state and control input.\n",
    "\n",
    "        Parameters:\n",
    "        - q (torch.Tensor): The current state of the system, [x, theta, xdot, thetadot].\n",
    "        - u (torch.Tensor): The current control input.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: Accelerations [x_dotdot, theta_dotdot] of the cart and the pole.\n",
    "        \"\"\"\n",
    "        x, theta, xdot, thetadot = q.T\n",
    "\n",
    "        if len(u.shape) == 2:\n",
    "            u = torch.flatten(u)\n",
    "\n",
    "        x_dotdot = (\n",
    "            u + self.m_p * torch.sin(theta) * (\n",
    "                self.l * torch.pow(thetadot,2) + self.g * torch.cos(theta)\n",
    "            )\n",
    "        ) / (self.m_c + self.m_p * torch.sin(theta)**2)\n",
    "\n",
    "        theta_dotdot = (\n",
    "            -u*torch.cos(theta) -\n",
    "            self.m_p * self.l * torch.pow(thetadot,2) * torch.cos(theta) * torch.sin(theta) -\n",
    "            (self.m_c + self.m_p) * self.g * torch.sin(theta)\n",
    "        ) / (self.l * (self.m_c + self.m_p * torch.sin(theta)**2))\n",
    "\n",
    "        return torch.stack((x_dotdot, theta_dotdot), dim=-1)\n",
    "\n",
    "    def _euler_int(self, q, qdotdot):\n",
    "        \"\"\"\n",
    "        Performs Euler integration to calculate the new state given the current state and accelerations.\n",
    "\n",
    "        Parameters:\n",
    "        - q (torch.Tensor): The current state of the system, [x, theta, xdot, thetadot].\n",
    "        - qdotdot (torch.Tensor): The accelerations [x_dotdot, theta_dotdot] of the cart and the pole.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: The new state of the system after a single time step.\n",
    "        \"\"\"\n",
    "\n",
    "        qdot_new = q[...,2:] + qdotdot * self.dt\n",
    "        q_new = q[...,:2] + self.dt * qdot_new\n",
    "\n",
    "        return torch.cat((q_new, qdot_new), dim=-1)\n",
    "\n",
    "    def step(self, q, u):\n",
    "        \"\"\"\n",
    "        Performs a single step of simulation given the current state and control input.\n",
    "\n",
    "        Parameters:\n",
    "        - q (torch.Tensor or np.ndarray): The current state of the system.\n",
    "        - u (torch.Tensor or np.ndarray): The current control input.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: The new state of the system after the step.\n",
    "        \"\"\"\n",
    "\n",
    "        # Check for numpy array\n",
    "        if isinstance(q, np.ndarray):\n",
    "            q = torch.from_numpy(q)\n",
    "        if isinstance(u, np.ndarray):\n",
    "            u = torch.from_numpy(u)\n",
    "\n",
    "        scaled_u = u * float(self.u_range)\n",
    "\n",
    "        # Check for shape issues\n",
    "        if len(q.shape) == 2:\n",
    "            q_dotdot = self._qdotdot(q, scaled_u)\n",
    "        elif len(q.shape) == 1:\n",
    "            q_dotdot = self._qdotdot(q.reshape(1,-1), scaled_u)\n",
    "        else:\n",
    "            raise RuntimeError('Invalid q shape')\n",
    "\n",
    "        new_q = self._euler_int(q, q_dotdot)\n",
    "\n",
    "        if len(q.shape) == 1:\n",
    "            new_q = new_q[0]\n",
    "\n",
    "        return new_q\n",
    "\n",
    "    # given q [bs, q_shape] and u [bs, t, u_shape] run the trajectories\n",
    "    def run_batch_of_trajectories(self, q, u):\n",
    "        \"\"\"\n",
    "        Simulates a batch of trajectories given initial states and control inputs over time.\n",
    "\n",
    "        Parameters:\n",
    "        - q (torch.Tensor): Initial states for each trajectory in the batch.\n",
    "        - u (torch.Tensor): Control inputs for each trajectory over time.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: The states of the system at each time step for each trajectory.\n",
    "        \"\"\"\n",
    "        qs = [q]\n",
    "\n",
    "        for t in range(u.shape[1]):\n",
    "            qs.append(self.step(qs[-1], u[:,t]))\n",
    "\n",
    "        return torch.stack(qs, dim=1)\n",
    "\n",
    "    # given q [bs, t, q_shape] and u [bs, t, u_shape] calculate the rewards\n",
    "    def reward(self, q, u):\n",
    "        \"\"\"\n",
    "        Calculates the reward for given states and control inputs.\n",
    "\n",
    "        Parameters:\n",
    "        - q (torch.Tensor or np.ndarray): States of the system.\n",
    "        - u (torch.Tensor or np.ndarray): Control inputs applied.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: The calculated rewards for the states and inputs.\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(q, np.ndarray):\n",
    "            q = torch.from_numpy(q)\n",
    "        if isinstance(u, np.ndarray):\n",
    "            u = torch.from_numpy(u)\n",
    "\n",
    "        angle_term = 0.5*(1-torch.cos(q[...,1]))\n",
    "        pos_term = -0.5*torch.pow(q[...,0],2)\n",
    "        ctrl_cost = -0.001*(u**2).sum(dim=-1)\n",
    "\n",
    "        return angle_term + pos_term + ctrl_cost\n",
    "\n",
    "class CartpoleGym(gym.Env):\n",
    "    def __init__(self, timestep_limit=200):\n",
    "        \"\"\"\n",
    "        Initializes the Cartpole environment with a specified time step limit.\n",
    "\n",
    "        Parameters:\n",
    "        - timestep_limit (int): The maximum number of timesteps for each episode.\n",
    "\n",
    "        Sets up the dynamics model and initializes the simulation state.\n",
    "        \"\"\"\n",
    "        self.dynamics = CartpoleDynamics()\n",
    "\n",
    "        self.timestep_limit = timestep_limit\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the environment to the initial state.\n",
    "\n",
    "        Returns:\n",
    "        - np.ndarray: The initial state of the environment.\n",
    "        \"\"\"\n",
    "\n",
    "        self.q_sim = np.zeros(4)\n",
    "        self.timesteps = 0\n",
    "\n",
    "        self.traj = [self.get_observation()]\n",
    "\n",
    "        return self.traj[-1]\n",
    "\n",
    "    def get_observation(self):\n",
    "        \"\"\"\n",
    "        Retrieves the current state of the environment.\n",
    "\n",
    "        Returns:\n",
    "        - np.ndarray: The current state of the simulation.\n",
    "        \"\"\"\n",
    "\n",
    "        return self.q_sim\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Executes one time step within the environment using the given action.\n",
    "\n",
    "        Parameters:\n",
    "        - action (np.ndarray): The action to apply for this timestep.\n",
    "\n",
    "        Returns:\n",
    "        - Tuple[np.ndarray, float, bool, dict]: A tuple containing the new state, the reward received,\n",
    "          a boolean indicating whether the episode is done, and an info dictionary.\n",
    "        \"\"\"\n",
    "        action = np.clip(action, self.action_space.low, self.action_space.high)[0]\n",
    "\n",
    "        new_q = self.dynamics.step(\n",
    "            self.q_sim, action\n",
    "        )\n",
    "\n",
    "        if not isinstance(action, torch.Tensor):\n",
    "            action = torch.tensor(action)\n",
    "\n",
    "        reward = self.dynamics.reward(\n",
    "            new_q, action\n",
    "        ).numpy()\n",
    "\n",
    "        self.q_sim = new_q.numpy()\n",
    "        done = self.is_done()\n",
    "\n",
    "        self.timesteps += 1\n",
    "\n",
    "        self.traj.append(self.q_sim)\n",
    "\n",
    "        return self.q_sim, reward, done, {}\n",
    "\n",
    "    def is_done(self):\n",
    "        \"\"\"\n",
    "        Checks if the episode has finished based on the timestep limit.\n",
    "\n",
    "        Returns:\n",
    "        - bool: True if the episode is finished, False otherwise.\n",
    "        \"\"\"\n",
    "        # Kill trial when too much time has passed\n",
    "        if self.timesteps >= self.timestep_limit:\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def plot_func(self, to_plot, i=None):\n",
    "        \"\"\"\n",
    "        Plots the current state of the cartpole system for visualization.\n",
    "\n",
    "        Parameters:\n",
    "        - to_plot (matplotlib.axes.Axes or dict): Axes for plotting or a dictionary of plot elements to update.\n",
    "        - i (int, optional): The index of the current state in the trajectory to plot.\n",
    "        \"\"\"\n",
    "        def _square(center_x, center_y, shape, angle):\n",
    "            trans_points = np.array([\n",
    "                [shape[0], shape[1]],\n",
    "                [-shape[0], shape[1]],\n",
    "                [-shape[0], -shape[1]],\n",
    "                [shape[0], -shape[1]],\n",
    "                [shape[0], shape[1]]\n",
    "            ]) @ np.array([\n",
    "                [np.cos(angle), np.sin(angle)],\n",
    "                [-np.sin(angle), np.cos(angle)]\n",
    "            ]) + np.array([center_x, center_y])\n",
    "\n",
    "            return trans_points[:, 0], trans_points[:, 1]\n",
    "\n",
    "        if isinstance(to_plot, Axes):\n",
    "            imgs = dict(\n",
    "                cart=to_plot.plot([], [], c=\"k\")[0],\n",
    "                pole=to_plot.plot([], [], c=\"k\", linewidth=5)[0],\n",
    "                center=to_plot.plot([], [], marker=\"o\", c=\"k\",\n",
    "                                          markersize=10)[0]\n",
    "            )\n",
    "\n",
    "            x_width = max(1,max(np.abs(t[0]) for t in self.traj) * 1.3)\n",
    "\n",
    "            # centerline\n",
    "            to_plot.plot(np.linspace(-x_width, x_width, num=50), np.zeros(50),\n",
    "                         c=\"k\", linestyle=\"dashed\")\n",
    "\n",
    "            # set axis\n",
    "            to_plot.set_xlim([-x_width, x_width])\n",
    "            to_plot.set_ylim([-self.dynamics.l*1.2, self.dynamics.l*1.2])\n",
    "\n",
    "            return imgs\n",
    "\n",
    "        curr_x = self.traj[i]\n",
    "\n",
    "        cart_size = (0.15, 0.1)\n",
    "\n",
    "        cart_x, cart_y = _square(curr_x[0], 0.,\n",
    "                                cart_size, 0.)\n",
    "\n",
    "        pole_x = np.array([curr_x[0], curr_x[0] + self.dynamics.l\n",
    "                           * np.cos(curr_x[1]-np.pi/2)])\n",
    "        pole_y = np.array([0., self.dynamics.l\n",
    "                           * np.sin(curr_x[1]-np.pi/2)])\n",
    "\n",
    "        to_plot[\"cart\"].set_data(cart_x, cart_y)\n",
    "        to_plot[\"pole\"].set_data(pole_x, pole_y)\n",
    "        to_plot[\"center\"].set_data(self.traj[i][0], 0.)\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        \"\"\"\n",
    "        Renders the current state of the environment using a matplotlib animation.\n",
    "\n",
    "        This function creates a matplotlib figure and uses the plot_func method to update the figure with the current\n",
    "        state of the cartpole system at each timestep. The animation is created with the FuncAnimation class and is\n",
    "        configured to play at a specified frame rate.\n",
    "\n",
    "        Parameters:\n",
    "        - mode (str): The mode for rendering. Currently, only \"human\" mode is supported, which displays the animation\n",
    "          on screen.\n",
    "\n",
    "        Returns:\n",
    "        - matplotlib.animation.FuncAnimation: The animation object that can be displayed in a Jupyter notebook or\n",
    "          saved to file.\n",
    "        \"\"\"\n",
    "        self.anim_fig = plt.figure()\n",
    "\n",
    "        self.axis = self.anim_fig.add_subplot(111)\n",
    "        self.axis.set_aspect('equal', adjustable='box')\n",
    "\n",
    "        imgs = self.plot_func(self.axis)\n",
    "        _update_img = lambda i: self.plot_func(imgs, i)\n",
    "\n",
    "        Writer = animation.writers['ffmpeg']\n",
    "        writer = Writer(fps=15, metadata=dict(artist='Me'), bitrate=1800)\n",
    "\n",
    "        ani = FuncAnimation(\n",
    "            self.anim_fig, _update_img, interval=self.dynamics.dt*1000,\n",
    "            frames=len(self.traj)-1\n",
    "        )\n",
    "\n",
    "        plt.close()\n",
    "\n",
    "        return ani\n",
    "\n",
    "    @property\n",
    "    def action_space(self):\n",
    "        \"\"\"\n",
    "        Defines the action space of the environment using a Box space from OpenAI Gym.\n",
    "\n",
    "        The action space is defined based on the lower and upper bounds for the control input specified in the\n",
    "        dynamics model. This allows for a continuous range of actions that can be applied to the cartpole system.\n",
    "\n",
    "        Returns:\n",
    "        - gym.spaces.Box: The action space as a Box object, with low and high bounds derived from the dynamics model's\n",
    "          control input bounds.\n",
    "        \"\"\"\n",
    "        return gym.spaces.Box(low=self.dynamics.u_lb.numpy(), high=self.dynamics.u_ub.numpy())\n",
    "\n",
    "    @property\n",
    "    def observation_space(self):\n",
    "        \"\"\"\n",
    "        Defines the observation space of the environment using a Box space from OpenAI Gym.\n",
    "\n",
    "        The observation space is defined with no bounds on the values, representing the position and velocity of the\n",
    "        cart and the angle and angular velocity of the pole. This space allows for any real-valued vector of\n",
    "        positions and velocities to be a valid observation in the environment.\n",
    "\n",
    "        Returns:\n",
    "        - gym.spaces.Box: The observation space as a Box object, with low and high bounds set to negative and\n",
    "          positive infinity, respectively, for each dimension of the state vector.\n",
    "        \"\"\"\n",
    "        return gym.spaces.Box(\n",
    "            low= np.array([-np.inf, -np.inf, -np.inf, -np.inf]),\n",
    "            high=np.array([np.inf,   np.inf,  np.inf,  np.inf])\n",
    "        )\n",
    "\n",
    "env_name = 'CartpoleSwingUp-v0'\n",
    "if env_name in registry.env_specs:\n",
    "    del registry.env_specs[env_name]\n",
    "register(\n",
    "    id=env_name,\n",
    "    entry_point=f'{__name__}:CartpoleGym',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameNotFound",
     "evalue": "Environment CartpoleSwingUp doesn't exist. Did you mean: `CartPole`?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameNotFound\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCartpoleSwingUp-v0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m q \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m      4\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl/lib/python3.11/site-packages/gym/envs/registration.py:569\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    563\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    564\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the latest versioned environment `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_env_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    565\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead of the unversioned environment `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mid\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    566\u001b[0m         )\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spec_ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 569\u001b[0m         \u001b[43m_check_version_exists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    570\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo registered env with id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mid\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    572\u001b[0m _kwargs \u001b[38;5;241m=\u001b[39m spec_\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m~/miniconda3/envs/rl/lib/python3.11/site-packages/gym/envs/registration.py:219\u001b[0m, in \u001b[0;36m_check_version_exists\u001b[0;34m(ns, name, version)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m get_env_id(ns, name, version) \u001b[38;5;129;01min\u001b[39;00m registry:\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 219\u001b[0m \u001b[43m_check_name_exists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl/lib/python3.11/site-packages/gym/envs/registration.py:197\u001b[0m, in \u001b[0;36m_check_name_exists\u001b[0;34m(ns, name)\u001b[0m\n\u001b[1;32m    194\u001b[0m namespace_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in namespace \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mns\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ns \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    195\u001b[0m suggestion_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDid you mean: `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuggestion[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`?\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m suggestion \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 197\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mNameNotFound(\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnvironment \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt exist\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnamespace_msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuggestion_msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    199\u001b[0m )\n",
      "\u001b[0;31mNameNotFound\u001b[0m: Environment CartpoleSwingUp doesn't exist. Did you mean: `CartPole`?"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartpoleSwingUp-v0')\n",
    "\n",
    "q = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    q, r, done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "env.render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
