{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "import torch.nn.functional as F\n",
    "import gym_minigrid\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from tqdm.notebook import tqdm\n",
    "from gym_minigrid.envs.doorkey import DoorKeyEnv\n",
    "from gym.envs.registration import registry, register\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from typing_extensions import Self\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# rc('animation', html='jshtml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "# elif torch.backends.mps.is_available():\n",
    "#     DEVICE = torch.device(\"mps\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "print(DEVICE)\n",
    "\n",
    "class Config:\n",
    "    \"\"\"\n",
    "    Stores algorithmic hyperparameters.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                score_threshold=0.93,\n",
    "                discount=0.995,\n",
    "                lr=1e-3,\n",
    "                max_grad_norm=0.5,\n",
    "                log_interval=10,\n",
    "                max_episodes=2000,\n",
    "                gae_lambda=0.95,\n",
    "                use_critic=True,\n",
    "                clip_ratio=0.2,\n",
    "                target_kl=0.01,\n",
    "                fit_badly_coeff=0.05,\n",
    "                train_ac_iters=5,\n",
    "                use_discounted_reward=False,\n",
    "                entropy_coef=0.01,\n",
    "                use_gae=True,\n",
    "                my_var=0):\n",
    "\n",
    "        self.score_threshold = score_threshold # criterion for early stopping. If the rolling average reward (over the last 100 episodes) is greater than it, it ends.\n",
    "        self.discount = discount # discount factor\n",
    "        self.lr = lr # learning rate\n",
    "        self.max_grad_norm = max_grad_norm # the maximum gradient norm (https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html)\n",
    "        self.log_interval = log_interval # logging interval\n",
    "        self.max_episodes = max_episodes # the maximum number of episodes.\n",
    "        self.use_critic = use_critic # whether to use critic or not.\n",
    "        self.clip_ratio = clip_ratio # clip_ratio of PPO.\n",
    "        self.target_kl = target_kl # target KL divergence for early stoping train_ac_iters for PPO\n",
    "        self.fit_badly_coeff = fit_badly_coeff # the bound of approx. KL of when the model fits badly to the data.\n",
    "        self.train_ac_iters = train_ac_iters # how many time to train ac_model using current computed old_logps\n",
    "        self.gae_lambda=gae_lambda # lambda in Generalized Advantage Estimation (GAE)\n",
    "        self.use_discounted_reward=use_discounted_reward # whether use discounted reward or not.\n",
    "        self.entropy_coef = entropy_coef # entropy coefficient for PPO\n",
    "        self.use_gae = use_gae # whether to use GAE or not.\n",
    "        self.my_var = my_var # a variable for testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function from https://github.com/ikostrikov/pytorch-a2c-ppo-acktr/blob/master/model.py\n",
    "def init_params(m):\n",
    "    \"\"\"\n",
    "    Initialize parameters of the network.\n",
    "    m: torch.nn.Module\n",
    "    \"\"\"\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Linear\") != -1:\n",
    "        m.weight.data.normal_(0, 1)\n",
    "        m.weight.data *= 1 / torch.sqrt(m.weight.data.pow(2).sum(1, keepdim=True))\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0)\n",
    "\n",
    "def preprocess_obss(obss, device=None):\n",
    "    \"\"\"\n",
    "    Convert observation into Torch.Tensor\n",
    "\n",
    "    Parameters\n",
    "    ----\n",
    "    obss: dictionary or np.ndarray\n",
    "    device: target device of torch.Tensor ('cpu', 'cuda')\n",
    "\n",
    "    Return\n",
    "    ----\n",
    "    Torch Tensor\n",
    "    \"\"\"\n",
    "    if isinstance(obss, dict):\n",
    "        images = np.array([obss[\"image\"]])\n",
    "    else:\n",
    "        images = np.array([o[\"image\"] for o in obss])\n",
    "\n",
    "    return torch.tensor(images, device=device, dtype=torch.float)\n",
    "\n",
    "class DoorKeyEnv5x5(DoorKeyEnv):\n",
    "    def __init__(self):\n",
    "        self.render_mode = \"rgb_array\"\n",
    "        super().__init__(size=5)\n",
    "\n",
    "    def _reward(self):\n",
    "        \"\"\"\n",
    "        Compute the reward to be given upon success\n",
    "        \"\"\"\n",
    "        return 1\n",
    "    \n",
    "class DoorKeyEnv6x6(DoorKeyEnv):\n",
    "    def __init__(self):\n",
    "        self.render_mode = \"rgb_array\"\n",
    "        super().__init__(size=6)\n",
    "\n",
    "    def _reward(self):\n",
    "        \"\"\"\n",
    "        Compute the reward to be given upon success\n",
    "        \"\"\"\n",
    "        return 1\n",
    "    \n",
    "class DoorKeyEnv8x8(DoorKeyEnv):\n",
    "    def __init__(self):\n",
    "        self.render_mode = \"rgb_array\"\n",
    "        super().__init__(size=8)\n",
    "\n",
    "    def _reward(self):\n",
    "        \"\"\"\n",
    "        Compute the reward to be given upon success\n",
    "        \"\"\"\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(7)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fdeb21ea520>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = DoorKeyEnv6x6() # define environment.\n",
    "env.reset()\n",
    "action = env.action_space.sample()\n",
    "obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "# get an RGB image corresponding to the whole environment or the agent's point of view (https://github.com/Farama-Foundation/Minigrid/blob/master/minigrid/minigrid_env.py#L716)\n",
    "#            highlight (bool): If true, the agent's field of view or point of view is highlighted with a lighter gray color.\n",
    "#            tile_size (int): How many pixels will form a tile from the NxM grid.\n",
    "#            agent_pov (bool): If true, the rendered frame will only contain the point of view of the agent.\n",
    "frame = env.get_frame(highlight=env.highlight, tile_size=env.tile_size, agent_pov=env.agent_pov)\n",
    "# show an image to the notebook.\n",
    "plt.imshow(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACModel(nn.Module):\n",
    "    def __init__(self, num_actions, use_critic=True):\n",
    "        \"\"\"\n",
    "        Represents an Actor Crictic model that takes a 2d, multi-channeled\n",
    "        image as input.\n",
    "\n",
    "        Parameters\n",
    "        ----\n",
    "        num_actions : int\n",
    "\n",
    "                      The action space of the environment.\n",
    "                      The action space for DoorKey5x5 is 7-dimensional:\n",
    "                      0: turn left,\n",
    "                      1: turn right,\n",
    "                      2: forward,\n",
    "                      3: pickup an object,\n",
    "                      4: drop an object,\n",
    "                      5: activate an object,\n",
    "                      6: done completing task\n",
    "\n",
    "        use_critics : bool\n",
    "\n",
    "                      Critic network will be used in forward pass if flag is set\n",
    "                      to true.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.use_critic = use_critic\n",
    "\n",
    "        # Define actor's model\n",
    "        self.image_conv_actor = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, (2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 2)),\n",
    "            nn.Conv2d(16, 32, (2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, (2, 2)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, num_actions)\n",
    "        )\n",
    "\n",
    "        # Define critic's model\n",
    "        if self.use_critic:\n",
    "            self.image_conv_critic = nn.Sequential(\n",
    "                nn.Conv2d(3, 16, (2, 2)),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d((2, 2)),\n",
    "                nn.Conv2d(16, 32, (2, 2)),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(32, 64, (2, 2)),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "            self.critic = nn.Sequential(\n",
    "                nn.Linear(64, 64),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(64, 1)\n",
    "            )\n",
    "\n",
    "        # Initialize parameters correctly\n",
    "        self.apply(init_params)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        \"\"\"\n",
    "        Performs a forward pass through the actor-critic network\n",
    "\n",
    "        Parameters\n",
    "        ----\n",
    "        obs : int tensor. Shape [Batch size, ImWidth, ImHeight, Channels]\n",
    "\n",
    "              input to the network.\n",
    "        ----\n",
    "\n",
    "        returns:\n",
    "\n",
    "        dist : torch.distribution\n",
    "            The distribution of actions from policy. A Categorical distribution\n",
    "            for discreet action spaces.\n",
    "        value : torch.Tensor (Batch size, 1)\n",
    "            value output by critic network\n",
    "        \"\"\"\n",
    "        conv_in = obs.transpose(1, 3).transpose(2, 3) # reshape into [b, c, h, w]\n",
    "\n",
    "        dist, value = None, None\n",
    "\n",
    "        x = self.image_conv_actor(conv_in)\n",
    "        embedding = x.reshape(x.shape[0], -1)\n",
    "\n",
    "        x = self.actor(embedding)\n",
    "        dist = Categorical(logits=F.log_softmax(x, dim=1))\n",
    "\n",
    "        if self.use_critic:\n",
    "            y = self.image_conv_critic(conv_in)\n",
    "            embedding = y.reshape(y.shape[0], -1)\n",
    "\n",
    "            value = self.critic(embedding).squeeze(1)\n",
    "        else:\n",
    "            value = torch.zeros((x.shape[0], 1), device=x.device)\n",
    "        \n",
    "        return dist, value\n",
    "\n",
    "def compute_discounted_return(rewards, discount, device=DEVICE):\n",
    "    \"\"\"\n",
    "\t\trewards: reward obtained at timestep.  Shape: (T,)\n",
    "\t\tdiscount: discount factor. float\n",
    "\n",
    "    ----\n",
    "    returns: sum of discounted rewards. Shape: (T,)\n",
    "\t\t\"\"\"\n",
    "    returns = torch.zeros(*rewards.shape, device=device)\n",
    "\n",
    "    R = 0\n",
    "    for t in reversed(range((rewards.shape[0]))):\n",
    "        R = rewards[t] + discount * R\n",
    "        returns[t] = R\n",
    "    return returns\n",
    "\n",
    "def compute_advantage_gae(values, rewards, T, gae_lambda, discount):\n",
    "    \"\"\"\n",
    "    Compute Adavantage with GAE. See Section 4.4.2 in the lecture notes.\n",
    "\n",
    "    values: value at each timestep (T,)\n",
    "    rewards: reward obtained at each timestep.  Shape: (T,)\n",
    "    T: the number of frames, float\n",
    "    gae_lambda: hyperparameter, float\n",
    "    discount: discount factor, float\n",
    "\n",
    "    -----\n",
    "\n",
    "    returns:\n",
    "\n",
    "    advantages : tensor.float. Shape [T,]\n",
    "\n",
    "                 gae advantage term for timesteps 0 to T\n",
    "\n",
    "    \"\"\"\n",
    "    advantages = torch.zeros_like(values)\n",
    "    for i in reversed(range(T)):\n",
    "        next_value = values[i+1]\n",
    "        next_advantage = advantages[i+1]\n",
    "\n",
    "        delta = rewards[i] + discount * next_value  - values[i]\n",
    "        advantages[i] = delta + discount * gae_lambda * next_advantage\n",
    "    return advantages[:T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Categorical(logits: torch.Size([1, 7])),\n",
       " tensor([0.1054], grad_fn=<SqueezeBackward1>))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TESTS\n",
    "\n",
    "model = ACModel(num_actions=7, use_critic=True).to(DEVICE)\n",
    "obss = env.reset()\n",
    "obs = preprocess_obss(obss[0], DEVICE)\n",
    "model(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Machine:\n",
    "    def __init__(self, num_actions=7, args=Config()):\n",
    "        \"\"\"\n",
    "        A Machine object consists of a ACModel, its hyperparameters, and its history of rewards.\n",
    "\n",
    "        Args:\n",
    "            num_actions: size of action space\n",
    "            args: Model hyperparameters\n",
    "        \"\"\"\n",
    "        self.args = args\n",
    "        self.model = ACModel(num_actions, use_critic=True).to(DEVICE)\n",
    "        self.optim = torch.optim.Adam(self.model.parameters(), lr=self.args.lr)\n",
    "        self.rewards = []\n",
    "\n",
    "    def reset_rs(self) -> None:\n",
    "        self.rewards = []\n",
    "\n",
    "    def copy_machine(self, other: Self) -> None:\n",
    "        \"\"\"\n",
    "        Copy state dict from 'other'. Reset rs.\n",
    "        \"\"\"\n",
    "        state_dict = other.model.state_dict()\n",
    "        for key, v in state_dict.items():\n",
    "            self.model.state_dict()[key].copy_(v)\n",
    "        \n",
    "        self.reset_rs()\n",
    "\n",
    "    def add_r(self, r: float) -> None:\n",
    "        self.rewards.append(r)\n",
    "\n",
    "    def avg_r(self) -> float:\n",
    "        assert len(self.rewards) > 0, 'No rewards yet'\n",
    "        return sum(self.rewards) / len(self.rewards)\n",
    "    \n",
    "    def collect_experiences(self, env:gym.Env):\n",
    "        \"\"\"\n",
    "        Collects rollouts and computes advantages.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        exps : dict\n",
    "            Contains actions, rewards, advantages etc as attributes.\n",
    "            Each attribute, e.g. `exps['reward']` has a shape\n",
    "            (self.num_frames, ...).\n",
    "        logs : dict\n",
    "            Useful stats about the training process, including the average\n",
    "            reward, policy loss, value loss, etc.\n",
    "        \"\"\"\n",
    "        acmodel = self.model\n",
    "\n",
    "        MAX_FRAMES_PER_EP = 300\n",
    "        shape = (MAX_FRAMES_PER_EP, )\n",
    "\n",
    "        actions = torch.zeros(*shape, device=DEVICE, dtype=torch.int)\n",
    "        values = torch.zeros(*shape, device=DEVICE)\n",
    "        rewards = torch.zeros(*shape, device=DEVICE)\n",
    "        log_probs = torch.zeros(*shape, device=DEVICE)\n",
    "        obss = [None]*MAX_FRAMES_PER_EP\n",
    "\n",
    "        obs, _ = env.reset()\n",
    "\n",
    "        total_return = 0\n",
    "\n",
    "        T = 0\n",
    "\n",
    "        while True:\n",
    "            # Do one agent-environment interaction\n",
    "\n",
    "            preprocessed_obs = preprocess_obss(obs, device=DEVICE)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                dist, value = acmodel(preprocessed_obs)\n",
    "\n",
    "            dist: Categorical\n",
    "            action = dist.sample()[0]\n",
    "\n",
    "            obss[T] = obs\n",
    "            obs, reward, done, _, _ = env.step(action.item())\n",
    "\n",
    "\n",
    "            # Update experiences values\n",
    "            actions[T] = action\n",
    "            values[T] = value\n",
    "            rewards[T] = reward\n",
    "            log_probs[T] = dist.log_prob(action)\n",
    "\n",
    "\n",
    "            total_return += reward\n",
    "            T += 1\n",
    "\n",
    "            if done or T>=MAX_FRAMES_PER_EP-1:\n",
    "                break\n",
    "\n",
    "        discounted_reward = compute_discounted_return(rewards[:T], self.args.discount, DEVICE)\n",
    "        exps = dict(\n",
    "            obs = preprocess_obss([\n",
    "                obss[i]\n",
    "                for i in range(T)\n",
    "            ], device=DEVICE),\n",
    "            action = actions[:T],\n",
    "            value  = values[:T],\n",
    "            reward = rewards[:T],\n",
    "            log_prob = log_probs[:T],\n",
    "            discounted_reward = discounted_reward,\n",
    "            T = T\n",
    "        )\n",
    "\n",
    "        logs = {\n",
    "            \"return_per_episode\": total_return,\n",
    "            \"num_frames\": T\n",
    "        }\n",
    "\n",
    "        return exps, logs\n",
    "\n",
    "    def _compute_policy_loss_ppo(self, obs, init_logp, old_logp, actions, advantages):\n",
    "        \"\"\"\n",
    "        Computes the policy loss for PPO.\n",
    "\n",
    "        obs: observeration to pass into acmodel. shape: (T,)\n",
    "        init_logp: log probabilities we get from the agent performing the action. shape: (T,)\n",
    "        old_logp: log probabilities from previous timestep. shape: (T,)\n",
    "        actions: action at this timestep. shape: (T,ImWidth,ImHeight,Channels)\n",
    "        advantages: the computed advantages. shape: (T,)\n",
    "\n",
    "        ---\n",
    "        Returns:\n",
    "\n",
    "        policy_loss: ppo policy loss as shown in line 6 of PPO alg. tensor.float. Shape (,1)\n",
    "        fit_badly: an approximation of kl divergence between init_logp and old_logp. tensor.float. Shape (,1)\n",
    "        approx_kl: an approximation of kl divergence between old_logp and new_logp. tensor.float. Shape (,1)\n",
    "        \"\"\"\n",
    "        policy_loss, approx_kl = 0, 0\n",
    "        dist, _ = self.model(obs)\n",
    "        dist: Categorical\n",
    "\n",
    "        entropy = dist.entropy()\n",
    "        logps = dist.log_prob(actions)\n",
    "        r_terms = torch.exp(logps - old_logp)\n",
    "        ppo_loss = torch.min(r_terms * advantages, torch.clamp(r_terms, 1 - self.args.clip_ratio, 1 + self.args.clip_ratio) * advantages)\n",
    "        \n",
    "        # Importance sampling factor\n",
    "        factors = torch.exp(old_logp - init_logp)\n",
    "        \n",
    "        indices = factors < 100\n",
    "\n",
    "        policy_loss_tensor = factors * ppo_loss + self.args.entropy_coef * entropy\n",
    "        policy_loss = - torch.mean(policy_loss_tensor[indices])\n",
    "\n",
    "        # approx_kl = torch.sum(torch.exp(old_logp) * (old_logp - logps)) / torch.sum(torch.exp(old_logp))\n",
    "        approx_kl = torch.mean((old_logp - logps) ** 2) / 2\n",
    "        fit_badly = torch.mean((old_logp - init_logp) ** 2)\n",
    "\n",
    "        return policy_loss, fit_badly, approx_kl\n",
    "    \n",
    "    def _compute_value_loss(self, obs, returns):\n",
    "        _, value = self.model(obs)\n",
    "\n",
    "        value_loss = torch.mean((value - returns) ** 2)\n",
    "\n",
    "        return value_loss\n",
    "\n",
    "\n",
    "    def update_parameters(self, sb):\n",
    "        \n",
    "        MAX_FRAMES_PER_EP = 300\n",
    "        T = sb['T']\n",
    "        dist, values = self.model(sb['obs'])\n",
    "        values = values.reshape(-1)\n",
    "        dist: Categorical\n",
    "        old_logp = dist.log_prob(sb['action']).detach()\n",
    "        init_logp = sb['log_prob']\n",
    "\n",
    "        # add 0 to end of values until it has length MAX_FRAMES_PER_EP\n",
    "        values_extended = torch.cat([values, torch.zeros((MAX_FRAMES_PER_EP - len(values), ), device=DEVICE)], dim=0)\n",
    "        full_reward = torch.cat([sb['reward'], torch.zeros((MAX_FRAMES_PER_EP - len(sb['reward']), ), device=DEVICE)], dim=0)\n",
    "\n",
    "        if self.args.use_gae:\n",
    "            advantage = compute_advantage_gae(values_extended, full_reward, T, self.args.gae_lambda, self.args.discount)\n",
    "        else:\n",
    "            advantage = sb['discounted_reward'] - values.reshape(-1)\n",
    "\n",
    "        policy_loss, _ = self._compute_policy_loss_ppo(sb['obs'], init_logp, old_logp, sb['action'], advantage)\n",
    "        value_loss = self._compute_value_loss(sb['obs'], sb['discounted_reward'])\n",
    "\n",
    "        for i in range(self.args.train_ac_iters):\n",
    "            self.optim.zero_grad()\n",
    "            loss_pi, approx_kl = self._compute_policy_loss_ppo(sb['obs'], init_logp, old_logp, sb['action'], advantage)\n",
    "            if sb['label'] == self.label:\n",
    "                loss_v = self._compute_value_loss(sb['obs'], sb['discounted_reward'])\n",
    "            else:\n",
    "                loss_v = 0.0\n",
    "\n",
    "            loss = loss_v + loss_pi\n",
    "            if approx_kl > 1.5 * self.args.target_kl:\n",
    "                break\n",
    "\n",
    "            loss.backward(retain_graph=True)\n",
    "            self.optim.step()\n",
    "\n",
    "        update_policy_loss = policy_loss.item()\n",
    "        update_value_loss = value_loss.item()\n",
    "\n",
    "        logs = {\n",
    "            \"policy_loss\": update_policy_loss,\n",
    "            \"value_loss\": update_value_loss,\n",
    "        }\n",
    "\n",
    "        return logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MachineCluster:\n",
    "    def __init__(self, hps:list[str], num_machines:int=3, num_survivors:int=1, num_trajs:int=16):\n",
    "        \"\"\"\n",
    "        The cluster of machines.\n",
    "\n",
    "        hps: A list of hyperparameters to optimize. Must be a subset of keys in Machine.args.\n",
    "        num_machines: The number of machines in the cluster.\n",
    "        num_survivors: The number of survivors to select from the cluster.\n",
    "        num_trajs: The number of trajectories collected by each machine in each training epoch.\n",
    "        batch_size: The number of trajectories used in each gradient step.\n",
    "        \"\"\"\n",
    "        self.num_machines = num_machines\n",
    "        self.num_survivors = num_survivors\n",
    "        self.hps = hps\n",
    "        self.num_trajs = num_trajs\n",
    "        self.machines = []\n",
    "        for _ in range(self.num_machines):\n",
    "            self.machines.append(Machine())\n",
    "\n",
    "        # now let's say hps = ['entropy_coef']\n",
    "        for hp in hps:\n",
    "            # sample num_models values from a uniform distribution\n",
    "            values = np.random.uniform(0, 1, size=num_machines)\n",
    "            # TODO: Should it be other distributions?\n",
    "    \n",
    "            # set the hyperparameters of the models\n",
    "            self.update_hp(hp, values)\n",
    "\n",
    "    def update_hp(self, hp:str, targets:list[float]) -> None:\n",
    "        \"\"\"\n",
    "        Update the hyperparameters of the models in the cluster.\n",
    "\n",
    "        Args:\n",
    "            hp: The hyperparameter to update.\n",
    "            targets: The target hyperparameter values.\n",
    "                    Has length self.num_models.\n",
    "        \"\"\"\n",
    "        for i, target in enumerate(targets):\n",
    "            setattr(self.machines[i].args, hp, target)\n",
    "\n",
    "    def generate_data(self):\n",
    "        \"\"\"\n",
    "        Generate batches of data for training the models.\n",
    "\n",
    "        Return:\n",
    "            trajs: list of exps.  \n",
    "        \"\"\"\n",
    "\n",
    "        trajs = []\n",
    "\n",
    "        for i in range(self.num_machines):\n",
    "            for _ in range(self.num_trajs):\n",
    "                exps, _ = self.machines[i].collect_experiences(env)\n",
    "                trajs.append(exps)\n",
    "            \n",
    "            print(f'Generated data for machine {i}.')\n",
    "        \n",
    "        return trajs\n",
    "    \n",
    "    def train_one_step(self, data, batch_size):\n",
    "        \"\"\"\n",
    "        Do at most self.args.train_ac_iters step of optimization on each model in the cluster.\n",
    "\n",
    "        Args:\n",
    "            data: list of exps.\n",
    "            batch_size: number of trajectories used in each gradient step.\n",
    "\n",
    "        Return:\n",
    "            logs\n",
    "        \"\"\"\n",
    "        # TODO: actually do this\n",
    "\n",
    "        MAX_FRAMES_PER_EP = 300\n",
    "\n",
    "        logs = [[]] * self.num_machines\n",
    "        fit_badly_counter = False\n",
    "        \n",
    "        for machine_idx in range(self.num_machines):\n",
    "\n",
    "            machine = self.machines[machine_idx]\n",
    "\n",
    "            total_policy_loss = 0\n",
    "            total_value_loss = 0\n",
    "            fit_badly_count = 0\n",
    "\n",
    "            # select random subset of data\n",
    "            idx = np.random.choice(len(data), batch_size, replace=False)\n",
    "\n",
    "            for i in idx:\n",
    "                sb = data[i]\n",
    "                T = sb['T']\n",
    "                obs = sb['obs']\n",
    "                dist, values = machine.model(obs)\n",
    "                init_logp = sb['log_prob']\n",
    "                old_logp = dist.log_prob(sb['action']).detach()\n",
    "\n",
    "                # add 0 to end of values until it has length MAX_FRAMES_PER_EP\n",
    "                values_extended = torch.cat([values, torch.zeros((MAX_FRAMES_PER_EP - len(values), ), device=DEVICE)], dim=0)\n",
    "                full_reward = torch.cat([sb['reward'], torch.zeros((MAX_FRAMES_PER_EP - len(sb['reward']), ), device=DEVICE)], dim=0)\n",
    "                assert len(values_extended) == MAX_FRAMES_PER_EP\n",
    "                assert len(full_reward) == MAX_FRAMES_PER_EP\n",
    "                \n",
    "                # compute advantage\n",
    "                if machine.args.use_gae:\n",
    "                    advantage = compute_advantage_gae(values_extended, full_reward, T, machine.args.gae_lambda, machine.args.discount)\n",
    "                else:\n",
    "                    advantage = sb['discounted_reward'] - values.reshape(-1)\n",
    "                \n",
    "                # compute policy and value loss\n",
    "                policy_loss, fit_badly, approx_kl = machine._compute_policy_loss_ppo(obs, init_logp, old_logp, sb['action'], advantage)\n",
    "\n",
    "                value_loss = machine._compute_value_loss(obs, sb['discounted_reward'])\n",
    "                \n",
    "                if fit_badly < machine.args.fit_badly_coeff:\n",
    "                    total_policy_loss += policy_loss\n",
    "                    total_value_loss += value_loss\n",
    "                else:\n",
    "                    fit_badly_count += 1\n",
    "            \n",
    "            print(fit_badly_count / (batch_size * self.num_machines))\n",
    "            \n",
    "            machine.optim.zero_grad()\n",
    "            loss = total_policy_loss + total_value_loss\n",
    "            loss.backward()\n",
    "            machine.optim.step()\n",
    "            \n",
    "            logs[machine_idx].append({\n",
    "            \"policy_loss\": total_policy_loss,\n",
    "            \"value_loss\": total_value_loss\n",
    "            })\n",
    "\n",
    "            if fit_badly_count / (batch_size * self.num_machines) > 0.5:\n",
    "                fit_badly_counter = True\n",
    "        \n",
    "        return logs, fit_badly_counter\n",
    "\n",
    "    def train(self, data, batch_size, max_steps=50):\n",
    "        \"\"\"\n",
    "        Train each model in the cluster until data fits poorly, or until reaches max_steps.\n",
    "        \"\"\" \n",
    "\n",
    "        logs = []\n",
    "        steps = 0\n",
    "        fit_badly_counter = False\n",
    "    \n",
    "        while not fit_badly_counter:\n",
    "            if steps >= max_steps:\n",
    "                break\n",
    "            new_logs, fit_badly_counter = self.train_one_step(data, batch_size)\n",
    "            logs = [[*i, *j] for i, j in zip(logs, new_logs)]\n",
    "            steps += 1\n",
    "        \n",
    "        return logs\n",
    "            \n",
    "        \n",
    "    def evaluate(self, trained_models):\n",
    "        \"\"\"\n",
    "        Evaluate the performance of the models in the cluster.\n",
    "\n",
    "        Args:\n",
    "        - trained_models: list of ExtendedModels.\n",
    "\n",
    "        Returns:\n",
    "        - performances:\n",
    "        \"\"\"\n",
    "\n",
    "        pass\n",
    "\n",
    "    def eliminate(self, trained_models, performances):\n",
    "        \"\"\"\n",
    "        The elimination process to select the survivors.\n",
    "        Also shift hyperparams.\n",
    "        \"\"\"\n",
    "\n",
    "        pass\n",
    "    \n",
    "    def reproduce(self):\n",
    "        \"\"\"\n",
    "        Reproduce models.\n",
    "        \"\"\"\n",
    "\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cluster_experiment(hps:list[str], \n",
    "                           num_machines:int=3, \n",
    "                           num_survivors:int=1, \n",
    "                           num_trajs:int=16, \n",
    "                           num_epochs:int=10, \n",
    "                           seed=42):\n",
    "    \"\"\"\n",
    "    Run the cluster experiment.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    cluster = MachineCluster(hps = hps, num_machines = num_machines, num_survivors = num_survivors, num_trajs = num_trajs)\n",
    "\n",
    "    pbar = tqdm(range(num_epochs))\n",
    "    logs = dict(\n",
    "        rewards = [],\n",
    "        smooth_rewards = []\n",
    "    )\n",
    "    pdlogs = []\n",
    "\n",
    "    for epoch in pbar:\n",
    "        # Generate data\n",
    "        obs = cluster.generate_data()\n",
    "\n",
    "        # Train until data fits poorly\n",
    "        logs = cluster.train(data=obs, batch_size=8)\n",
    "        trained_machines = cluster.machines\n",
    "\n",
    "        # # Evaluate performances\n",
    "        # performances = cluster.evaluate(trained_models)\n",
    "        # logs.append(performances) # what is in logs?\n",
    "\n",
    "        # # Survival of the fittest\n",
    "        # survivors = cluster.eliminate(trained_models, performances)\n",
    "        # cluster.reproduce(survivors)\n",
    "\n",
    "        print(f'Epoch {epoch} done!')\n",
    "\n",
    "    # return pd.DataFrame(pdlogs).set_index('epoch')\n",
    "    return logs, trained_machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated data for machine 0.\n",
      "Generated data for machine 1.\n",
      "Generated data for machine 2.\n",
      "0.125\n",
      "0.20833333333333334\n",
      "0.08333333333333333\n",
      "0.08333333333333333\n",
      "0.041666666666666664\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.041666666666666664\n",
      "0.16666666666666666\n",
      "0.08333333333333333\n",
      "0.041666666666666664\n",
      "0.125\n",
      "0.16666666666666666\n",
      "0.08333333333333333\n",
      "0.25\n",
      "0.16666666666666666\n",
      "0.125\n",
      "0.25\n",
      "0.16666666666666666\n",
      "0.041666666666666664\n",
      "0.20833333333333334\n",
      "0.20833333333333334\n",
      "0.08333333333333333\n",
      "0.2916666666666667\n",
      "0.16666666666666666\n",
      "0.08333333333333333\n",
      "0.16666666666666666\n",
      "0.16666666666666666\n",
      "0.041666666666666664\n",
      "0.125\n",
      "0.25\n",
      "0.08333333333333333\n",
      "0.20833333333333334\n",
      "0.08333333333333333\n",
      "0.08333333333333333\n",
      "0.20833333333333334\n",
      "0.08333333333333333\n",
      "0.041666666666666664\n",
      "0.125\n",
      "0.125\n",
      "0.08333333333333333\n",
      "0.08333333333333333\n",
      "0.041666666666666664\n",
      "0.125\n",
      "0.041666666666666664\n",
      "0.16666666666666666\n",
      "0.041666666666666664\n",
      "0.041666666666666664\n",
      "0.041666666666666664\n",
      "0.08333333333333333\n",
      "0.0\n",
      "0.08333333333333333\n",
      "0.041666666666666664\n",
      "0.0\n",
      "0.041666666666666664\n",
      "0.08333333333333333\n",
      "0.041666666666666664\n",
      "0.08333333333333333\n",
      "0.08333333333333333\n",
      "0.0\n",
      "0.08333333333333333\n",
      "0.041666666666666664\n",
      "0.041666666666666664\n",
      "0.08333333333333333\n",
      "0.08333333333333333\n",
      "0.041666666666666664\n",
      "0.041666666666666664\n",
      "0.08333333333333333\n",
      "0.0\n",
      "0.0\n",
      "0.08333333333333333\n",
      "0.041666666666666664\n",
      "0.0\n",
      "0.08333333333333333\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.08333333333333333\n",
      "0.041666666666666664\n",
      "0.08333333333333333\n",
      "0.041666666666666664\n",
      "0.08333333333333333\n",
      "0.125\n",
      "0.041666666666666664\n",
      "0.041666666666666664\n",
      "0.0\n",
      "0.08333333333333333\n",
      "0.08333333333333333\n",
      "0.0\n",
      "0.08333333333333333\n",
      "0.041666666666666664\n",
      "0.0\n",
      "0.041666666666666664\n",
      "0.041666666666666664\n",
      "0.041666666666666664\n",
      "0.08333333333333333\n",
      "0.041666666666666664\n",
      "0.041666666666666664\n",
      "0.041666666666666664\n",
      "0.041666666666666664\n",
      "0.041666666666666664\n",
      "0.125\n",
      "0.0\n",
      "0.041666666666666664\n",
      "0.041666666666666664\n",
      "0.041666666666666664\n",
      "0.041666666666666664\n",
      "0.16666666666666666\n",
      "0.041666666666666664\n",
      "0.08333333333333333\n",
      "0.041666666666666664\n",
      "0.0\n",
      "0.125\n",
      "0.041666666666666664\n",
      "0.041666666666666664\n",
      "0.08333333333333333\n",
      "0.0\n",
      "0.041666666666666664\n",
      "0.16666666666666666\n",
      "0.041666666666666664\n",
      "0.08333333333333333\n",
      "0.125\n",
      "0.08333333333333333\n",
      "0.041666666666666664\n",
      "0.08333333333333333\n",
      "0.041666666666666664\n",
      "0.041666666666666664\n",
      "0.041666666666666664\n",
      "0.0\n",
      "0.125\n",
      "0.041666666666666664\n",
      "0.041666666666666664\n",
      "0.16666666666666666\n",
      "0.08333333333333333\n",
      "0.0\n",
      "0.0\n",
      "0.08333333333333333\n",
      "0.08333333333333333\n",
      "0.041666666666666664\n",
      "0.041666666666666664\n",
      "0.0\n",
      "0.125\n",
      "0.041666666666666664\n",
      "0.0\n",
      "0.041666666666666664\n",
      "0.041666666666666664\n",
      "0.08333333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [01:58<17:49, 118.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 done!\n",
      "Generated data for machine 0.\n",
      "Generated data for machine 1.\n",
      "Generated data for machine 2.\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [04:19<17:34, 131.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 done!\n",
      "Generated data for machine 0.\n",
      "Generated data for machine 1.\n",
      "Generated data for machine 2.\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [07:05<17:12, 147.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 done!\n",
      "Generated data for machine 0.\n",
      "Generated data for machine 1.\n",
      "Generated data for machine 2.\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [09:47<15:17, 152.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 done!\n",
      "Generated data for machine 0.\n",
      "Generated data for machine 1.\n",
      "Generated data for machine 2.\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [12:45<13:30, 162.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 done!\n",
      "Generated data for machine 0.\n",
      "Generated data for machine 1.\n",
      "Generated data for machine 2.\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [15:18<10:35, 158.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 done!\n",
      "Generated data for machine 0.\n",
      "Generated data for machine 1.\n",
      "Generated data for machine 2.\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [17:28<07:28, 149.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 done!\n",
      "Generated data for machine 0.\n",
      "Generated data for machine 1.\n",
      "Generated data for machine 2.\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [19:50<04:54, 147.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 done!\n",
      "Generated data for machine 0.\n",
      "Generated data for machine 1.\n",
      "Generated data for machine 2.\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.041666666666666664\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [22:12<02:25, 145.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 done!\n",
      "Generated data for machine 0.\n",
      "Generated data for machine 1.\n",
      "Generated data for machine 2.\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [24:20<00:00, 146.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df, trained_machines = run_cluster_experiment(hps=['entropy_coef'])\n",
    "\n",
    "# df.plot(x='num_frames', y=['reward', 'smooth_reward'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(coefs:list[float], max_steps:int, env=DoorKeyEnv6x6(), args=Config(), seed=0):\n",
    "    \"\"\"\n",
    "    Upper level function for running experiments to analyze reinforce and\n",
    "    policy gradient methods. Instantiates a model, collects epxeriences, and\n",
    "    then updates the neccessary parameters.\n",
    "\n",
    "    Args:\n",
    "        coefs: a list of coefs each machine has\n",
    "        max_steps: maximum number of steps to run the experiment for.\n",
    "        env: the environment to use. \n",
    "        args: Config object with hyperparameters.\n",
    "        seed: random seed. int\n",
    "\n",
    "    Returns:\n",
    "        DataFrame indexed by episode\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    device = DEVICE\n",
    "\n",
    "    machines = [Machine(coef, label=idx, args=args) for idx, coef in enumerate(coefs)]\n",
    "    winners = []\n",
    "    full_rs = []\n",
    "\n",
    "    is_solved = False\n",
    "\n",
    "    SMOOTH_REWARD_WINDOW = 50\n",
    "\n",
    "    pd_logs, rewards = [], [0]*SMOOTH_REWARD_WINDOW\n",
    "\n",
    "    num_frames = 0\n",
    "\n",
    "    pbar = tqdm(range(max_steps))\n",
    "    for update in pbar:\n",
    "\n",
    "        label = update % len(machines)\n",
    "\n",
    "        machine = machines[label]\n",
    "        \n",
    "        exps, logs1 = machine.collect_experiences(env)\n",
    "\n",
    "        for m in machines:\n",
    "            if m is machine:\n",
    "                logs2 = m.update_parameters(exps)\n",
    "            else:\n",
    "                m.update_parameters(exps)\n",
    "\n",
    "        logs = {**logs1, **logs2}\n",
    "\n",
    "        num_frames += logs[\"num_frames\"]\n",
    "\n",
    "        rewards.append(logs[\"return_per_episode\"])\n",
    "\n",
    "        smooth_reward = np.mean(rewards[-SMOOTH_REWARD_WINDOW:])\n",
    "\n",
    "        data = {'episode':update, 'num_frames':num_frames, 'smooth_reward':smooth_reward,\n",
    "               'reward':logs[\"return_per_episode\"], 'policy_loss':logs[\"policy_loss\"]}\n",
    "\n",
    "        if args.use_critic:\n",
    "            data['value_loss'] = logs[\"value_loss\"]\n",
    "\n",
    "        pd_logs.append(data)\n",
    "\n",
    "        pbar.set_postfix(data)\n",
    "\n",
    "        machine.add_r(rewards[-1])\n",
    "        if update % (20 * len(machines)) == 0 and update > 0:\n",
    "            full_rs.append([m.avg_r() for m in machines])\n",
    "            winner = survival_fittest(machines)\n",
    "            winners.append(winner)\n",
    "\n",
    "        # # Early terminate\n",
    "        # if smooth_reward >= args.score_threshold:\n",
    "        #     is_solved = True\n",
    "        #     break\n",
    "\n",
    "    if is_solved:\n",
    "        print('Solved!')\n",
    "\n",
    "    for m in machines:\n",
    "        print(m.avg_r())\n",
    "    print(winners)\n",
    "    print(full_rs)\n",
    "\n",
    "    return pd.DataFrame(pd_logs).set_index('episode')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'registry' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 372\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m gym\u001b[38;5;241m.\u001b[39mspaces\u001b[38;5;241m.\u001b[39mBox(\n\u001b[1;32m    367\u001b[0m             low\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf, \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf, \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf, \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf]),\n\u001b[1;32m    368\u001b[0m             high\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray([np\u001b[38;5;241m.\u001b[39minf,   np\u001b[38;5;241m.\u001b[39minf,  np\u001b[38;5;241m.\u001b[39minf,  np\u001b[38;5;241m.\u001b[39minf])\n\u001b[1;32m    369\u001b[0m         )\n\u001b[1;32m    371\u001b[0m env_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCartpoleSwingUp-v0\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m env_name \u001b[38;5;129;01min\u001b[39;00m \u001b[43mregistry\u001b[49m\u001b[38;5;241m.\u001b[39menv_specs:\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m registry\u001b[38;5;241m.\u001b[39menv_specs[env_name]\n\u001b[1;32m    374\u001b[0m register(\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39menv_name,\n\u001b[1;32m    376\u001b[0m     entry_point\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:CartpoleGym\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    377\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'registry' is not defined"
     ]
    }
   ],
   "source": [
    "class CartpoleDynamics:\n",
    "    def __init__(self,\n",
    "                 timestep=0.02,\n",
    "                 m_p=0.5,\n",
    "                 m_c=0.5,\n",
    "                 l=0.6,\n",
    "                 g=-9.81,\n",
    "                 u_range=15):\n",
    "        \"\"\"\n",
    "        Initializes the Cartpole Dynamics model with given parameters.\n",
    "\n",
    "        Parameters:\n",
    "        - timestep (float): The time step for the simulation.\n",
    "        - m_p (float): Mass of the pole.\n",
    "        - m_c (float): Mass of the cart.\n",
    "        - l (float): Length of the pole.\n",
    "        - g (float): Acceleration due to gravity. Negative values indicate direction.\n",
    "        - u_range (float): Range of the control input.\n",
    "        \"\"\"\n",
    "\n",
    "        self.m_p  = m_p\n",
    "        self.m_c  = m_c\n",
    "        self.l    = l\n",
    "        self.g    = -g\n",
    "        self.dt   = timestep\n",
    "\n",
    "        self.u_range = u_range\n",
    "\n",
    "        self.u_lb = torch.tensor([-1]).float()\n",
    "        self.u_ub = torch.tensor([1]).float()\n",
    "        self.q_shape = 4\n",
    "        self.u_shape = 1\n",
    "\n",
    "    def _qdotdot(self, q, u):\n",
    "        \"\"\"\n",
    "        Calculates the acceleration of both cart and pole as a function of the current state and control input.\n",
    "\n",
    "        Parameters:\n",
    "        - q (torch.Tensor): The current state of the system, [x, theta, xdot, thetadot].\n",
    "        - u (torch.Tensor): The current control input.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: Accelerations [x_dotdot, theta_dotdot] of the cart and the pole.\n",
    "        \"\"\"\n",
    "        x, theta, xdot, thetadot = q.T\n",
    "\n",
    "        if len(u.shape) == 2:\n",
    "            u = torch.flatten(u)\n",
    "\n",
    "        x_dotdot = (\n",
    "            u + self.m_p * torch.sin(theta) * (\n",
    "                self.l * torch.pow(thetadot,2) + self.g * torch.cos(theta)\n",
    "            )\n",
    "        ) / (self.m_c + self.m_p * torch.sin(theta)**2)\n",
    "\n",
    "        theta_dotdot = (\n",
    "            -u*torch.cos(theta) -\n",
    "            self.m_p * self.l * torch.pow(thetadot,2) * torch.cos(theta) * torch.sin(theta) -\n",
    "            (self.m_c + self.m_p) * self.g * torch.sin(theta)\n",
    "        ) / (self.l * (self.m_c + self.m_p * torch.sin(theta)**2))\n",
    "\n",
    "        return torch.stack((x_dotdot, theta_dotdot), dim=-1)\n",
    "\n",
    "    def _euler_int(self, q, qdotdot):\n",
    "        \"\"\"\n",
    "        Performs Euler integration to calculate the new state given the current state and accelerations.\n",
    "\n",
    "        Parameters:\n",
    "        - q (torch.Tensor): The current state of the system, [x, theta, xdot, thetadot].\n",
    "        - qdotdot (torch.Tensor): The accelerations [x_dotdot, theta_dotdot] of the cart and the pole.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: The new state of the system after a single time step.\n",
    "        \"\"\"\n",
    "\n",
    "        qdot_new = q[...,2:] + qdotdot * self.dt\n",
    "        q_new = q[...,:2] + self.dt * qdot_new\n",
    "\n",
    "        return torch.cat((q_new, qdot_new), dim=-1)\n",
    "\n",
    "    def step(self, q, u):\n",
    "        \"\"\"\n",
    "        Performs a single step of simulation given the current state and control input.\n",
    "\n",
    "        Parameters:\n",
    "        - q (torch.Tensor or np.ndarray): The current state of the system.\n",
    "        - u (torch.Tensor or np.ndarray): The current control input.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: The new state of the system after the step.\n",
    "        \"\"\"\n",
    "\n",
    "        # Check for numpy array\n",
    "        if isinstance(q, np.ndarray):\n",
    "            q = torch.from_numpy(q)\n",
    "        if isinstance(u, np.ndarray):\n",
    "            u = torch.from_numpy(u)\n",
    "\n",
    "        scaled_u = u * float(self.u_range)\n",
    "\n",
    "        # Check for shape issues\n",
    "        if len(q.shape) == 2:\n",
    "            q_dotdot = self._qdotdot(q, scaled_u)\n",
    "        elif len(q.shape) == 1:\n",
    "            q_dotdot = self._qdotdot(q.reshape(1,-1), scaled_u)\n",
    "        else:\n",
    "            raise RuntimeError('Invalid q shape')\n",
    "\n",
    "        new_q = self._euler_int(q, q_dotdot)\n",
    "\n",
    "        if len(q.shape) == 1:\n",
    "            new_q = new_q[0]\n",
    "\n",
    "        return new_q\n",
    "\n",
    "    # given q [bs, q_shape] and u [bs, t, u_shape] run the trajectories\n",
    "    def run_batch_of_trajectories(self, q, u):\n",
    "        \"\"\"\n",
    "        Simulates a batch of trajectories given initial states and control inputs over time.\n",
    "\n",
    "        Parameters:\n",
    "        - q (torch.Tensor): Initial states for each trajectory in the batch.\n",
    "        - u (torch.Tensor): Control inputs for each trajectory over time.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: The states of the system at each time step for each trajectory.\n",
    "        \"\"\"\n",
    "        qs = [q]\n",
    "\n",
    "        for t in range(u.shape[1]):\n",
    "            qs.append(self.step(qs[-1], u[:,t]))\n",
    "\n",
    "        return torch.stack(qs, dim=1)\n",
    "\n",
    "    # given q [bs, t, q_shape] and u [bs, t, u_shape] calculate the rewards\n",
    "    def reward(self, q, u):\n",
    "        \"\"\"\n",
    "        Calculates the reward for given states and control inputs.\n",
    "\n",
    "        Parameters:\n",
    "        - q (torch.Tensor or np.ndarray): States of the system.\n",
    "        - u (torch.Tensor or np.ndarray): Control inputs applied.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: The calculated rewards for the states and inputs.\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(q, np.ndarray):\n",
    "            q = torch.from_numpy(q)\n",
    "        if isinstance(u, np.ndarray):\n",
    "            u = torch.from_numpy(u)\n",
    "\n",
    "        angle_term = 0.5*(1-torch.cos(q[...,1]))\n",
    "        pos_term = -0.5*torch.pow(q[...,0],2)\n",
    "        ctrl_cost = -0.001*(u**2).sum(dim=-1)\n",
    "\n",
    "        return angle_term + pos_term + ctrl_cost\n",
    "\n",
    "class CartpoleGym(gym.Env):\n",
    "    def __init__(self, timestep_limit=200):\n",
    "        \"\"\"\n",
    "        Initializes the Cartpole environment with a specified time step limit.\n",
    "\n",
    "        Parameters:\n",
    "        - timestep_limit (int): The maximum number of timesteps for each episode.\n",
    "\n",
    "        Sets up the dynamics model and initializes the simulation state.\n",
    "        \"\"\"\n",
    "        self.dynamics = CartpoleDynamics()\n",
    "\n",
    "        self.timestep_limit = timestep_limit\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the environment to the initial state.\n",
    "\n",
    "        Returns:\n",
    "        - np.ndarray: The initial state of the environment.\n",
    "        \"\"\"\n",
    "\n",
    "        self.q_sim = np.zeros(4)\n",
    "        self.timesteps = 0\n",
    "\n",
    "        self.traj = [self.get_observation()]\n",
    "\n",
    "        return self.traj[-1]\n",
    "\n",
    "    def get_observation(self):\n",
    "        \"\"\"\n",
    "        Retrieves the current state of the environment.\n",
    "\n",
    "        Returns:\n",
    "        - np.ndarray: The current state of the simulation.\n",
    "        \"\"\"\n",
    "\n",
    "        return self.q_sim\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Executes one time step within the environment using the given action.\n",
    "\n",
    "        Parameters:\n",
    "        - action (np.ndarray): The action to apply for this timestep.\n",
    "\n",
    "        Returns:\n",
    "        - Tuple[np.ndarray, float, bool, dict]: A tuple containing the new state, the reward received,\n",
    "          a boolean indicating whether the episode is done, and an info dictionary.\n",
    "        \"\"\"\n",
    "        action = np.clip(action, self.action_space.low, self.action_space.high)[0]\n",
    "\n",
    "        new_q = self.dynamics.step(\n",
    "            self.q_sim, action\n",
    "        )\n",
    "\n",
    "        if not isinstance(action, torch.Tensor):\n",
    "            action = torch.tensor(action)\n",
    "\n",
    "        reward = self.dynamics.reward(\n",
    "            new_q, action\n",
    "        ).numpy()\n",
    "\n",
    "        self.q_sim = new_q.numpy()\n",
    "        done = self.is_done()\n",
    "\n",
    "        self.timesteps += 1\n",
    "\n",
    "        self.traj.append(self.q_sim)\n",
    "\n",
    "        return self.q_sim, reward, done, {}\n",
    "\n",
    "    def is_done(self):\n",
    "        \"\"\"\n",
    "        Checks if the episode has finished based on the timestep limit.\n",
    "\n",
    "        Returns:\n",
    "        - bool: True if the episode is finished, False otherwise.\n",
    "        \"\"\"\n",
    "        # Kill trial when too much time has passed\n",
    "        if self.timesteps >= self.timestep_limit:\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def plot_func(self, to_plot, i=None):\n",
    "        \"\"\"\n",
    "        Plots the current state of the cartpole system for visualization.\n",
    "\n",
    "        Parameters:\n",
    "        - to_plot (matplotlib.axes.Axes or dict): Axes for plotting or a dictionary of plot elements to update.\n",
    "        - i (int, optional): The index of the current state in the trajectory to plot.\n",
    "        \"\"\"\n",
    "        def _square(center_x, center_y, shape, angle):\n",
    "            trans_points = np.array([\n",
    "                [shape[0], shape[1]],\n",
    "                [-shape[0], shape[1]],\n",
    "                [-shape[0], -shape[1]],\n",
    "                [shape[0], -shape[1]],\n",
    "                [shape[0], shape[1]]\n",
    "            ]) @ np.array([\n",
    "                [np.cos(angle), np.sin(angle)],\n",
    "                [-np.sin(angle), np.cos(angle)]\n",
    "            ]) + np.array([center_x, center_y])\n",
    "\n",
    "            return trans_points[:, 0], trans_points[:, 1]\n",
    "\n",
    "        if isinstance(to_plot, Axes):\n",
    "            imgs = dict(\n",
    "                cart=to_plot.plot([], [], c=\"k\")[0],\n",
    "                pole=to_plot.plot([], [], c=\"k\", linewidth=5)[0],\n",
    "                center=to_plot.plot([], [], marker=\"o\", c=\"k\",\n",
    "                                          markersize=10)[0]\n",
    "            )\n",
    "\n",
    "            x_width = max(1,max(np.abs(t[0]) for t in self.traj) * 1.3)\n",
    "\n",
    "            # centerline\n",
    "            to_plot.plot(np.linspace(-x_width, x_width, num=50), np.zeros(50),\n",
    "                         c=\"k\", linestyle=\"dashed\")\n",
    "\n",
    "            # set axis\n",
    "            to_plot.set_xlim([-x_width, x_width])\n",
    "            to_plot.set_ylim([-self.dynamics.l*1.2, self.dynamics.l*1.2])\n",
    "\n",
    "            return imgs\n",
    "\n",
    "        curr_x = self.traj[i]\n",
    "\n",
    "        cart_size = (0.15, 0.1)\n",
    "\n",
    "        cart_x, cart_y = _square(curr_x[0], 0.,\n",
    "                                cart_size, 0.)\n",
    "\n",
    "        pole_x = np.array([curr_x[0], curr_x[0] + self.dynamics.l\n",
    "                           * np.cos(curr_x[1]-np.pi/2)])\n",
    "        pole_y = np.array([0., self.dynamics.l\n",
    "                           * np.sin(curr_x[1]-np.pi/2)])\n",
    "\n",
    "        to_plot[\"cart\"].set_data(cart_x, cart_y)\n",
    "        to_plot[\"pole\"].set_data(pole_x, pole_y)\n",
    "        to_plot[\"center\"].set_data(self.traj[i][0], 0.)\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        \"\"\"\n",
    "        Renders the current state of the environment using a matplotlib animation.\n",
    "\n",
    "        This function creates a matplotlib figure and uses the plot_func method to update the figure with the current\n",
    "        state of the cartpole system at each timestep. The animation is created with the FuncAnimation class and is\n",
    "        configured to play at a specified frame rate.\n",
    "\n",
    "        Parameters:\n",
    "        - mode (str): The mode for rendering. Currently, only \"human\" mode is supported, which displays the animation\n",
    "          on screen.\n",
    "\n",
    "        Returns:\n",
    "        - matplotlib.animation.FuncAnimation: The animation object that can be displayed in a Jupyter notebook or\n",
    "          saved to file.\n",
    "        \"\"\"\n",
    "        self.anim_fig = plt.figure()\n",
    "\n",
    "        self.axis = self.anim_fig.add_subplot(111)\n",
    "        self.axis.set_aspect('equal', adjustable='box')\n",
    "\n",
    "        imgs = self.plot_func(self.axis)\n",
    "        _update_img = lambda i: self.plot_func(imgs, i)\n",
    "\n",
    "        Writer = animation.writers['ffmpeg']\n",
    "        writer = Writer(fps=15, metadata=dict(artist='Me'), bitrate=1800)\n",
    "\n",
    "        ani = FuncAnimation(\n",
    "            self.anim_fig, _update_img, interval=self.dynamics.dt*1000,\n",
    "            frames=len(self.traj)-1\n",
    "        )\n",
    "\n",
    "        plt.close()\n",
    "\n",
    "        return ani\n",
    "\n",
    "    @property\n",
    "    def action_space(self):\n",
    "        \"\"\"\n",
    "        Defines the action space of the environment using a Box space from OpenAI Gym.\n",
    "\n",
    "        The action space is defined based on the lower and upper bounds for the control input specified in the\n",
    "        dynamics model. This allows for a continuous range of actions that can be applied to the cartpole system.\n",
    "\n",
    "        Returns:\n",
    "        - gym.spaces.Box: The action space as a Box object, with low and high bounds derived from the dynamics model's\n",
    "          control input bounds.\n",
    "        \"\"\"\n",
    "        return gym.spaces.Box(low=self.dynamics.u_lb.numpy(), high=self.dynamics.u_ub.numpy())\n",
    "\n",
    "    @property\n",
    "    def observation_space(self):\n",
    "        \"\"\"\n",
    "        Defines the observation space of the environment using a Box space from OpenAI Gym.\n",
    "\n",
    "        The observation space is defined with no bounds on the values, representing the position and velocity of the\n",
    "        cart and the angle and angular velocity of the pole. This space allows for any real-valued vector of\n",
    "        positions and velocities to be a valid observation in the environment.\n",
    "\n",
    "        Returns:\n",
    "        - gym.spaces.Box: The observation space as a Box object, with low and high bounds set to negative and\n",
    "          positive infinity, respectively, for each dimension of the state vector.\n",
    "        \"\"\"\n",
    "        return gym.spaces.Box(\n",
    "            low= np.array([-np.inf, -np.inf, -np.inf, -np.inf]),\n",
    "            high=np.array([np.inf,   np.inf,  np.inf,  np.inf])\n",
    "        )\n",
    "\n",
    "env_name = 'CartpoleSwingUp-v0'\n",
    "if env_name in registry.env_specs:\n",
    "    del registry.env_specs[env_name]\n",
    "register(\n",
    "    id=env_name,\n",
    "    entry_point=f'{__name__}:CartpoleGym',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameNotFound",
     "evalue": "Environment CartpoleSwingUp doesn't exist. Did you mean: `CartPole`?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameNotFound\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCartpoleSwingUp-v0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m q \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m      4\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl/lib/python3.11/site-packages/gym/envs/registration.py:569\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    563\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    564\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the latest versioned environment `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_env_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    565\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead of the unversioned environment `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mid\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    566\u001b[0m         )\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spec_ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 569\u001b[0m         \u001b[43m_check_version_exists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    570\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo registered env with id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mid\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    572\u001b[0m _kwargs \u001b[38;5;241m=\u001b[39m spec_\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m~/miniconda3/envs/rl/lib/python3.11/site-packages/gym/envs/registration.py:219\u001b[0m, in \u001b[0;36m_check_version_exists\u001b[0;34m(ns, name, version)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m get_env_id(ns, name, version) \u001b[38;5;129;01min\u001b[39;00m registry:\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 219\u001b[0m \u001b[43m_check_name_exists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl/lib/python3.11/site-packages/gym/envs/registration.py:197\u001b[0m, in \u001b[0;36m_check_name_exists\u001b[0;34m(ns, name)\u001b[0m\n\u001b[1;32m    194\u001b[0m namespace_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in namespace \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mns\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ns \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    195\u001b[0m suggestion_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDid you mean: `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuggestion[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`?\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m suggestion \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 197\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mNameNotFound(\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnvironment \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt exist\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnamespace_msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuggestion_msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    199\u001b[0m )\n",
      "\u001b[0;31mNameNotFound\u001b[0m: Environment CartpoleSwingUp doesn't exist. Did you mean: `CartPole`?"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartpoleSwingUp-v0')\n",
    "\n",
    "q = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    q, r, done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "env.render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
