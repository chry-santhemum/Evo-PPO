{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.9.7)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/atticusw/opt/anaconda3/lib/python3.9/site-packages/gym/envs/registration.py:307: DeprecationWarning: The package name gym_minigrid has been deprecated in favor of minigrid. Please uninstall gym_minigrid and install minigrid with `pip install minigrid`. Future releases will be maintained under the new package name minigrid.\n",
      "  fn()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "import torch.nn.functional as F\n",
    "import minigrid\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from tqdm.notebook import tqdm\n",
    "from minigrid.envs.doorkey import DoorKeyEnv\n",
    "from minigrid.envs.crossing import CrossingEnv\n",
    "from minigrid.core.world_object import Goal, Lava\n",
    "import pandas as pd\n",
    "from gym.envs.registration import registry, register\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from typing_extensions import Self\n",
    "import matplotlib.pyplot as plt\n",
    "from minigrid.wrappers import ObservationWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device() -> torch.device:\n",
    "    \"\"\"\n",
    "    Returns the device to use for training.\n",
    "    \"\"\"\n",
    "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def set_random_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function from https://github.com/ikostrikov/pytorch-a2c-ppo-acktr/blob/master/model.py\n",
    "def init_params(m):\n",
    "    \"\"\"\n",
    "    Initialize parameters of the network.\n",
    "    m: torch.nn.Module\n",
    "    \"\"\"\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Linear\") != -1:\n",
    "        m.weight.data.normal_(0, 1)\n",
    "        m.weight.data *= 1 / torch.sqrt(m.weight.data.pow(2).sum(1, keepdim=True))\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0)\n",
    "\n",
    "class MyDoorKeyEnv(DoorKeyEnv):\n",
    "    def __init__(self, size):\n",
    "        self.render_mode = \"rgb_array\"\n",
    "        super().__init__(size=size)\n",
    "\n",
    "    def _reward(self):\n",
    "        \"\"\"\n",
    "        Compute the reward to be given upon success\n",
    "        \"\"\"\n",
    "        return 1\n",
    "\n",
    "class ImgObsWrapper(ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Use the image as the only observation output, no language/mission.\n",
    "\n",
    "    Parameters:\n",
    "    - env (gym.Env): The environment to wrap.\n",
    "\n",
    "    Methods:\n",
    "    - observation(self, obs): Returns the image from the observation.\n",
    "    - reset(self): Resets the environment and returns the initial observation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        \"\"\"\n",
    "        Initializes the ImgObsWrapper with the given environment.\n",
    "\n",
    "        Parameters:\n",
    "        - env (gym.Env): The environment whose observations are to be wrapped.\n",
    "        \"\"\"\n",
    "        super().__init__(env)\n",
    "        self.observation_space = env.observation_space.spaces[\"image\"]\n",
    "\n",
    "    def observation(self, obs):\n",
    "        \"\"\"\n",
    "        Extracts and returns the image data from the observation.\n",
    "\n",
    "        Parameters:\n",
    "        - obs (dict or tuple): The original observation from the environment, which could be either\n",
    "        a dictionary or a tuple containing a dictionary.\n",
    "\n",
    "        Returns:\n",
    "        - np.ndarray: The image data extracted from the observation.\n",
    "        \"\"\"\n",
    "        if type(obs) == tuple:\n",
    "            return obs[0][\"image\"]\n",
    "        return obs[\"image\"]\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the environment and returns the initial observation image.\n",
    "\n",
    "        Returns:\n",
    "        - np.ndarray: The initial observation image of the reset environment.\n",
    "        \"\"\"\n",
    "        obs = super().reset()\n",
    "        return obs[0]\n",
    "    \n",
    "def get_door_key_env(size):\n",
    "    \"\"\"\n",
    "    Returns a DoorKeyEnv environment with the given size.\n",
    "    \"\"\"\n",
    "    env = MyDoorKeyEnv(size=size)\n",
    "    env = ImgObsWrapper(env)\n",
    "\n",
    "    env.reset()\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    # get an RGB image corresponding to the whole environment or the agent's point of view (https://github.com/Farama-Foundation/Minigrid/blob/master/minigrid/minigrid_env.py#L716)\n",
    "    #            highlight (bool): If true, the agent's field of view or point of view is highlighted with a lighter gray color.\n",
    "    #            tile_size (int): How many pixels will form a tile from the NxM grid.\n",
    "    #            agent_pov (bool): If true, the rendered frame will only contain the point of view of the agent.\n",
    "    frame = env.get_frame(highlight=env.highlight, tile_size=env.tile_size, agent_pov=env.agent_pov)\n",
    "    # show an image to the notebook.\n",
    "    plt.imshow(frame)\n",
    "\n",
    "    return env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"\n",
    "    Stores algorithmic hyperparameters.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                score_threshold=0.93,\n",
    "                discount=0.995,\n",
    "                lr=1e-3,\n",
    "                max_grad_norm=0.5,\n",
    "                log_interval=10,\n",
    "                gae_lambda=0.95,\n",
    "                clip_ratio=0.2,\n",
    "                target_kl=0.01,\n",
    "                train_ac_iters=5,\n",
    "                use_discounted_reward=True,\n",
    "                use_gae=True,\n",
    "                importance_sampling_clip = 2.0,\n",
    "                bad_fit_threshold = 0.8,\n",
    "                bad_fit_increment = 0.02,\n",
    "                replay_buffer_capacity = 10,\n",
    "                large_buffer_capacity = 20):\n",
    "\n",
    "        self.score_threshold = score_threshold # criterion for early stopping. If the rolling average reward (over the last 100 episodes) is greater than it, it ends.\n",
    "        self.discount = discount # discount factor\n",
    "        self.lr = lr # learning rate\n",
    "        self.max_grad_norm = max_grad_norm # the maximum gradient norm (https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html)\n",
    "        self.log_interval = log_interval # logging interval\n",
    "        self.clip_ratio = clip_ratio # clip_ratio of PPO.\n",
    "        self.target_kl = target_kl # target KL divergence for early stoping train_ac_iters for PPO\n",
    "        self.train_ac_iters = train_ac_iters # how many time to train ac_model using current computed old_logps\n",
    "        self.gae_lambda=gae_lambda # lambda in Generalized Advantage Estimation (GAE)\n",
    "        self.use_discounted_reward=use_discounted_reward # whether use discounted reward or not.\n",
    "        self.use_gae = use_gae # whether to use GAE or not.\n",
    "        self.importance_sampling_clip = importance_sampling_clip # importance sampling clip threshold\n",
    "        self.bad_fit_threshold = bad_fit_threshold # threshold for bad fit.\n",
    "        self.bad_fit_increment = bad_fit_increment # increment for each sb in replay buffer.\n",
    "        self.replay_buffer_capacity = replay_buffer_capacity # capacity of replay buffer.\n",
    "        self.large_buffer_capacity = large_buffer_capacity # capacity of large replay buffer.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Machine:\n",
    "    def __init__(self, entropy_coef, init_model:nn.Module, args:Config=None):\n",
    "        \"\"\"\n",
    "        A Machine object consists of a Model and its entropy_coef\n",
    "\n",
    "        Args:\n",
    "            entropy_coef: Entropy coefficient.\n",
    "            init_model: Initial model.\n",
    "            args\n",
    "        \"\"\"\n",
    "        if args is None:\n",
    "            self.args = Config()\n",
    "        else:\n",
    "            self.args = args\n",
    "\n",
    "        self.model = init_model\n",
    "        self.optim = torch.optim.Adam(self.model.parameters(), lr=self.args.lr)\n",
    "        self.coef = entropy_coef\n",
    "        self.device = get_device()\n",
    "\n",
    "    def copy_model(self, other_model:nn.Module) -> None:\n",
    "        \"\"\"\n",
    "        Copy state dict from 'model'. Reset rs.\n",
    "        \"\"\"\n",
    "        state_dict = other_model.state_dict()\n",
    "        for key, v in state_dict.items():\n",
    "            if key in self.model.state_dict():\n",
    "                self.model.state_dict()[key].copy_(v)\n",
    "\n",
    "    def copy_machine(self, other:Self) -> None:\n",
    "        \"\"\"\n",
    "        Copy state dict from 'other'. Reset rs.\n",
    "        \"\"\"\n",
    "        self.copy_model(other.model)\n",
    "\n",
    "    def _compute_discounted_return(self, rewards):\n",
    "        \"\"\"\n",
    "            rewards: reward obtained at timestep.  Shape: (T,)\n",
    "            discount: discount factor. float\n",
    "\n",
    "        ----\n",
    "        returns: sum of discounted rewards. Shape: (T,)\n",
    "        \"\"\"\n",
    "        returns = torch.zeros(*rewards.shape, device=self.device)\n",
    "\n",
    "        R = 0\n",
    "        for t in reversed(range((rewards.shape[0]))):\n",
    "            R = rewards[t] + self.args.discount * R\n",
    "            returns[t] = R\n",
    "        return returns\n",
    "\n",
    "    def _compute_advantage_gae(self, values, rewards, T):\n",
    "        \"\"\"\n",
    "        Compute Adavantage wiht GAE. See Section 4.4.2 in the lecture notes.\n",
    "\n",
    "        values: value at each timestep (T,)\n",
    "        rewards: reward obtained at each timestep.  Shape: (T,)\n",
    "        T: the number of frames, float\n",
    "        gae_lambda: hyperparameter, float\n",
    "        discount: discount factor, float\n",
    "\n",
    "        -----\n",
    "\n",
    "        returns:\n",
    "\n",
    "        advantages : tensor.float. Shape [T,]\n",
    "\n",
    "                    gae advantage term for timesteps 0 to T\n",
    "\n",
    "        \"\"\"\n",
    "        advantages = torch.zeros_like(values)\n",
    "        for i in reversed(range(T)):\n",
    "            next_value = values[i+1]\n",
    "            next_advantage = advantages[i+1]\n",
    "\n",
    "            delta = rewards[i] + self.args.discount * next_value  - values[i]\n",
    "            advantages[i] = delta + self.args.discount * self.args.gae_lambda * next_advantage\n",
    "        return advantages[:T]\n",
    "    \n",
    "    def collect_experiences(self, env:gym.Env):\n",
    "        \"\"\"\n",
    "        Collects rollouts and computes advantages.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        exps : dict\n",
    "            Contains actions, rewards, advantages etc as attributes.\n",
    "            Each attribute, e.g. `exps['reward']` has a shape\n",
    "            (self.num_frames, ...).\n",
    "        logs : dict\n",
    "            Useful stats about the training process, including the average\n",
    "            reward, policy loss, value loss, etc.\n",
    "        \"\"\"\n",
    "        device = get_device()\n",
    "\n",
    "        MAX_FRAMES_PER_EP = 300\n",
    "        shape = (MAX_FRAMES_PER_EP, )\n",
    "\n",
    "        actions = torch.zeros(*shape, device=device, dtype=torch.int)\n",
    "        values = torch.zeros(*shape, device=device)\n",
    "        rewards = torch.zeros(*shape, device=device)\n",
    "        log_probs = torch.zeros(*shape, device=device)\n",
    "        obss = [None]*MAX_FRAMES_PER_EP\n",
    "\n",
    "        obs = env.reset()\n",
    "\n",
    "        total_return = 0\n",
    "\n",
    "        T = 0\n",
    "\n",
    "        while True:\n",
    "            # Do one agent-environment interaction\n",
    "            with torch.no_grad():\n",
    "                dist, value = self.model(obs)\n",
    "\n",
    "            dist: Categorical\n",
    "            action = dist.sample()[0]\n",
    "\n",
    "            obss[T] = obs\n",
    "            obs, reward, done, _, _ = env.step(action.item())\n",
    "\n",
    "            # Update experiences values\n",
    "            actions[T] = action\n",
    "            values[T] = value\n",
    "            rewards[T] = reward\n",
    "            log_probs[T] = dist.log_prob(action)\n",
    "\n",
    "            total_return += reward\n",
    "            T += 1\n",
    "\n",
    "            if done or T >= MAX_FRAMES_PER_EP-1:\n",
    "                break\n",
    "\n",
    "        success = (total_return > 0.5)\n",
    "        \n",
    "        discounted_reward = self._compute_discounted_return(rewards[:T])\n",
    "        exps = dict(\n",
    "            obs = torch.tensor(obss[:T], device=device),\n",
    "            action = actions[:T],\n",
    "            value  = values[:T],\n",
    "            reward = rewards[:T],\n",
    "            log_prob = log_probs[:T],\n",
    "            discounted_reward = discounted_reward,\n",
    "            T = T\n",
    "        )\n",
    "\n",
    "        logs = {\n",
    "            \"return_per_episode\": total_return,\n",
    "            \"num_frames\": T,\n",
    "            'success': success\n",
    "        }\n",
    "\n",
    "        return exps, logs\n",
    "\n",
    "    def _compute_policy_loss_ppo(self, dist:Categorical, factors, indices, old_logp, actions, advantages):\n",
    "        \"\"\"\n",
    "        Computes the policy loss for PPO.\n",
    "\n",
    "        obs: observeration to pass into acmodel. shape: (T,)\n",
    "        init_logp: log probabilities we get from the agent performing the action. shape: (T,)\n",
    "        old_logp: log probabilities from previous timestep. shape: (T,)\n",
    "        actions: action at this timestep. shape: (T,ImWidth,ImHeight,Channels)\n",
    "        advantages: the computed advantages. shape: (T,)\n",
    "\n",
    "        ---\n",
    "        returns\n",
    "\n",
    "        policy_loss : ppo policy loss as shown in line 6 of PPO alg. tensor.float. Shape (,1)\n",
    "        approx_kl: an appoximation of the kl_divergence. tensor.float. Shape (,1)\n",
    "        \"\"\"\n",
    "        policy_loss, approx_kl = 0, 0\n",
    "\n",
    "        coef = self.coef\n",
    "\n",
    "        entropy = dist.entropy()\n",
    "        logps = dist.log_prob(actions)\n",
    "        r_terms = torch.exp(logps - old_logp)\n",
    "        ppo_loss = torch.min(r_terms * advantages, torch.clamp(r_terms, 1 - self.args.clip_ratio, 1 + self.args.clip_ratio) * advantages)\n",
    "        \n",
    "        policy_loss_tensor = factors * ppo_loss + coef * entropy\n",
    "\n",
    "        policy_loss = - torch.mean(policy_loss_tensor[indices])\n",
    "\n",
    "        # approx_kl = torch.sum(torch.exp(old_logp) * (old_logp - logps)) / torch.sum(torch.exp(old_logp))\n",
    "        approx_kl = torch.mean((old_logp - logps) ** 2) / 2\n",
    "\n",
    "        return policy_loss, approx_kl\n",
    "    \n",
    "    def _compute_value_loss(self, values, returns):\n",
    "        value_loss = torch.mean((values - returns) ** 2)\n",
    "\n",
    "        return value_loss\n",
    "\n",
    "    def update_parameters(self, sb, update_v=True):\n",
    "        MAX_FRAMES_PER_EP = 300\n",
    "        T = sb['T']\n",
    "        with torch.no_grad():\n",
    "            dist, values = self.model(sb['obs'])\n",
    "        values = values.reshape(-1)\n",
    "        dist: Categorical\n",
    "        old_logp = dist.log_prob(sb['action'])\n",
    "        init_logp = sb['log_prob']\n",
    "\n",
    "        # add 0 to end of values until it has length MAX_FRAMES_PER_EP\n",
    "        values_extended = torch.cat([values, torch.zeros((MAX_FRAMES_PER_EP - len(values), ), device=get_device())], dim=0)\n",
    "        full_reward = torch.cat([sb['reward'], torch.zeros((MAX_FRAMES_PER_EP - len(sb['reward']), ), device=get_device())], dim=0)\n",
    "\n",
    "        if self.args.use_gae:\n",
    "            advantage = self._compute_advantage_gae(values_extended, full_reward, T)\n",
    "        else:\n",
    "            advantage = sb['discounted_reward'] - values.reshape(-1)\n",
    "\n",
    "        for i in range(self.args.train_ac_iters):\n",
    "            self.optim.zero_grad()\n",
    "            dist, values = self.model(sb['obs'])\n",
    "            values = values.reshape(-1)\n",
    "            # policy loss\n",
    "            factors = torch.exp(old_logp - init_logp)\n",
    "            indices = factors < self.args.importance_sampling_clip\n",
    "            fit = torch.mean(indices.to(torch.float32))\n",
    "\n",
    "            loss_pi, approx_kl = self._compute_policy_loss_ppo(dist, factors, indices, old_logp, sb['action'], advantage)\n",
    "            if update_v:\n",
    "                loss_v = self._compute_value_loss(values, sb['discounted_reward'])\n",
    "            else:\n",
    "                loss_v = 0.0\n",
    "\n",
    "            if i == 0:\n",
    "                policy_loss = loss_pi\n",
    "                value_loss = loss_v\n",
    "\n",
    "            loss = loss_v + loss_pi\n",
    "            if approx_kl > 1.5 * self.args.target_kl:\n",
    "                break\n",
    "\n",
    "            loss.backward(retain_graph=True)\n",
    "            self.optim.step()\n",
    "\n",
    "        update_policy_loss = policy_loss.item()\n",
    "        update_value_loss = value_loss.item()\n",
    "\n",
    "        logs = {\n",
    "            \"policy_loss\": update_policy_loss,\n",
    "            \"value_loss\": update_value_loss,\n",
    "            \"fit\": fit.item()\n",
    "        }\n",
    "\n",
    "        return logs\n",
    "    \n",
    "    def decrease_prob(self, sb, lr=0.1) -> None:\n",
    "        self.optim.zero_grad()\n",
    "\n",
    "        dist, _ = self.model(sb['obs'])\n",
    "        dist: Categorical\n",
    "        logps = dist.log_prob(sb['action'])\n",
    "\n",
    "        loss = lr * torch.mean(logps)\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self, ACModelClass, env, args:Config=None, seed=0):\n",
    "        \"\"\"\n",
    "        The PPO agent.\n",
    "        \"\"\"\n",
    "\n",
    "        self.env = env\n",
    "\n",
    "        set_random_seed(seed)\n",
    "        model = ACModelClass(use_critic=True)\n",
    "\n",
    "        if args is None:\n",
    "            args = Config()\n",
    "        self.machine = Machine(0.01, model, args)\n",
    "    \n",
    "    def train(self, max_episodes:int=10000, nonstop:bool=False) -> tuple[list[int], list[float]]:\n",
    "        \"\"\"\n",
    "        Train the PPO agent.\n",
    "\n",
    "        Returns:\n",
    "            num_frames, smooth_rs\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f'Start! Agent: PPO.')\n",
    "\n",
    "        is_solved = False\n",
    "\n",
    "        SMOOTH_REWARD_WINDOW = 50\n",
    "\n",
    "        rewards = [0]*SMOOTH_REWARD_WINDOW\n",
    "\n",
    "        total_smooth_rs = []\n",
    "        total_num_frames = []\n",
    "\n",
    "        num_frames = 0\n",
    "\n",
    "        pbar = tqdm(range(max_episodes))\n",
    "        for update in pbar:\n",
    "\n",
    "            exps, logs1 = self.machine.collect_experiences(self.env)\n",
    "\n",
    "            logs2 = self.machine.update_parameters(exps)\n",
    "\n",
    "            logs = {**logs1, **logs2}\n",
    "\n",
    "            total_num_frames.append(num_frames)\n",
    "            num_frames += logs[\"num_frames\"]\n",
    "\n",
    "            rewards.append(logs[\"return_per_episode\"])\n",
    "\n",
    "            smooth_reward = np.mean(rewards[-SMOOTH_REWARD_WINDOW:])\n",
    "\n",
    "            total_smooth_rs.append(smooth_reward)\n",
    "\n",
    "            data = {'num_frames':num_frames, 'smooth_reward':smooth_reward,\n",
    "                'reward':logs[\"return_per_episode\"], 'policy_loss':logs[\"policy_loss\"], 'value_loss': logs['value_loss'], 'episode':update}\n",
    "\n",
    "            pbar.set_postfix(data)\n",
    "\n",
    "            if not nonstop and smooth_reward >= self.machine.args.score_threshold:\n",
    "                is_solved = True\n",
    "                break\n",
    "\n",
    "        if not nonstop and is_solved:\n",
    "            print('Solved!')\n",
    "\n",
    "        return total_num_frames, total_smooth_rs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REPRO:\n",
    "    def __init__(self, ACModelClass, env, args:Config=None, seed=0):\n",
    "        \"\"\"\n",
    "        The REPRO agent.\n",
    "        \"\"\"\n",
    "\n",
    "        self.env = env\n",
    "\n",
    "        set_random_seed(seed)\n",
    "        m1 = ACModelClass(use_critic=True)\n",
    "\n",
    "        if args is None:\n",
    "            args = Config()\n",
    "\n",
    "        self.exploiter = Machine(0.01, m1, args)\n",
    "\n",
    "        m2 = ACModelClass(use_critic=False)\n",
    "        m3 = ACModelClass(use_critic=False)\n",
    "\n",
    "        self.explorer_random = Machine(0.03, m2, args)\n",
    "        self.explorer_thirsty = Machine(0.02, m3, args)\n",
    "        self.explorer_thirsty.copy_machine(self.exploiter)\n",
    "\n",
    "        self.temp_machine = Machine(0.01, m3, args)\n",
    "\n",
    "    def _replay(self, machine:Machine, buffer:list, cutoff:float) -> float:\n",
    "        \"\"\"\n",
    "        Replay random sample from buffer on machine.\n",
    "\n",
    "        Args:\n",
    "            machine: Machine to replay on\n",
    "            buffer: list of experiences\n",
    "            cutoff: if fit < cutoff, delete sb from buffer\n",
    "\n",
    "        Returns:\n",
    "            fit\n",
    "        \"\"\"\n",
    "\n",
    "        idx = np.random.randint(len(buffer))\n",
    "        sb = buffer[idx]\n",
    "        logs_replay = machine.update_parameters(sb)\n",
    "        fit = logs_replay['fit']\n",
    "        if fit < cutoff:\n",
    "            # delete sb from buffer\n",
    "            del buffer[idx]\n",
    "        \n",
    "        return fit\n",
    "    \n",
    "    def _add_sb_to_buffer(self, exps, buffer_r:list, larger_buffer_r:list) -> None:\n",
    "        buffer_r.append(exps)\n",
    "        larger_buffer_r.append(exps)\n",
    "        if len(buffer_r) > self.exploiter.args.replay_buffer_capacity:\n",
    "            del buffer_r[0]\n",
    "        if len(larger_buffer_r) > self.exploiter.args.large_buffer_capacity:\n",
    "            del larger_buffer_r[0]\n",
    "\n",
    "\n",
    "    def train(self, max_episodes:int=10000, nonstop:bool=False) -> tuple[list[int], list[float], list[float]]:\n",
    "        \"\"\"\n",
    "        Train the agent.\n",
    "\n",
    "        Returns:\n",
    "            num_frames, smooth_rs, fits\n",
    "        \"\"\"\n",
    "\n",
    "        print('Start! Agent: REPRO.')\n",
    "        EXPLORE_MODE = 0\n",
    "        EXPLOIT_MODE = 1\n",
    "        mode = EXPLORE_MODE\n",
    "\n",
    "        is_solved = False\n",
    "\n",
    "        SMOOTH_REWARD_WINDOW = 50\n",
    "\n",
    "        rewards = [0]*SMOOTH_REWARD_WINDOW\n",
    "\n",
    "        total_smooth_rs = []\n",
    "        total_num_frames = []\n",
    "        larger_buffer_r = []\n",
    "        buffer_r = []\n",
    "        fits = []\n",
    "\n",
    "        num_frames = 0\n",
    "\n",
    "        pbar = tqdm(range(max_episodes))\n",
    "        for update in pbar:\n",
    "            total_num_frames.append(num_frames)\n",
    "\n",
    "            if mode == EXPLORE_MODE:\n",
    "                if np.random.rand() < 0.5:\n",
    "                    m = self.explorer_random\n",
    "                else:\n",
    "                    m = self.explorer_thirsty\n",
    "\n",
    "                exps, logs1 = m.collect_experiences(self.env)\n",
    "\n",
    "                m.update_parameters(exps)\n",
    "                logs2 = self.exploiter.update_parameters(exps)\n",
    "\n",
    "                if len(larger_buffer_r) >= 1:\n",
    "                    self._replay(self.explorer_thirsty, larger_buffer_r, cutoff=0.0)\n",
    "\n",
    "            elif mode == EXPLOIT_MODE:\n",
    "                exps, logs1 = self.exploiter.collect_experiences(self.env)\n",
    "\n",
    "                logs2 = self.exploiter.update_parameters(exps)\n",
    "\n",
    "                assert len(buffer_r) > 0, f'buffer_r should not be empty.'\n",
    "\n",
    "                cutoff = self.exploiter.args.bad_fit_threshold + self.exploiter.args.bad_fit_increment * (len(buffer_r) - 1)\n",
    "                fit = self._replay(self.exploiter, buffer_r, cutoff)\n",
    "                fits.append(fit)\n",
    "\n",
    "                if len(buffer_r) == 0:\n",
    "                    print('empty')\n",
    "                    mode = EXPLORE_MODE\n",
    "                    self.explorer_thirsty.copy_machine(self.exploiter)\n",
    "                    self.explorer_random.copy_machine(self.temp_machine)\n",
    "                    self.temp_machine.copy_machine(self.exploiter)\n",
    "\n",
    "            else:\n",
    "                raise ValueError(f'Invalid mode: {mode} is not in [0, 1]')\n",
    "            \n",
    "            logs = {**logs1, **logs2}\n",
    "\n",
    "            num_frames += logs[\"num_frames\"]\n",
    "\n",
    "            rewards.append(logs[\"return_per_episode\"])\n",
    "\n",
    "            if logs['success']:\n",
    "                self._add_sb_to_buffer(exps, buffer_r, larger_buffer_r)\n",
    "                mode = EXPLOIT_MODE\n",
    "\n",
    "            smooth_reward = np.mean(rewards[-SMOOTH_REWARD_WINDOW:])\n",
    "\n",
    "            total_smooth_rs.append(smooth_reward)\n",
    "\n",
    "            data = {'num_frames':num_frames, 'smooth_reward':smooth_reward,\n",
    "                'reward':logs[\"return_per_episode\"], 'episode':update}\n",
    "\n",
    "            pbar.set_postfix(data)\n",
    "\n",
    "            if not nonstop and smooth_reward >= self.exploiter.args.score_threshold:\n",
    "                    is_solved = True\n",
    "                    break\n",
    "    \n",
    "        if is_solved:\n",
    "            print('Solved!')\n",
    "\n",
    "        return total_num_frames, total_smooth_rs, fits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACModel(nn.Module):\n",
    "    def __init__(self, use_critic=False):\n",
    "        \"\"\"\n",
    "        Represents an Actor Crictic model that takes a 2d, multi-channeled\n",
    "        image as input.\n",
    "\n",
    "        Parameters\n",
    "        ----\n",
    "        num_actions : int\n",
    "\n",
    "                      The action space of the environment.\n",
    "                      The action space for DoorKey5x5 is 7-dimensional:\n",
    "                      0: turn left,\n",
    "                      1: turn right,\n",
    "                      2: forward,\n",
    "                      3: pickup an object,\n",
    "                      4: drop an object,\n",
    "                      5: activate an object,\n",
    "                      6: done completing task\n",
    "\n",
    "        use_critics : bool\n",
    "\n",
    "                      Critic network will be used in forward pass if flag is set\n",
    "                      to true.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.use_critic = use_critic\n",
    "\n",
    "        # Define actor's model\n",
    "        self.image_conv_actor = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, (2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 2)),\n",
    "            nn.Conv2d(16, 32, (2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, (2, 2)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 7)\n",
    "        )\n",
    "\n",
    "        # Define critic's model\n",
    "        if self.use_critic:\n",
    "            self.image_conv_critic = nn.Sequential(\n",
    "                nn.Conv2d(3, 16, (2, 2)),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d((2, 2)),\n",
    "                nn.Conv2d(16, 32, (2, 2)),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(32, 64, (2, 2)),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "            self.critic = nn.Sequential(\n",
    "                nn.Linear(64, 64),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(64, 1)\n",
    "            )\n",
    "\n",
    "        # Initialize parameters correctly\n",
    "        self.apply(init_params)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        \"\"\"\n",
    "        Performs a forward pass through the actor-critic network\n",
    "\n",
    "        Parameters\n",
    "        ----\n",
    "        obs : int tensor. Shape [Batch size, ImWidth, ImHeight, Channels]\n",
    "\n",
    "              input to the network.\n",
    "        ----\n",
    "\n",
    "        returns:\n",
    "\n",
    "        dist : torch.distribution\n",
    "            The distribution of actions from policy. A Categorical distribution\n",
    "            for discreet action spaces.\n",
    "        value : torch.Tensor (Batch size, 1)\n",
    "            value output by critic network\n",
    "        \"\"\"\n",
    "        obs = torch.tensor(obs).float() # convert to float tensor\n",
    "        if len(obs.shape) == 3:\n",
    "            obs = obs.unsqueeze(0) # add batch dimension if not already there\n",
    "            \n",
    "        conv_in = obs.transpose(1, 3).transpose(2, 3) # reshape into [b, c, h, w]\n",
    "\n",
    "        dist, value = None, None\n",
    "\n",
    "        x = self.image_conv_actor(conv_in)\n",
    "        embedding = x.reshape(x.shape[0], -1)\n",
    "\n",
    "        x = self.actor(embedding)\n",
    "        dist = Categorical(logits=F.log_softmax(x, dim=1))\n",
    "\n",
    "        if self.use_critic:\n",
    "            y = self.image_conv_critic(conv_in)\n",
    "            embedding = y.reshape(y.shape[0], -1)\n",
    "\n",
    "            value = self.critic(embedding).squeeze(1)\n",
    "        else:\n",
    "            value = torch.zeros((x.shape[0], 1), device=x.device)\n",
    "        \n",
    "        return dist, value\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/atticusw/opt/anaconda3/lib/python3.9/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.get_frame to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.get_frame` for environment variables or `env.get_wrapper_attr('get_frame')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/atticusw/opt/anaconda3/lib/python3.9/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.highlight to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.highlight` for environment variables or `env.get_wrapper_attr('highlight')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/atticusw/opt/anaconda3/lib/python3.9/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.tile_size to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.tile_size` for environment variables or `env.get_wrapper_attr('tile_size')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/atticusw/opt/anaconda3/lib/python3.9/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.agent_pov to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.agent_pov` for environment variables or `env.get_wrapper_attr('agent_pov')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = get_door_key_env(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiments\n",
    "seed = 12\n",
    "\n",
    "# agent_PPO = PPO(ACModel, env=env, args=Config(), seed=seed)\n",
    "# num_frames_1, smooth_rs_1 = agent_PPO.train()\n",
    "\n",
    "logs = []\n",
    "\n",
    "for bad_fit_threshold in [0.8, 0.5, 0.2]:\n",
    "    for importance_sampling_clip in [3.0, 2.0, 1.5]:\n",
    "        agent = REPRO(ACModel, env=env, args=Config(bad_fit_threshold=bad_fit_threshold, importance_sampling_clip=importance_sampling_clip), seed=seed)\n",
    "        num_frames, smooth_rs, fits = agent.train()\n",
    "        logs.append((num_frames.copy(), smooth_rs.copy(), fits.copy()))\n",
    "        plt.plot(num_frames, smooth_rs, label=f'bad_fit={bad_fit_threshold}, clip={importance_sampling_clip}')\n",
    "        print(\"done one test\")\n",
    "\n",
    "plt.xlabel('Time steps')\n",
    "plt.ylabel('Average reward (smoothed)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CrossingEnv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/8m/r3kmm3xs1lngf9bhwg216mlc0000gn/T/ipykernel_34898/3997101515.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Crossing environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mMyCrossingEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCrossingEnv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobstacle_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \"\"\"\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CrossingEnv' is not defined"
     ]
    }
   ],
   "source": [
    "# Crossing environment\n",
    "\n",
    "class MyCrossingEnv(CrossingEnv):\n",
    "    def __init__(self, size, obstacle_type):\n",
    "        \"\"\"\n",
    "        size: odd number\n",
    "        obstacle_type: \n",
    "            - Lava: episode ends when agent steps on lava.\n",
    "            - anything else: lava is wall\n",
    "        \"\"\"\n",
    "        self.render_mode = \"rgb_array\"\n",
    "        super().__init__(size=size, obstacle_type=obstacle_type)\n",
    "    \n",
    "    def _reward(self):\n",
    "        \"\"\"\n",
    "        Compute the reward to be given upon success\n",
    "        \"\"\"\n",
    "        return 1\n",
    "\n",
    "\n",
    "def get_crossing_env(size, obstacle_type):\n",
    "    \"\"\"\n",
    "    Returns a Crossing environment with the given size.\n",
    "    \"\"\"\n",
    "    env = MyCrossingEnv(size=size, obstacle_type=obstacle_type)\n",
    "    env = ImgObsWrapper(env)\n",
    "\n",
    "    env.reset()\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    # get an RGB image corresponding to the whole environment or the agent's point of view (https://github.com/Farama-Foundation/Minigrid/blob/master/minigrid/minigrid_env.py#L716)\n",
    "    #            highlight (bool): If true, the agent's field of view or point of view is highlighted with a lighter gray color.\n",
    "    #            tile_size (int): How many pixels will form a tile from the NxM grid.\n",
    "    #            agent_pov (bool): If true, the rendered frame will only contain the point of view of the agent.\n",
    "    frame = env.get_frame(highlight=env.highlight, tile_size=env.tile_size, agent_pov=env.agent_pov)\n",
    "    # show an image to the notebook.\n",
    "    plt.imshow(frame)\n",
    "\n",
    "    return env\n",
    "\n",
    "env = get_crossing_env(11, Lava)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
