{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.10.14)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "import torch.nn.functional as F\n",
    "import minigrid\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from tqdm.notebook import tqdm\n",
    "from minigrid.envs.doorkey import DoorKeyEnv\n",
    "import pandas as pd\n",
    "from gym.envs.registration import registry, register\n",
    "import random\n",
    "from typing_extensions import Self\n",
    "import matplotlib.pyplot as plt\n",
    "from minigrid.wrappers import ObservationWrapper\n",
    "\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import pandas as pd\n",
    "from torch import optim\n",
    "from typing import Any\n",
    "from copy import deepcopy\n",
    "from collections import deque\n",
    "from minigrid.wrappers import ObservationWrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device() -> torch.device:\n",
    "    \"\"\"\n",
    "    Returns the device to use for training.\n",
    "    \"\"\"\n",
    "    #return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "def set_random_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "# Function from https://github.com/ikostrikov/pytorch-a2c-ppo-acktr/blob/master/model.py\n",
    "def init_params(m):\n",
    "    \"\"\"\n",
    "    Initialize parameters of the network.\n",
    "    m: torch.nn.Module\n",
    "    \"\"\"\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Linear\") != -1:\n",
    "        m.weight.data.normal_(0, 1)\n",
    "        m.weight.data *= 1 / torch.sqrt(m.weight.data.pow(2).sum(1, keepdim=True))\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0)\n",
    "\n",
    "class MyDoorKeyEnv(DoorKeyEnv):\n",
    "    def __init__(self, size):\n",
    "        self.render_mode = \"rgb_array\"\n",
    "        super().__init__(size=size)\n",
    "\n",
    "    def _reward(self):\n",
    "        \"\"\"\n",
    "        Compute the reward to be given upon success\n",
    "        \"\"\"\n",
    "        return 1\n",
    "\n",
    "class ImgObsWrapper(ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Use the image as the only observation output, no language/mission.\n",
    "\n",
    "    Parameters:\n",
    "    - env (gym.Env): The environment to wrap.\n",
    "\n",
    "    Methods:\n",
    "    - observation(self, obs): Returns the image from the observation.\n",
    "    - reset(self): Resets the environment and returns the initial observation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        \"\"\"\n",
    "        Initializes the ImgObsWrapper with the given environment.\n",
    "\n",
    "        Parameters:\n",
    "        - env (gym.Env): The environment whose observations are to be wrapped.\n",
    "        \"\"\"\n",
    "        super().__init__(env)\n",
    "        self.observation_space = env.observation_space.spaces[\"image\"]\n",
    "\n",
    "    def observation(self, obs):\n",
    "        \"\"\"\n",
    "        Extracts and returns the image data from the observation.\n",
    "\n",
    "        Parameters:\n",
    "        - obs (dict or tuple): The original observation from the environment, which could be either\n",
    "        a dictionary or a tuple containing a dictionary.\n",
    "\n",
    "        Returns:\n",
    "        - np.ndarray: The image data extracted from the observation.\n",
    "        \"\"\"\n",
    "        if type(obs) == tuple:\n",
    "            return obs[0][\"image\"]\n",
    "        return obs[\"image\"]\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the environment and returns the initial observation image.\n",
    "\n",
    "        Returns:\n",
    "        - np.ndarray: The initial observation image of the reset environment.\n",
    "        \"\"\"\n",
    "        obs = super().reset()\n",
    "        return obs[0]\n",
    "\n",
    "def get_door_key_env(size):\n",
    "    \"\"\"\n",
    "    Returns a DoorKeyEnv environment with the given size.\n",
    "    \"\"\"\n",
    "    env = MyDoorKeyEnv(size=size)\n",
    "    env = ImgObsWrapper(env)\n",
    "\n",
    "    env.reset()\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    # get an RGB image corresponding to the whole environment or the agent's point of view (https://github.com/Farama-Foundation/Minigrid/blob/master/minigrid/minigrid_env.py#L716)\n",
    "    #            highlight (bool): If true, the agent's field of view or point of view is highlighted with a lighter gray color.\n",
    "    #            tile_size (int): How many pixels will form a tile from the NxM grid.\n",
    "    #            agent_pov (bool): If true, the rendered frame will only contain the point of view of the agent.\n",
    "    frame = env.get_frame(highlight=env.highlight, tile_size=env.tile_size, agent_pov=env.agent_pov)\n",
    "    # show an image to the notebook.\n",
    "    plt.imshow(frame)\n",
    "\n",
    "    return env\n",
    "\n",
    "class Config:\n",
    "    \"\"\"\n",
    "    Stores algorithmic hyperparameters.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                score_threshold=0.93,\n",
    "                discount=0.995,\n",
    "                lr=1e-3,\n",
    "                max_grad_norm=0.5,\n",
    "                log_interval=10,\n",
    "                gae_lambda=0.95,\n",
    "                clip_ratio=0.2,\n",
    "                target_kl=0.01,\n",
    "                train_ac_iters=5,\n",
    "                use_discounted_reward=True,\n",
    "                use_gae=True,\n",
    "                importance_sampling_clip=2.0,\n",
    "                bad_fit_threshold=0.8,\n",
    "                bad_fit_increment=None,\n",
    "                replay_buffer_capacity=10,\n",
    "                large_buffer_capacity=20):\n",
    "\n",
    "        self.score_threshold = score_threshold # criterion for early stopping. If the rolling average reward (over the last 100 episodes) is greater than it, it ends.\n",
    "        self.discount = discount # discount factor\n",
    "        self.lr = lr # learning rate\n",
    "        self.max_grad_norm = max_grad_norm # the maximum gradient norm (https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html)\n",
    "        self.log_interval = log_interval # logging interval\n",
    "        self.clip_ratio = clip_ratio # clip_ratio of PPO.\n",
    "        self.target_kl = target_kl # target KL divergence for early stoping train_ac_iters for PPO\n",
    "        self.train_ac_iters = train_ac_iters # how many time to train ac_model using current computed old_logps\n",
    "        self.gae_lambda=gae_lambda # lambda in Generalized Advantage Estimation (GAE)\n",
    "        self.use_discounted_reward=use_discounted_reward # whether use discounted reward or not.\n",
    "        self.use_gae = use_gae # whether to use GAE or not.\n",
    "        self.importance_sampling_clip = importance_sampling_clip # importance sampling clip threshold\n",
    "        self.bad_fit_threshold = bad_fit_threshold # threshold for bad fit.\n",
    "        if bad_fit_increment is None:\n",
    "            bad_fit_increment = (1.0 - bad_fit_threshold) / replay_buffer_capacity\n",
    "        self.bad_fit_increment = bad_fit_increment # increment for bad fit.\n",
    "        self.replay_buffer_capacity = replay_buffer_capacity # capacity of replay buffer.\n",
    "        self.large_buffer_capacity = large_buffer_capacity # capacity of large replay buffer.\n",
    "\n",
    "class Machine:\n",
    "    def __init__(self, entropy_coef, init_model:nn.Module, args:Config=None):\n",
    "        \"\"\"\n",
    "        A Machine object consists of a Model and its entropy_coef\n",
    "\n",
    "        Args:\n",
    "            entropy_coef: Entropy coefficient.\n",
    "            init_model: Initial model.\n",
    "            args\n",
    "        \"\"\"\n",
    "        if args is None:\n",
    "            self.args = Config()\n",
    "        else:\n",
    "            self.args = args\n",
    "\n",
    "        self.model = init_model\n",
    "        self.optim = torch.optim.Adam(self.model.parameters(), lr=self.args.lr)\n",
    "        self.coef = entropy_coef\n",
    "        self.device = get_device()\n",
    "\n",
    "    def copy_model(self, other_model:nn.Module) -> None:\n",
    "        \"\"\"\n",
    "        Copy state dict from 'model'. Reset rs.\n",
    "        \"\"\"\n",
    "        state_dict = other_model.state_dict()\n",
    "        for key, v in state_dict.items():\n",
    "            if key in self.model.state_dict():\n",
    "                self.model.state_dict()[key].copy_(v)\n",
    "\n",
    "    def copy_machine(self, other:Self) -> None:\n",
    "        \"\"\"\n",
    "        Copy state dict from 'other'. Reset rs.\n",
    "        \"\"\"\n",
    "        self.copy_model(other.model)\n",
    "\n",
    "    def _compute_discounted_return(self, rewards):\n",
    "        \"\"\"\n",
    "            rewards: reward obtained at timestep.  Shape: (T,)\n",
    "            discount: discount factor. float\n",
    "\n",
    "        ----\n",
    "        returns: sum of discounted rewards. Shape: (T,)\n",
    "        \"\"\"\n",
    "        returns = torch.zeros(*rewards.shape, device=self.device)\n",
    "\n",
    "        R = 0\n",
    "        for t in reversed(range((rewards.shape[0]))):\n",
    "            R = rewards[t] + self.args.discount * R\n",
    "            returns[t] = R\n",
    "        return returns\n",
    "\n",
    "    def _compute_advantage_gae(self, values, rewards, T):\n",
    "        \"\"\"\n",
    "        Compute Adavantage wiht GAE. See Section 4.4.2 in the lecture notes.\n",
    "\n",
    "        values: value at each timestep (T,)\n",
    "        rewards: reward obtained at each timestep.  Shape: (T,)\n",
    "        T: the number of frames, float\n",
    "        gae_lambda: hyperparameter, float\n",
    "        discount: discount factor, float\n",
    "\n",
    "        -----\n",
    "\n",
    "        returns:\n",
    "\n",
    "        advantages : tensor.float. Shape [T,]\n",
    "\n",
    "                    gae advantage term for timesteps 0 to T\n",
    "\n",
    "        \"\"\"\n",
    "        advantages = torch.zeros_like(values)\n",
    "        for i in reversed(range(T)):\n",
    "            next_value = values[i+1]\n",
    "            next_advantage = advantages[i+1]\n",
    "\n",
    "            delta = rewards[i] + self.args.discount * next_value  - values[i]\n",
    "            advantages[i] = delta + self.args.discount * self.args.gae_lambda * next_advantage\n",
    "        return advantages[:T]\n",
    "\n",
    "    def collect_experiences(self, env:gym.Env):\n",
    "        \"\"\"\n",
    "        Collects rollouts and computes advantages.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        exps : dict\n",
    "            Contains actions, rewards, advantages etc as attributes.\n",
    "            Each attribute, e.g. `exps['reward']` has a shape\n",
    "            (self.num_frames, ...).\n",
    "        logs : dict\n",
    "            Useful stats about the training process, including the average\n",
    "            reward, policy loss, value loss, etc.\n",
    "        \"\"\"\n",
    "        device = get_device()\n",
    "\n",
    "        MAX_FRAMES_PER_EP = 300\n",
    "        shape = (MAX_FRAMES_PER_EP, )\n",
    "\n",
    "        actions = torch.zeros(*shape, device=device, dtype=torch.int)\n",
    "        values = torch.zeros(*shape, device=device)\n",
    "        rewards = torch.zeros(*shape, device=device)\n",
    "        log_probs = torch.zeros(*shape, device=device)\n",
    "        obss = [None]*MAX_FRAMES_PER_EP\n",
    "\n",
    "        obs = env.reset()\n",
    "\n",
    "        total_return = 0\n",
    "\n",
    "        T = 0\n",
    "\n",
    "        while True:\n",
    "            # Do one agent-environment interaction\n",
    "            with torch.no_grad():\n",
    "                dist, value = self.model(obs)\n",
    "\n",
    "            dist: Categorical\n",
    "            action = dist.sample()[0]\n",
    "\n",
    "            obss[T] = obs\n",
    "            obs, reward, done, _, _ = env.step(action.item())\n",
    "\n",
    "            # Update experiences values\n",
    "            actions[T] = action\n",
    "            values[T] = value\n",
    "            rewards[T] = reward\n",
    "            log_probs[T] = dist.log_prob(action)\n",
    "\n",
    "            total_return += reward\n",
    "            T += 1\n",
    "\n",
    "            if done or T >= MAX_FRAMES_PER_EP-1:\n",
    "                break\n",
    "\n",
    "        success = (total_return > 0.5)\n",
    "\n",
    "        discounted_reward = self._compute_discounted_return(rewards[:T])\n",
    "        exps = dict(\n",
    "            obs = torch.tensor(np.array(obss[:T]), device=device),\n",
    "            action = actions[:T],\n",
    "            value  = values[:T],\n",
    "            reward = rewards[:T],\n",
    "            log_prob = log_probs[:T],\n",
    "            discounted_reward = discounted_reward,\n",
    "            T = T\n",
    "        )\n",
    "\n",
    "        logs = {\n",
    "            \"return_per_episode\": total_return,\n",
    "            \"num_frames\": T,\n",
    "            'success': success\n",
    "        }\n",
    "\n",
    "        return exps, logs\n",
    "\n",
    "    def _compute_policy_loss_ppo(self, dist:Categorical, factors, indices, old_logp, actions, advantages):\n",
    "        \"\"\"\n",
    "        Computes the policy loss for PPO.\n",
    "\n",
    "        obs: observeration to pass into acmodel. shape: (T,)\n",
    "        init_logp: log probabilities we get from the agent performing the action. shape: (T,)\n",
    "        old_logp: log probabilities from previous timestep. shape: (T,)\n",
    "        actions: action at this timestep. shape: (T,ImWidth,ImHeight,Channels)\n",
    "        advantages: the computed advantages. shape: (T,)\n",
    "\n",
    "        ---\n",
    "        returns\n",
    "\n",
    "        policy_loss : ppo policy loss as shown in line 6 of PPO alg. tensor.float. Shape (,1)\n",
    "        approx_kl: an appoximation of the kl_divergence. tensor.float. Shape (,1)\n",
    "        \"\"\"\n",
    "        policy_loss, approx_kl = 0, 0\n",
    "\n",
    "        coef = self.coef\n",
    "\n",
    "        entropy = dist.entropy()\n",
    "        logps = dist.log_prob(actions)\n",
    "        r_terms = torch.exp(logps - old_logp)\n",
    "        ppo_loss = torch.min(r_terms * advantages, torch.clamp(r_terms, 1 - self.args.clip_ratio, 1 + self.args.clip_ratio) * advantages)\n",
    "\n",
    "        policy_loss_tensor = factors * ppo_loss + coef * entropy\n",
    "\n",
    "        policy_loss = - torch.mean(policy_loss_tensor[indices])\n",
    "\n",
    "        # approx_kl = torch.sum(torch.exp(old_logp) * (old_logp - logps)) / torch.sum(torch.exp(old_logp))\n",
    "        approx_kl = torch.mean((old_logp - logps) ** 2) / 2\n",
    "\n",
    "        return policy_loss, approx_kl\n",
    "\n",
    "    def _compute_value_loss(self, values, returns):\n",
    "        value_loss = torch.mean((values - returns) ** 2)\n",
    "\n",
    "        return value_loss\n",
    "\n",
    "    def update_parameters(self, sb, update_v=True):\n",
    "        MAX_FRAMES_PER_EP = 300\n",
    "        T = sb['T']\n",
    "        with torch.no_grad():\n",
    "            dist, values = self.model(sb['obs'])\n",
    "        values = values.reshape(-1)\n",
    "        dist: Categorical\n",
    "        old_logp = dist.log_prob(sb['action'])\n",
    "        init_logp = sb['log_prob']\n",
    "\n",
    "        # add 0 to end of values until it has length MAX_FRAMES_PER_EP\n",
    "        values_extended = torch.cat([values, torch.zeros((MAX_FRAMES_PER_EP - len(values), ), device=get_device())], dim=0)\n",
    "        full_reward = torch.cat([sb['reward'], torch.zeros((MAX_FRAMES_PER_EP - len(sb['reward']), ), device=get_device())], dim=0)\n",
    "\n",
    "        if self.args.use_gae:\n",
    "            advantage = self._compute_advantage_gae(values_extended, full_reward, T)\n",
    "        else:\n",
    "            advantage = sb['discounted_reward'] - values.reshape(-1)\n",
    "\n",
    "        for i in range(self.args.train_ac_iters):\n",
    "            self.optim.zero_grad()\n",
    "            dist, values = self.model(sb['obs'])\n",
    "            values = values.reshape(-1)\n",
    "            # policy loss\n",
    "            factors = torch.exp(old_logp - init_logp)\n",
    "            indices = factors < self.args.importance_sampling_clip\n",
    "            fit = torch.mean(indices.to(torch.float32))\n",
    "\n",
    "            loss_pi, approx_kl = self._compute_policy_loss_ppo(dist, factors, indices, old_logp, sb['action'], advantage)\n",
    "            if update_v:\n",
    "                loss_v = self._compute_value_loss(values, sb['discounted_reward'])\n",
    "            else:\n",
    "                loss_v = 0.0\n",
    "\n",
    "            if i == 0:\n",
    "                policy_loss = loss_pi\n",
    "                value_loss = loss_v\n",
    "\n",
    "            loss = loss_v + loss_pi\n",
    "            if approx_kl > 1.5 * self.args.target_kl:\n",
    "                break\n",
    "\n",
    "            loss.backward(retain_graph=True)\n",
    "            self.optim.step()\n",
    "\n",
    "        update_policy_loss = policy_loss.item()\n",
    "        update_value_loss = value_loss.item()\n",
    "\n",
    "        logs = {\n",
    "            \"policy_loss\": update_policy_loss,\n",
    "            \"value_loss\": update_value_loss,\n",
    "            \"fit\": fit.item()\n",
    "        }\n",
    "\n",
    "        return logs\n",
    "\n",
    "    def decrease_prob(self, sb, lr=0.1) -> None:\n",
    "        self.optim.zero_grad()\n",
    "\n",
    "        dist, _ = self.model(sb['obs'])\n",
    "        dist: Categorical\n",
    "        logps = dist.log_prob(sb['action'])\n",
    "\n",
    "        loss = lr * torch.mean(logps)\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, ACModelClass, env, args:Config=None, seed=0):\n",
    "        \"\"\"\n",
    "        The PPO agent.\n",
    "        \"\"\"\n",
    "\n",
    "        self.env = env\n",
    "\n",
    "        set_random_seed(seed)\n",
    "        model = ACModelClass(use_critic=True).to(get_device())\n",
    "\n",
    "        if args is None:\n",
    "            args = Config()\n",
    "        self.machine = Machine(0.01, model, args)\n",
    "\n",
    "    def train(self, max_episodes:int=10000, nonstop:bool=False, max_frames=float('inf')) -> tuple[list[int], list[float]]:\n",
    "        \"\"\"\n",
    "        Train the PPO agent.\n",
    "\n",
    "        Returns:\n",
    "            num_frames, smooth_rs\n",
    "        \"\"\"\n",
    "\n",
    "        print(f'Start! Agent: PPO.')\n",
    "\n",
    "        is_solved = False\n",
    "\n",
    "        SMOOTH_REWARD_WINDOW = 50\n",
    "\n",
    "        rewards = [0]*SMOOTH_REWARD_WINDOW\n",
    "\n",
    "        total_smooth_rs = []\n",
    "        total_num_frames = []\n",
    "\n",
    "        num_frames = 0\n",
    "\n",
    "        pbar = tqdm(range(max_episodes))\n",
    "        for update in pbar:\n",
    "\n",
    "            exps, logs1 = self.machine.collect_experiences(self.env)\n",
    "\n",
    "            logs2 = self.machine.update_parameters(exps)\n",
    "\n",
    "            logs = {**logs1, **logs2}\n",
    "\n",
    "            total_num_frames.append(num_frames)\n",
    "            num_frames += logs[\"num_frames\"]\n",
    "\n",
    "            rewards.append(logs[\"return_per_episode\"])\n",
    "\n",
    "            smooth_reward = np.mean(rewards[-SMOOTH_REWARD_WINDOW:])\n",
    "\n",
    "            total_smooth_rs.append(smooth_reward)\n",
    "\n",
    "            data = {'num_frames':num_frames, 'smooth_reward':smooth_reward,\n",
    "                'reward':logs[\"return_per_episode\"], 'policy_loss':logs[\"policy_loss\"], 'value_loss': logs['value_loss'], 'episode':update}\n",
    "\n",
    "            pbar.set_postfix(data)\n",
    "\n",
    "            if not nonstop and smooth_reward >= self.machine.args.score_threshold:\n",
    "                is_solved = True\n",
    "                break\n",
    "            if num_frames >= max_frames:\n",
    "                break\n",
    "\n",
    "        if not nonstop and is_solved:\n",
    "            print('Solved!')\n",
    "\n",
    "        return total_num_frames, total_smooth_rs\n",
    "\n",
    "class DQNetwork(nn.Module):\n",
    "    def __init__(self, action_dim):\n",
    "        \"\"\"\n",
    "        Initializes the DQNetwork with a convolutional neural network architecture.\n",
    "\n",
    "        Parameters:\n",
    "        - action_dim (int): The number of possible actions, determining the output size of the network.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.conv_net = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, (3, 3)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, (3, 3)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, (3, 3)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fcs = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, ob):\n",
    "        \"\"\"\n",
    "        Processes an observation through the network to predict Q values for each action.\n",
    "\n",
    "        Parameters:\n",
    "        - ob (torch.Tensor): The input observation image of shape [batch_size, height, width, channels].\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: The predicted Q values for each action, of shape [batch_size, action_dim].\n",
    "        \"\"\"\n",
    "        #### TODO (5pts): get the Q values for each action given the input\n",
    "        #### the input shape is: [batch_size, H, W, 3]\n",
    "        #### output shape should be: [batch_size, # of actions]\n",
    "        ob = ob.permute(0, 3, 1, 2)\n",
    "        bs = ob.size(0)\n",
    "        out = self.conv_net(ob)\n",
    "        out = out.view(bs, -1)\n",
    "        out = self.fcs(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "# create a replay buffer\n",
    "class CyclicBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = []\n",
    "        self.capacity = capacity\n",
    "        self.cur_pos = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.buffer[item]\n",
    "\n",
    "    def append(self, data):\n",
    "        \"\"\"\n",
    "        Adds a new piece of data to the buffer.\n",
    "\n",
    "        Parameters:\n",
    "        - data: The data to be added to the buffer.\n",
    "        \"\"\"\n",
    "        #### TODO (10pts): add data to the buffer\n",
    "        #### if the buffer is not full yet, you can simply append the data to the buffer\n",
    "        #### otherwise, you need to replace the oldest data with the current data (FIFO)\n",
    "        #### Hint: you may find self.cur_pos useful, it can be used as a position index\n",
    "        #### to keep track of where to add data\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(data)\n",
    "            self.cur_pos = (self.cur_pos + 1) % self.capacity\n",
    "        else:\n",
    "            self.buffer[self.cur_pos] = data\n",
    "            self.cur_pos = (self.cur_pos + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Randomly selects a batch of data from the buffer. The size of the batch is the minimum of the requested\n",
    "        batch size and the current size of the buffer. If the requested batch size equals the buffer size, all\n",
    "        data in the buffer is returned.\n",
    "\n",
    "        Parameters:\n",
    "        - batch_size (int): The size of the batch to sample.\n",
    "\n",
    "        Returns:\n",
    "        - list: A list containing the sampled batch of data.\n",
    "        \"\"\"\n",
    "        #### TODO (10pts): sample a batch from the buffer\n",
    "        bs = min(batch_size, len(self.buffer))\n",
    "        return random.sample(self.buffer, bs)\n",
    "\n",
    "    def get_all(self):\n",
    "        \"\"\"\n",
    "        Retrieves all data stored in the buffer.\n",
    "\n",
    "        Returns:\n",
    "        - list: A deepcopy of all data currently stored in the buffer.\n",
    "        \"\"\"\n",
    "        return deepcopy(self.buffer)\n",
    "\n",
    "    def clear(self):\n",
    "        \"\"\"\n",
    "        Removes all data from the buffer, effectively resetting its state.\n",
    "        \"\"\"\n",
    "        self.buffer.clear()\n",
    "        self.cur_pos = 0\n",
    "\n",
    "@dataclass\n",
    "class DQNAgent:\n",
    "    env: gym.Env\n",
    "    learning_rate: float\n",
    "    gamma: float\n",
    "    memory_size: int # We use memory/buffer interchangeably\n",
    "    initial_epsilon: float\n",
    "    min_epsilon: float\n",
    "    max_epsilon_decay_steps: int\n",
    "    warmup_steps: int\n",
    "    batch_size: int\n",
    "    target_update_freq: int\n",
    "    enable_double_q: bool = False\n",
    "    disable_target_net: bool = False\n",
    "    device: str = None\n",
    "    tau: float = 0.0005 # changed from 0.005 to 0.0005\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the agent to its initial state.\n",
    "        \"\"\"\n",
    "        if self.device is None:\n",
    "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        #### TODO: create a Deep Q network Agent.\n",
    "        #### For our purposes include the following:\n",
    "        #### -a replay buffer with capacity=self.memory_size. Memory = Buffer\n",
    "        self.memory = CyclicBuffer(self.memory_size)\n",
    "        #### -an Adam optimizer with lr=self.learning_rate,\n",
    "        self.qnet = DQNetwork(action_dim=7).to(self.device)\n",
    "        self.optim = torch.optim.Adam(self.qnet.parameters(), lr=self.learning_rate)\n",
    "        #### -SmoothL1 loss instance,\n",
    "        self.loss = nn.SmoothL1Loss()\n",
    "        ####  -a deep Q network\n",
    "        ####  -A deep Q TARGET network (Make sure to set to eval mode)\n",
    "        self.target_qnet = DQNetwork(action_dim=7).to(self.device)\n",
    "        self.target_qnet.eval()\n",
    "        #### -Make sure the networks are on the correct device!!! use [tensor].to(self.device)\n",
    "\n",
    "        ####\n",
    "        self.epsilon = self.initial_epsilon\n",
    "        self.ep_reduction = (self.epsilon - self.min_epsilon) / float(self.max_epsilon_decay_steps)\n",
    "        if self.disable_target_net:\n",
    "            #### TODO: set the target_update_freq to be proper value so that the target Q network will always be same as the Q network\n",
    "            #### You don't need to fill in this value until Q4.3\n",
    "            self.target_update_freq = 1\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_action(self, ob, greedy_only=False):\n",
    "        \"\"\"\n",
    "        Selects an action based on the current state observation using an epsilon-greedy policy.\n",
    "\n",
    "        Parameters:\n",
    "        - ob (numpy.ndarray): The current state observation.\n",
    "        - greedy_only (bool): If True, the method always selects the action with the highest Q value. If False, it\n",
    "        selects a random action with probability epsilon.\n",
    "\n",
    "        Returns:\n",
    "        - int: The selected action.\n",
    "        \"\"\"\n",
    "        ob = ob[np.newaxis, :]\n",
    "        ob = torch.from_numpy(ob).float().to(self.device)\n",
    "        q_val = self.qnet(ob)\n",
    "        action = self.epsilon_greedy_policy(q_val, greedy_only=greedy_only)\n",
    "        return action\n",
    "\n",
    "    def epsilon_greedy_policy(self, q_values, greedy_only=False):\n",
    "        \"\"\"\n",
    "        Implements an epsilon-greedy policy for action selection.\n",
    "\n",
    "        Parameters:\n",
    "        - q_values (torch.Tensor): The Q values for all actions in the current state.\n",
    "        - greedy_only (bool): If True, ignores epsilon and selects the action with the highest Q value.\n",
    "\n",
    "        Returns:\n",
    "        - int: The index of the selected action.\n",
    "        \"\"\"\n",
    "        #### TODO: epsilon greedy exploration\n",
    "        #### we have an extra flag `greedy_only` here,\n",
    "        #### if greedy_only is True, then we need to return the action that\n",
    "        #### has the maximum Q values.\n",
    "        #### if greedy_only is False, we do epsilon greedy.\n",
    "        mode = 'random'\n",
    "        if greedy_only:\n",
    "            mode = 'greedy'\n",
    "        else:\n",
    "            if random.random() < self.epsilon:\n",
    "                mode = 'random'\n",
    "            else:\n",
    "                mode = 'greedy'\n",
    "\n",
    "        if mode == 'random':\n",
    "            action = np.random.randint(0, 7)\n",
    "        elif mode == 'greedy':\n",
    "            action = torch.argmax(q_values).item()\n",
    "        else:\n",
    "            raise ValueError('Unknown mode')\n",
    "\n",
    "        return action\n",
    "\n",
    "    def add_to_memory(self, ob, next_ob, action, reward, done):\n",
    "        \"\"\"\n",
    "        Adds an experience tuple to the replay buffer.\n",
    "\n",
    "        Parameters:\n",
    "        - ob: The current state observation.\n",
    "        - next_ob: The next state observation after taking the action.\n",
    "        - action: The action taken in the current state.\n",
    "        - reward: The reward received after taking the action.\n",
    "        - done: A boolean indicating whether the episode has ended.\n",
    "        \"\"\"\n",
    "        #### TODO: add data to the replay buffer. Avoid np.arrays.\n",
    "        data = (ob, next_ob, action, reward, done)\n",
    "        self.memory.append(data)\n",
    "\n",
    "    def update_Q(self):\n",
    "        \"\"\"\n",
    "        Performs a single update step of the Q-network based on a batch of experiences from the replay buffer.\n",
    "\n",
    "        Returns:\n",
    "        - float: The loss value of the update step.\n",
    "        \"\"\"\n",
    "        # we only start updating the Q network if there are enough samples in the replay buffer\n",
    "        if len(self.memory) < self.warmup_steps:\n",
    "            return 0\n",
    "\n",
    "        #### TODO: sample data from the replay buffer, and put them on the correct device (use [tensor].to(self.device))\n",
    "        #### you need to make sure the variables are in the right tensor shape.\n",
    "        data = self.memory.sample(self.batch_size)\n",
    "\n",
    "\n",
    "        #### TODO: update Q function with Bellman backup. Torch.gather() may prove useful for this section\n",
    "        ##### get Q(s_t, a_t)\n",
    "        obs, next_obs, actions, rewards, dones = zip(*data)\n",
    "        obs = np.array(obs)\n",
    "        next_obs = np.array(next_obs)\n",
    "        actions = np.array(actions)\n",
    "        rewards = np.array(rewards)\n",
    "        dones = np.array(dones)\n",
    "        obs = torch.tensor(obs, device=self.device, dtype=torch.float32)\n",
    "        next_obs = torch.tensor(next_obs, device=self.device, dtype=torch.float32)\n",
    "        actions = torch.tensor(actions, device=self.device)\n",
    "        rewards = torch.tensor(rewards, device=self.device, dtype=torch.float32)\n",
    "        dones = torch.tensor(dones, device=self.device, dtype=torch.int32)\n",
    "\n",
    "        q_values = self.qnet(obs)\n",
    "        # then select the index from q_values according to actions\n",
    "        q_values = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "\n",
    "        ##### get maxQ(s_{t+1}, a_{t+1})\n",
    "        ##### you will need to implement both DQN and double DQN here\n",
    "        ##### i.e., you need to check `if self.enable_double_q`\n",
    "        ##### remember to not propogate gradient when working with target\n",
    "        if self.enable_double_q:\n",
    "            with torch.no_grad():\n",
    "                out = self.qnet(next_obs)\n",
    "                max_q_indices = torch.max(out, dim=1)[1]\n",
    "\n",
    "                out = self.target_qnet(next_obs)\n",
    "                max_q_values = out.gather(1, max_q_indices.unsqueeze(1)).squeeze(1)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                out = self.target_qnet(next_obs)\n",
    "                max_q_values = torch.max(out, dim=1)[0]\n",
    "\n",
    "\n",
    "        ##### get the target Q value from the bellman equation\n",
    "        target_q_values = rewards + (1 - dones) * self.gamma * max_q_values\n",
    "\n",
    "        ##### update the Q network\n",
    "        self.optim.zero_grad()\n",
    "        loss = self.loss(target_q_values, q_values)\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"\n",
    "        Reduces epsilon linearly over time until it reaches min_epsilon, ensuring that the exploration rate decreases\n",
    "        as the agent learns more about the environment.\n",
    "        \"\"\"\n",
    "        #### TODO: linearly decay epsilon\n",
    "        #### reduce epsilon value by ep_reduction every time the function is called,\n",
    "        #### make sure epsilon is not smaller than self.min_epsilon\n",
    "        self.epsilon = max(self.epsilon - self.ep_reduction, self.min_epsilon)\n",
    "\n",
    "    def set_epsilon(self, eps) -> None:\n",
    "        \"\"\"\n",
    "        Sets the epsilon value to a specified value.\n",
    "\n",
    "        Parameters:\n",
    "        - eps (float): The new epsilon value.\n",
    "        \"\"\"\n",
    "        self.epsilon = eps\n",
    "\n",
    "    def update_target_qnet(self, step, soft=True):\n",
    "        \"\"\"\n",
    "        Updates the target Q-network.\n",
    "\n",
    "        Parameters:\n",
    "        - step (int): The current step number, used to determine when to perform hard updates.\n",
    "        - soft (bool): If True, performs a soft update; otherwise, performs a hard update.\n",
    "        \"\"\"\n",
    "        if not soft:\n",
    "            if step % self.target_update_freq == 0:\n",
    "                #### TODO: update the target Q function in a \"hard\" way\n",
    "                #### copy the parameter values in self.qnet into self.target_qnet\n",
    "                ### use .copy_() to avoid pointer issues\n",
    "                state_dict = self.qnet.state_dict()\n",
    "                for key, v in state_dict.items():\n",
    "                    self.target_qnet.state_dict()[key].copy_(v)\n",
    "                self.target_qnet.eval()\n",
    "        else:\n",
    "            #### TODO: soft update on taget Q network.\n",
    "            #### similar to polyak averaging, we update the target Q network slowly\n",
    "            #### $\\theta_Qtgt = \\tau*\\theta_Qtgt + (1-\\tau)*\\theta_Q\n",
    "            ###  use .copy_() to avoid pointer issues\n",
    "            state_dict = self.qnet.state_dict()\n",
    "            for key, v in state_dict.items():\n",
    "                self.target_qnet.state_dict()[key].copy_((1 - self.tau) * self.target_qnet.state_dict()[key] + self.tau * v)\n",
    "            self.target_qnet.eval()\n",
    "\n",
    "# you don't need to modify the following code.\n",
    "@dataclass\n",
    "class DQNEngine:\n",
    "    env: gym.Env\n",
    "    agent: DQNAgent\n",
    "    max_steps: int\n",
    "    show_progress: bool = False\n",
    "    show_video: bool = False\n",
    "\n",
    "    def test(self, env=None, render=False):\n",
    "        \"\"\"\n",
    "        Evaluates the agent's performance in the given environment.\n",
    "\n",
    "        Parameters:\n",
    "        - env (gym.Env, optional): The environment to test the agent in. If None, uses the engine's environment.\n",
    "        - render (bool): Whether to render the environment at each step.\n",
    "\n",
    "        Returns:\n",
    "        - float: The total reward accumulated over the episode.\n",
    "        \"\"\"\n",
    "        env = self.env if env is None else env\n",
    "        ob = env.reset()\n",
    "        ret = 0\n",
    "        for i in range(300):\n",
    "            if render:\n",
    "                env.render()\n",
    "            action = self.agent.get_action(ob, greedy_only=True)\n",
    "            next_ob, reward, done, truncated, info = env.step(action)\n",
    "            ret += reward\n",
    "            ob = next_ob\n",
    "            if done:\n",
    "                break\n",
    "        return ret\n",
    "\n",
    "    def run(self, n_runs=1):\n",
    "        \"\"\"\n",
    "        Executes multiple runs of the agent in the environment, training the agent in each run.\n",
    "\n",
    "        Parameters:\n",
    "        - n_runs (int): The number of separate runs to execute.\n",
    "\n",
    "        Returns:\n",
    "        - list: A list of pandas DataFrame logs for each run, containing columns for the cumulative reward ('return'),\n",
    "                the step count ('steps'), the episode number ('episode'), and the initial epsilon value ('epsilon').\n",
    "        \"\"\"\n",
    "        eps = 0.999\n",
    "\n",
    "        rewards = []\n",
    "        log = []\n",
    "\n",
    "        for i in tqdm(range(n_runs), desc='Runs'):\n",
    "            ep_rewards = []\n",
    "            ep_steps = []\n",
    "            test_rets = []\n",
    "            self.agent.reset()\n",
    "            # we plot the smoothed return values\n",
    "            smooth_ep_return = deque(maxlen=10)\n",
    "            ob = self.env.reset()\n",
    "            ret = 0\n",
    "            num_ep = 0\n",
    "            for t in tqdm(range(self.max_steps), desc='Step'):\n",
    "                if len(self.agent.memory) < self.agent.warmup_steps:\n",
    "                    action = self.env.action_space.sample()\n",
    "                else:\n",
    "                    action = self.agent.get_action(ob)\n",
    "                next_ob, reward, done, truncated, info = self.env.step(action)\n",
    "                true_done = done and not info.get('TimeLimit.truncated', False)\n",
    "                self.agent.add_to_memory(ob, next_ob, action, reward, true_done)\n",
    "                self.agent.update_Q()\n",
    "                ret += reward\n",
    "                ob = next_ob\n",
    "                if done or truncated:\n",
    "                    ob = self.env.reset()\n",
    "                    smooth_ep_return.append(ret)\n",
    "                    ep_rewards.append(np.mean(smooth_ep_return))\n",
    "                    ep_steps.append(t)\n",
    "                    ret = 0\n",
    "                    num_ep += 1\n",
    "                    if self.show_progress:\n",
    "                        print(f'Step:{t}  epsilon:{self.agent.epsilon}  '\n",
    "                            f'Smoothed Training Return:{np.mean(smooth_ep_return)}')\n",
    "                    if num_ep % 10 == 0:\n",
    "                        test_ret = self.test()\n",
    "                        test_rets.append(test_ret)\n",
    "                        if self.show_progress:\n",
    "                            print('==========================')\n",
    "                            print(f'Step:{t} Testing Return: {test_ret}')\n",
    "\n",
    "                self.agent.decay_epsilon()\n",
    "                self.agent.update_target_qnet(t, soft=not self.agent.disable_target_net)\n",
    "\n",
    "            rewards.append(ep_rewards)\n",
    "            run_log = pd.DataFrame({'return': ep_rewards,\n",
    "                                    'steps': ep_steps,\n",
    "                                    'episode': np.arange(len(ep_rewards)),\n",
    "                                    'epsilon': self.agent.initial_epsilon})\n",
    "            log.append(run_log)\n",
    "\n",
    "            log_alt = [ep_steps[9::10], test_rets]\n",
    "        return log, log_alt\n",
    "\n",
    "def dqn_sweep(agents, labels, n_runs=1, max_steps=100000, show_progress=False):\n",
    "    \"\"\"\n",
    "    Performs a sweep over different DQN agents to compare their performance.\n",
    "    This function takes a list of DQN agents and their labels, then runs each agent for a specified number of runs and\n",
    "    steps, tracking their performance. Useful for comparing the effects of different hyperparameters or architectures\n",
    "    across multiple agents.\n",
    "\n",
    "    Parameters:\n",
    "    - agents (list): A list of DQNAgent instances to be evaluated.\n",
    "    - labels (list): A list of strings representing labels for each agent, used in the logs to identify the agents.\n",
    "    - n_runs (int): The number of runs to execute for each agent.\n",
    "    - max_steps (int): The maximum number of steps to execute in each run.\n",
    "    - show_progress (bool): Whether to display progress bars during execution.\n",
    "\n",
    "    Returns:\n",
    "    - pandas.DataFrame: A concatenated DataFrame containing the logs of all runs for all agents, with an additional\n",
    "                         'Agent' column indicating the label of the agent for each log entry.\n",
    "    \"\"\"\n",
    "    logs = dict()\n",
    "    for idx, agent in enumerate(tqdm(agents)):\n",
    "        engine = DQNEngine(env=agent.env, agent=agent,\n",
    "                           max_steps=max_steps, show_progress=show_progress)\n",
    "        ep_log, log_alt = engine.run(n_runs)\n",
    "        ep_log = pd.concat(ep_log, ignore_index=True)\n",
    "        ep_log['Agent'] = labels[idx]\n",
    "        logs[f'{idx}'] = ep_log\n",
    "    logs = pd.concat(logs, ignore_index=True)\n",
    "    return logs, log_alt\n",
    "\n",
    "def get_default_config():\n",
    "    config = dict(\n",
    "        learning_rate=0.00025,\n",
    "        gamma=0.99,\n",
    "        memory_size=200000,\n",
    "        initial_epsilon=1.0,\n",
    "        min_epsilon=0.1,\n",
    "        max_epsilon_decay_steps=150000,\n",
    "        warmup_steps=500,\n",
    "        target_update_freq=2000,\n",
    "        batch_size=32,\n",
    "        device=None,\n",
    "        disable_target_net=False,\n",
    "        enable_double_q=True\n",
    "    )\n",
    "    return config\n",
    "\n",
    "class RRR:\n",
    "    def __init__(self, ACModelClass, env, args:Config=None, seed=0):\n",
    "        \"\"\"\n",
    "        The RRR agent.\n",
    "        \"\"\"\n",
    "\n",
    "        self.env = env\n",
    "\n",
    "        set_random_seed(seed)\n",
    "        m1 = ACModelClass(use_critic=True).to(get_device())\n",
    "\n",
    "        if args is None:\n",
    "            args = Config()\n",
    "\n",
    "        self.exploiter = Machine(0.01, m1, args)\n",
    "\n",
    "        m2 = ACModelClass(use_critic=False).to(get_device())\n",
    "        m3 = ACModelClass(use_critic=False).to(get_device())\n",
    "        m4 = ACModelClass(use_critic=False).to(get_device())\n",
    "\n",
    "        self.explorer_random = Machine(0.03, m2, args)\n",
    "        self.explorer_thirsty = Machine(0.02, m3, args)\n",
    "        self.explorer_thirsty.copy_machine(self.exploiter)\n",
    "\n",
    "        self.temp_machine = Machine(0.01, m3, args)\n",
    "        self.explorer_bengbuzhu = Machine(0.5, m4, args)\n",
    "\n",
    "    def _replay(self, machine:Machine, buffer:list, cutoff:float) -> float:\n",
    "        \"\"\"\n",
    "        Replay random sample from buffer on machine.\n",
    "\n",
    "        Args:\n",
    "            machine: Machine to replay on\n",
    "            buffer: list of experiences\n",
    "            cutoff: if fit < cutoff, delete sb from buffer\n",
    "\n",
    "        Returns:\n",
    "            fit\n",
    "        \"\"\"\n",
    "\n",
    "        idx = np.random.randint(len(buffer))\n",
    "        sb = buffer[idx]\n",
    "        logs_replay = machine.update_parameters(sb)\n",
    "        fit = logs_replay['fit']\n",
    "        if fit < cutoff:\n",
    "            # delete sb from buffer\n",
    "            buffer.pop(idx)\n",
    "\n",
    "        return fit\n",
    "\n",
    "    def _add_sb_to_buffer(self, exps, buffer:list, capacity:int) -> None:\n",
    "        buffer.append(exps)\n",
    "        if len(buffer) > capacity:\n",
    "            buffer.pop(0)\n",
    "\n",
    "\n",
    "    def train(self, max_episodes:int=10000, nonstop:bool=False, max_frames=float('inf')) -> tuple[list[int], list[float], list[float]]:\n",
    "        \"\"\"\n",
    "        Train the agent.\n",
    "\n",
    "        Returns:\n",
    "            num_frames, smooth_rs, fits\n",
    "        \"\"\"\n",
    "\n",
    "        print('Start! Agent: RRR.')\n",
    "        RANDOM_MODE = 0\n",
    "        EXPLORE_MODE = 1\n",
    "        EXPLOIT_MODE = 2\n",
    "        mode = RANDOM_MODE\n",
    "\n",
    "        is_solved = False\n",
    "\n",
    "        SMOOTH_REWARD_WINDOW = 50\n",
    "\n",
    "        rewards = [0]*SMOOTH_REWARD_WINDOW\n",
    "\n",
    "        total_smooth_rs = []\n",
    "        total_num_frames = []\n",
    "        larger_buffer_r = []\n",
    "        buffer_r = []\n",
    "        buffer_no_r = []\n",
    "        fits = []\n",
    "\n",
    "        num_frames = 0\n",
    "\n",
    "        pbar = tqdm(range(max_episodes))\n",
    "        for update in pbar:\n",
    "            total_num_frames.append(num_frames)\n",
    "\n",
    "            if mode == RANDOM_MODE:\n",
    "                exps, logs1 = self.explorer_bengbuzhu.collect_experiences(self.env)\n",
    "                self.explorer_bengbuzhu.update_parameters(exps)\n",
    "                logs2 = self.exploiter.update_parameters(exps)\n",
    "\n",
    "            elif mode == EXPLORE_MODE:\n",
    "                if np.random.rand() < 0.5:\n",
    "                    m = self.explorer_random\n",
    "                else:\n",
    "                    m = self.explorer_thirsty\n",
    "\n",
    "                exps, logs1 = m.collect_experiences(self.env)\n",
    "\n",
    "                m.update_parameters(exps)\n",
    "                logs2 = self.exploiter.update_parameters(exps)\n",
    "\n",
    "                if len(larger_buffer_r) >= 1:\n",
    "                    self._replay(self.explorer_thirsty, larger_buffer_r, cutoff=0.0)\n",
    "\n",
    "            elif mode == EXPLOIT_MODE:\n",
    "                exps, logs1 = self.exploiter.collect_experiences(self.env)\n",
    "\n",
    "                logs2 = self.exploiter.update_parameters(exps)\n",
    "\n",
    "                assert len(buffer_r) > 0, f'buffer_r should not be empty.'\n",
    "\n",
    "                cutoff = self.exploiter.args.bad_fit_threshold + self.exploiter.args.bad_fit_increment * (len(buffer_r) - 1)\n",
    "\n",
    "                if not logs1['success'] and total_smooth_rs[-1] <= 0.5:\n",
    "                    fit = self._replay(self.exploiter, buffer_r, cutoff)\n",
    "                    fits.append(fit)\n",
    "\n",
    "                if len(buffer_r) == 0:\n",
    "                    print(f'At episode {update}, we lost all data in replay buffer...')\n",
    "                    mode = EXPLORE_MODE\n",
    "                    self.explorer_thirsty.copy_machine(self.exploiter)\n",
    "                    self.explorer_random.copy_machine(self.temp_machine)\n",
    "                    self.temp_machine.copy_machine(self.exploiter)\n",
    "\n",
    "            else:\n",
    "                raise ValueError(f'Invalid mode: {mode}')\n",
    "\n",
    "            logs = {**logs1, **logs2}\n",
    "\n",
    "            num_frames += logs[\"num_frames\"]\n",
    "\n",
    "            rewards.append(logs[\"return_per_episode\"])\n",
    "\n",
    "            if logs['success']:\n",
    "                if mode == RANDOM_MODE:\n",
    "                    print(f'First successful data collected at episode {update}! We will be accelerating.')\n",
    "                elif mode == EXPLORE_MODE:\n",
    "                    if m is self.explorer_random:\n",
    "                        info_print = 'random explorer'\n",
    "                    elif m is self.explorer_thirsty:\n",
    "                        info_print = 'thirsty explorer'\n",
    "                    else:\n",
    "                        raise ValueError(f'Invalid explorer: {m}')\n",
    "\n",
    "                    print(f'At episode {update}, {info_print} collected a successful data.')\n",
    "                elif mode == EXPLOIT_MODE:\n",
    "                    pass\n",
    "                else:\n",
    "                    raise ValueError(f'Invalid mode: {mode}')\n",
    "                self._add_sb_to_buffer(exps, buffer_r, self.exploiter.args.replay_buffer_capacity)\n",
    "                self._add_sb_to_buffer(exps, larger_buffer_r, self.exploiter.args.large_buffer_capacity)\n",
    "                mode = EXPLOIT_MODE\n",
    "            else:\n",
    "                self._add_sb_to_buffer(exps, buffer_no_r, self.exploiter.args.replay_buffer_capacity)\n",
    "\n",
    "            smooth_reward = np.mean(rewards[-SMOOTH_REWARD_WINDOW:])\n",
    "\n",
    "            total_smooth_rs.append(smooth_reward)\n",
    "\n",
    "            data = {'num_frames':num_frames, 'smooth_reward':smooth_reward,\n",
    "                'reward':logs[\"return_per_episode\"], 'episode':update}\n",
    "\n",
    "            pbar.set_postfix(data)\n",
    "\n",
    "            if not nonstop and smooth_reward >= self.exploiter.args.score_threshold:\n",
    "                    is_solved = True\n",
    "                    break\n",
    "            if num_frames >= max_frames:\n",
    "                break\n",
    "\n",
    "        if is_solved:\n",
    "            print('Solved!')\n",
    "\n",
    "        return total_num_frames, total_smooth_rs, fits\n",
    "\n",
    "class ACModel(nn.Module):\n",
    "    def __init__(self, use_critic=False):\n",
    "        \"\"\"\n",
    "        Represents an Actor Crictic model that takes a 2d, multi-channeled\n",
    "        image as input.\n",
    "\n",
    "        Parameters\n",
    "        ----\n",
    "        num_actions : int\n",
    "\n",
    "                      The action space of the environment.\n",
    "                      The action space for DoorKey5x5 is 7-dimensional:\n",
    "                      0: turn left,\n",
    "                      1: turn right,\n",
    "                      2: forward,\n",
    "                      3: pickup an object,\n",
    "                      4: drop an object,\n",
    "                      5: activate an object,\n",
    "                      6: done completing task\n",
    "\n",
    "        use_critics : bool\n",
    "\n",
    "                      Critic network will be used in forward pass if flag is set\n",
    "                      to true.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.use_critic = use_critic\n",
    "\n",
    "        # Define actor's model\n",
    "        self.image_conv_actor = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, (2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 2)),\n",
    "            nn.Conv2d(16, 32, (2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, (2, 2)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 7)\n",
    "        )\n",
    "\n",
    "        # Define critic's model\n",
    "        if self.use_critic:\n",
    "            self.image_conv_critic = nn.Sequential(\n",
    "                nn.Conv2d(3, 16, (2, 2)),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d((2, 2)),\n",
    "                nn.Conv2d(16, 32, (2, 2)),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(32, 64, (2, 2)),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "            self.critic = nn.Sequential(\n",
    "                nn.Linear(64, 64),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(64, 1)\n",
    "            )\n",
    "\n",
    "        # Initialize parameters correctly\n",
    "        self.apply(init_params)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        \"\"\"\n",
    "        Performs a forward pass through the actor-critic network\n",
    "\n",
    "        Parameters\n",
    "        ----\n",
    "        obs : int tensor. Shape [Batch size, ImWidth, ImHeight, Channels]\n",
    "\n",
    "              input to the network.\n",
    "        ----\n",
    "\n",
    "        returns:\n",
    "\n",
    "        dist : torch.distribution\n",
    "            The distribution of actions from policy. A Categorical distribution\n",
    "            for discreet action spaces.\n",
    "        value : torch.Tensor (Batch size, 1)\n",
    "            value output by critic network\n",
    "        \"\"\"\n",
    "        device = get_device()\n",
    "        obs = torch.tensor(obs).float() # convert to float tensor\n",
    "        obs = obs.to(device)\n",
    "        if len(obs.shape) == 3:\n",
    "            obs = obs.unsqueeze(0) # add batch dimension if not already there\n",
    "\n",
    "        conv_in = obs.transpose(1, 3).transpose(2, 3) # reshape into [b, c, h, w]\n",
    "\n",
    "        dist, value = None, None\n",
    "\n",
    "        x = self.image_conv_actor(conv_in)\n",
    "        embedding = x.reshape(x.shape[0], -1)\n",
    "\n",
    "        x = self.actor(embedding)\n",
    "        dist = Categorical(logits=F.log_softmax(x, dim=1))\n",
    "\n",
    "        if self.use_critic:\n",
    "            y = self.image_conv_critic(conv_in)\n",
    "            embedding = y.reshape(y.shape[0], -1)\n",
    "\n",
    "            value = self.critic(embedding).squeeze(1)\n",
    "        else:\n",
    "            value = torch.zeros((x.shape[0], 1), device=x.device)\n",
    "\n",
    "        return dist, value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "size = 6\n",
    "seed = 62\n",
    "\n",
    "env = get_door_key_env(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f3aa41c3d2144cc8eaf0de739851d18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2841aa6318f94c848380c6da1706d355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Runs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "033b5172db1a43709575cdc007413e9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step:   0%|          | 0/150000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwarmup_steps\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m\n\u001b[1;32m     11\u001b[0m agent \u001b[38;5;241m=\u001b[39m DQNAgent(env\u001b[38;5;241m=\u001b[39menv, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig)\n\u001b[0;32m---> 13\u001b[0m dqn_logs, log_alt \u001b[38;5;241m=\u001b[39m \u001b[43mdqn_sweep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43magent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDDQN\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_frames\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m num_frames_DDQN \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(log_alt[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     16\u001b[0m rewards_DDQN \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(log_alt[\u001b[38;5;241m1\u001b[39m])\n",
      "Cell \u001b[0;32mIn[24], line 459\u001b[0m, in \u001b[0;36mdqn_sweep\u001b[0;34m(agents, labels, n_runs, max_steps, show_progress)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, agent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(agents)):\n\u001b[1;32m    457\u001b[0m     engine \u001b[38;5;241m=\u001b[39m DQNEngine(env\u001b[38;5;241m=\u001b[39magent\u001b[38;5;241m.\u001b[39menv, agent\u001b[38;5;241m=\u001b[39magent,\n\u001b[1;32m    458\u001b[0m                        max_steps\u001b[38;5;241m=\u001b[39mmax_steps, show_progress\u001b[38;5;241m=\u001b[39mshow_progress)\n\u001b[0;32m--> 459\u001b[0m     ep_log, log_alt \u001b[38;5;241m=\u001b[39m \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_runs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m     ep_log \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(ep_log, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    461\u001b[0m     ep_log[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAgent\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m labels[idx]\n",
      "Cell \u001b[0;32mIn[24], line 404\u001b[0m, in \u001b[0;36mDQNEngine.run\u001b[0;34m(self, n_runs)\u001b[0m\n\u001b[1;32m    402\u001b[0m true_done \u001b[38;5;241m=\u001b[39m done \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTimeLimit.truncated\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39madd_to_memory(ob, next_ob, action, reward, true_done)\n\u001b[0;32m--> 404\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_Q\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m ret \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m    406\u001b[0m ob \u001b[38;5;241m=\u001b[39m next_ob\n",
      "Cell \u001b[0;32mIn[24], line 257\u001b[0m, in \u001b[0;36mDQNAgent.update_Q\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m rewards \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(rewards, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    255\u001b[0m dones \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(dones, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint32)\n\u001b[0;32m--> 257\u001b[0m q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# then select the index from q_values according to actions\u001b[39;00m\n\u001b[1;32m    259\u001b[0m q_values \u001b[38;5;241m=\u001b[39m q_values\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m1\u001b[39m, actions\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/leg/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/leg/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[24], line 39\u001b[0m, in \u001b[0;36mDQNetwork.forward\u001b[0;34m(self, ob)\u001b[0m\n\u001b[1;32m     37\u001b[0m ob \u001b[38;5;241m=\u001b[39m ob\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     38\u001b[0m bs \u001b[38;5;241m=\u001b[39m ob\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 39\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mob\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mview(bs, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     41\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfcs(out)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/leg/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/leg/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/leg/lib/python3.10/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/leg/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/leg/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/leg/lib/python3.10/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/leg/lib/python3.10/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_frames = 150000\n",
    "max_episodes = 4000000\n",
    "nonstop = True\n",
    "set_random_seed(seed)\n",
    "\n",
    "config = get_default_config()\n",
    "config['batch_size'] = 128\n",
    "config['max_epsilon_decay_steps'] = int(1.5 * max_frames)\n",
    "config['memory_size'] = int(2 * max_frames)\n",
    "config['warmup_steps'] = 500\n",
    "agent = DQNAgent(env=env, **config)\n",
    "\n",
    "dqn_logs, log_alt = dqn_sweep([agent], ['DDQN'], max_steps=max_frames)\n",
    "\n",
    "num_frames_DDQN = list(log_alt[0])\n",
    "rewards_DDQN = list(log_alt[1])\n",
    "length = min(len(num_frames_DDQN), len(rewards_DDQN))\n",
    "num_frames_DDQN = num_frames_DDQN[:length]\n",
    "rewards_DDQN = rewards_DDQN[:length]\n",
    "# first prepend 49 zeros to the rewards to make the plot start at 0\n",
    "rewards_DDQN = [0] * 49 + rewards_DDQN\n",
    "# smooth the rewards by averaging on 50 of them\n",
    "rewards_DDQN = [sum(rewards_DDQN[i:i+50])/50 for i in range(len(rewards_DDQN)-49)]\n",
    "\n",
    "agent_PPO = PPO(ACModel, env=env, args=Config(), seed=seed)\n",
    "num_frames_1, smooth_rs_1 = agent_PPO.train(max_episodes, nonstop=nonstop, max_frames=max_frames)\n",
    "agent_RRR_2 = RRR(ACModel, env=env, args=Config(bad_fit_threshold=0.77, importance_sampling_clip=2.0), seed=seed)\n",
    "num_frames_2, smooth_rs_2, fits_2 = agent_RRR_2.train(max_episodes, nonstop=nonstop, max_frames=max_frames)\n",
    "\n",
    "plt.plot(num_frames_1, smooth_rs_1, label='PPO')\n",
    "plt.plot(num_frames_DDQN, rewards_DDQN, label='DDQN')\n",
    "plt.plot(num_frames_2, smooth_rs_2, label='RRR')\n",
    "plt.xlabel('Time steps')\n",
    "plt.ylabel('Average reward (smoothed)')\n",
    "plt.title(f'DoorKeyEnv, size = {size}, seed = {seed}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
