{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "import torch.nn.functional as F\n",
    "import minigrid\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from tqdm.notebook import tqdm\n",
    "from minigrid.envs.doorkey import DoorKeyEnv\n",
    "import pandas as pd\n",
    "from gym.envs.registration import registry, register\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from typing_extensions import Self\n",
    "import matplotlib.pyplot as plt\n",
    "from minigrid.wrappers import ObservationWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device() -> torch.device:\n",
    "    \"\"\"\n",
    "    Returns the device to use for training.\n",
    "    \"\"\"\n",
    "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def set_random_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function from https://github.com/ikostrikov/pytorch-a2c-ppo-acktr/blob/master/model.py\n",
    "def init_params(m):\n",
    "    \"\"\"\n",
    "    Initialize parameters of the network.\n",
    "    m: torch.nn.Module\n",
    "    \"\"\"\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Linear\") != -1:\n",
    "        m.weight.data.normal_(0, 1)\n",
    "        m.weight.data *= 1 / torch.sqrt(m.weight.data.pow(2).sum(1, keepdim=True))\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0)\n",
    "\n",
    "class MyDoorKeyEnv(DoorKeyEnv):\n",
    "    def __init__(self, size):\n",
    "        self.render_mode = \"rgb_array\"\n",
    "        super().__init__(size=size)\n",
    "\n",
    "    def _reward(self):\n",
    "        \"\"\"\n",
    "        Compute the reward to be given upon success\n",
    "        \"\"\"\n",
    "        return 1\n",
    "\n",
    "class ImgObsWrapper(ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Use the image as the only observation output, no language/mission.\n",
    "\n",
    "    Parameters:\n",
    "    - env (gym.Env): The environment to wrap.\n",
    "\n",
    "    Methods:\n",
    "    - observation(self, obs): Returns the image from the observation.\n",
    "    - reset(self): Resets the environment and returns the initial observation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        \"\"\"\n",
    "        Initializes the ImgObsWrapper with the given environment.\n",
    "\n",
    "        Parameters:\n",
    "        - env (gym.Env): The environment whose observations are to be wrapped.\n",
    "        \"\"\"\n",
    "        super().__init__(env)\n",
    "        self.observation_space = env.observation_space.spaces[\"image\"]\n",
    "\n",
    "    def observation(self, obs):\n",
    "        \"\"\"\n",
    "        Extracts and returns the image data from the observation.\n",
    "\n",
    "        Parameters:\n",
    "        - obs (dict or tuple): The original observation from the environment, which could be either\n",
    "        a dictionary or a tuple containing a dictionary.\n",
    "\n",
    "        Returns:\n",
    "        - np.ndarray: The image data extracted from the observation.\n",
    "        \"\"\"\n",
    "        if type(obs) == tuple:\n",
    "            return obs[0][\"image\"]\n",
    "        return obs[\"image\"]\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the environment and returns the initial observation image.\n",
    "\n",
    "        Returns:\n",
    "        - np.ndarray: The initial observation image of the reset environment.\n",
    "        \"\"\"\n",
    "        obs = super().reset()\n",
    "        return obs[0]\n",
    "    \n",
    "def get_door_key_env(size):\n",
    "    \"\"\"\n",
    "    Returns a DoorKeyEnv environment with the given size.\n",
    "    \"\"\"\n",
    "    env = MyDoorKeyEnv(size=size)\n",
    "    env = ImgObsWrapper(env)\n",
    "\n",
    "    env.reset()\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    # get an RGB image corresponding to the whole environment or the agent's point of view (https://github.com/Farama-Foundation/Minigrid/blob/master/minigrid/minigrid_env.py#L716)\n",
    "    #            highlight (bool): If true, the agent's field of view or point of view is highlighted with a lighter gray color.\n",
    "    #            tile_size (int): How many pixels will form a tile from the NxM grid.\n",
    "    #            agent_pov (bool): If true, the rendered frame will only contain the point of view of the agent.\n",
    "    frame = env.get_frame(highlight=env.highlight, tile_size=env.tile_size, agent_pov=env.agent_pov)\n",
    "    # show an image to the notebook.\n",
    "    plt.imshow(frame)\n",
    "\n",
    "    return env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"\n",
    "    Stores algorithmic hyperparameters.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                score_threshold=0.93,\n",
    "                discount=0.995,\n",
    "                lr=1e-3,\n",
    "                max_grad_norm=0.5,\n",
    "                log_interval=10,\n",
    "                gae_lambda=0.95,\n",
    "                clip_ratio=0.2,\n",
    "                target_kl=0.01,\n",
    "                train_ac_iters=5,\n",
    "                use_discounted_reward=True,\n",
    "                use_gae=True,\n",
    "                importance_sampling_clip = 2.0,\n",
    "                bad_fit_threshold = 0.8,\n",
    "                bad_fit_increment = 0.02,\n",
    "                replay_buffer_capacity = 10,\n",
    "                large_buffer_capacity = 20):\n",
    "\n",
    "        self.score_threshold = score_threshold # criterion for early stopping. If the rolling average reward (over the last 100 episodes) is greater than it, it ends.\n",
    "        self.discount = discount # discount factor\n",
    "        self.lr = lr # learning rate\n",
    "        self.max_grad_norm = max_grad_norm # the maximum gradient norm (https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html)\n",
    "        self.log_interval = log_interval # logging interval\n",
    "        self.clip_ratio = clip_ratio # clip_ratio of PPO.\n",
    "        self.target_kl = target_kl # target KL divergence for early stoping train_ac_iters for PPO\n",
    "        self.train_ac_iters = train_ac_iters # how many time to train ac_model using current computed old_logps\n",
    "        self.gae_lambda=gae_lambda # lambda in Generalized Advantage Estimation (GAE)\n",
    "        self.use_discounted_reward=use_discounted_reward # whether use discounted reward or not.\n",
    "        self.use_gae = use_gae # whether to use GAE or not.\n",
    "        self.importance_sampling_clip = importance_sampling_clip # importance sampling clip threshold\n",
    "        self.bad_fit_threshold = bad_fit_threshold # threshold for bad fit.\n",
    "        self.bad_fit_increment = bad_fit_increment # increment for each sb in replay buffer.\n",
    "        self.replay_buffer_capacity = replay_buffer_capacity # capacity of replay buffer.\n",
    "        self.large_buffer_capacity = large_buffer_capacity # capacity of large replay buffer.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        return None\n",
    "    \n",
    "    def forward(self, obs):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def copy_model(self, other:Self) -> None:\n",
    "        \"\"\"\n",
    "        Copy the parameters of another model to this model.\n",
    "        \"\"\"\n",
    "        state_dict = other.state_dict()\n",
    "        for key, v in state_dict.items():\n",
    "            if key in self.state_dict():\n",
    "                self.state_dict()[key].copy_(v)\n",
    "\n",
    "    def _init_params(self):\n",
    "        \"\"\"\n",
    "        Initialize parameters of the network.\n",
    "        m: torch.nn.Module\n",
    "        \"\"\"\n",
    "        classname = self.__class__.__name__\n",
    "        if classname.find(\"Linear\") != -1:\n",
    "            self.weight.data.normal_(0, 1)\n",
    "            self.weight.data *= 1 / torch.sqrt(self.weight.data.pow(2).sum(1, keepdim=True))\n",
    "            if self.bias is not None:\n",
    "                self.bias.data.fill_(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Machine:\n",
    "    def __init__(self, entropy_coef, init_model:Model, args:Config=None):\n",
    "        \"\"\"\n",
    "        A Machine object consists of a Model and its entropy_coef\n",
    "\n",
    "        Args:\n",
    "            entropy_coef: Entropy coefficient.\n",
    "            init_model: Initial model.\n",
    "            args\n",
    "        \"\"\"\n",
    "        if args is None:\n",
    "            self.args = Config()\n",
    "        else:\n",
    "            self.args = args\n",
    "\n",
    "        self.model = init_model\n",
    "        self.optim = torch.optim.Adam(self.model.parameters(), lr=self.args.lr)\n",
    "        self.coef = entropy_coef\n",
    "\n",
    "    def copy_model(self, model:Model) -> None:\n",
    "        \"\"\"\n",
    "        Copy state dict from 'model'. Reset rs.\n",
    "        \"\"\"\n",
    "        self.model.copy_model(model)\n",
    "\n",
    "    def copy_machine(self, other:Self) -> None:\n",
    "        \"\"\"\n",
    "        Copy state dict from 'other'. Reset rs.\n",
    "        \"\"\"\n",
    "        self.copy_model(other.model)\n",
    "\n",
    "    def _compute_discounted_return(rewards, discount, device=None):\n",
    "        \"\"\"\n",
    "            rewards: reward obtained at timestep.  Shape: (T,)\n",
    "            discount: discount factor. float\n",
    "\n",
    "        ----\n",
    "        returns: sum of discounted rewards. Shape: (T,)\n",
    "        \"\"\"\n",
    "        if device is None:\n",
    "            device = get_device()\n",
    "            \n",
    "        returns = torch.zeros(*rewards.shape, device=device)\n",
    "\n",
    "        R = 0\n",
    "        for t in reversed(range((rewards.shape[0]))):\n",
    "            R = rewards[t] + discount * R\n",
    "            returns[t] = R\n",
    "        return returns\n",
    "\n",
    "    def _compute_advantage_gae(values, rewards, T, gae_lambda, discount):\n",
    "        \"\"\"\n",
    "        Compute Adavantage wiht GAE. See Section 4.4.2 in the lecture notes.\n",
    "\n",
    "        values: value at each timestep (T,)\n",
    "        rewards: reward obtained at each timestep.  Shape: (T,)\n",
    "        T: the number of frames, float\n",
    "        gae_lambda: hyperparameter, float\n",
    "        discount: discount factor, float\n",
    "\n",
    "        -----\n",
    "\n",
    "        returns:\n",
    "\n",
    "        advantages : tensor.float. Shape [T,]\n",
    "\n",
    "                    gae advantage term for timesteps 0 to T\n",
    "\n",
    "        \"\"\"\n",
    "        advantages = torch.zeros_like(values)\n",
    "        for i in reversed(range(T)):\n",
    "            next_value = values[i+1]\n",
    "            next_advantage = advantages[i+1]\n",
    "\n",
    "            delta = rewards[i] + discount * next_value  - values[i]\n",
    "            advantages[i] = delta + discount * gae_lambda * next_advantage\n",
    "        return advantages[:T]\n",
    "    \n",
    "    def collect_experiences(self, env:gym.Env):\n",
    "        \"\"\"\n",
    "        Collects rollouts and computes advantages.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        exps : dict\n",
    "            Contains actions, rewards, advantages etc as attributes.\n",
    "            Each attribute, e.g. `exps['reward']` has a shape\n",
    "            (self.num_frames, ...).\n",
    "        logs : dict\n",
    "            Useful stats about the training process, including the average\n",
    "            reward, policy loss, value loss, etc.\n",
    "        \"\"\"\n",
    "        device = get_device()\n",
    "\n",
    "        MAX_FRAMES_PER_EP = 300\n",
    "        shape = (MAX_FRAMES_PER_EP, )\n",
    "\n",
    "        actions = torch.zeros(*shape, device=device, dtype=torch.int)\n",
    "        values = torch.zeros(*shape, device=device)\n",
    "        rewards = torch.zeros(*shape, device=device)\n",
    "        log_probs = torch.zeros(*shape, device=device)\n",
    "        obss = [None]*MAX_FRAMES_PER_EP\n",
    "\n",
    "        obs, _ = env.reset()\n",
    "\n",
    "        total_return = 0\n",
    "\n",
    "        T = 0\n",
    "\n",
    "        while True:\n",
    "            # Do one agent-environment interaction\n",
    "            with torch.no_grad():\n",
    "                dist, value = self.model(obs)\n",
    "\n",
    "            dist: Categorical\n",
    "            action = dist.sample()[0]\n",
    "\n",
    "            obss[T] = obs\n",
    "            obs, reward, done, _, _ = env.step(action.item())\n",
    "\n",
    "            # Update experiences values\n",
    "            actions[T] = action\n",
    "            values[T] = value\n",
    "            rewards[T] = reward\n",
    "            log_probs[T] = dist.log_prob(action)\n",
    "\n",
    "            total_return += reward\n",
    "            T += 1\n",
    "\n",
    "            if done or T >= MAX_FRAMES_PER_EP-1:\n",
    "                break\n",
    "\n",
    "        discounted_reward = self._compute_discounted_return(rewards[:T], self.args.discount, device)\n",
    "        exps = dict(\n",
    "            obs = torch.tensor(obss[:T], device=device),\n",
    "            action = actions[:T],\n",
    "            value  = values[:T],\n",
    "            reward = rewards[:T],\n",
    "            log_prob = log_probs[:T],\n",
    "            discounted_reward = discounted_reward,\n",
    "            T = T\n",
    "        )\n",
    "\n",
    "        logs = {\n",
    "            \"return_per_episode\": total_return,\n",
    "            \"num_frames\": T\n",
    "        }\n",
    "\n",
    "        return exps, logs\n",
    "\n",
    "    def _compute_policy_loss_ppo(self, dist:Categorical, factors, indices, old_logp, actions, advantages):\n",
    "        \"\"\"\n",
    "        Computes the policy loss for PPO.\n",
    "\n",
    "        obs: observeration to pass into acmodel. shape: (T,)\n",
    "        init_logp: log probabilities we get from the agent performing the action. shape: (T,)\n",
    "        old_logp: log probabilities from previous timestep. shape: (T,)\n",
    "        actions: action at this timestep. shape: (T,ImWidth,ImHeight,Channels)\n",
    "        advantages: the computed advantages. shape: (T,)\n",
    "\n",
    "        ---\n",
    "        returns\n",
    "\n",
    "        policy_loss : ppo policy loss as shown in line 6 of PPO alg. tensor.float. Shape (,1)\n",
    "        approx_kl: an appoximation of the kl_divergence. tensor.float. Shape (,1)\n",
    "        \"\"\"\n",
    "        policy_loss, approx_kl = 0, 0\n",
    "\n",
    "        coef = self.coef\n",
    "\n",
    "        entropy = dist.entropy()\n",
    "        logps = dist.log_prob(actions)\n",
    "        r_terms = torch.exp(logps - old_logp)\n",
    "        ppo_loss = torch.min(r_terms * advantages, torch.clamp(r_terms, 1 - self.args.clip_ratio, 1 + self.args.clip_ratio) * advantages)\n",
    "        \n",
    "        policy_loss_tensor = factors * ppo_loss + coef * entropy\n",
    "\n",
    "        policy_loss = - torch.mean(policy_loss_tensor[indices])\n",
    "\n",
    "        # approx_kl = torch.sum(torch.exp(old_logp) * (old_logp - logps)) / torch.sum(torch.exp(old_logp))\n",
    "        approx_kl = torch.mean((old_logp - logps) ** 2) / 2\n",
    "\n",
    "        return policy_loss, approx_kl\n",
    "    \n",
    "    def _compute_value_loss(self, values, returns):\n",
    "        value_loss = torch.mean((values - returns) ** 2)\n",
    "\n",
    "        return value_loss\n",
    "\n",
    "    def update_parameters(self, sb, update_v=True):\n",
    "        MAX_FRAMES_PER_EP = 300\n",
    "        T = sb['T']\n",
    "        with torch.no_grad():\n",
    "            dist, values = self.model(sb['obs'])\n",
    "        values = values.reshape(-1)\n",
    "        dist: Categorical\n",
    "        old_logp = dist.log_prob(sb['action'])\n",
    "        init_logp = sb['log_prob']\n",
    "\n",
    "        # add 0 to end of values until it has length MAX_FRAMES_PER_EP\n",
    "        values_extended = torch.cat([values, torch.zeros((MAX_FRAMES_PER_EP - len(values), ), device=get_device())], dim=0)\n",
    "        full_reward = torch.cat([sb['reward'], torch.zeros((MAX_FRAMES_PER_EP - len(sb['reward']), ), device=get_device())], dim=0)\n",
    "\n",
    "        if self.args.use_gae:\n",
    "            advantage = self._compute_advantage_gae(values_extended, full_reward, T, self.args.gae_lambda, self.args.discount)\n",
    "        else:\n",
    "            advantage = sb['discounted_reward'] - values.reshape(-1)\n",
    "\n",
    "        for i in range(self.args.train_ac_iters):\n",
    "            self.optim.zero_grad()\n",
    "            dist, values = self.model(sb['obs'])\n",
    "            values = values.reshape(-1)\n",
    "            # policy loss\n",
    "            factors = torch.exp(old_logp - init_logp)\n",
    "            indices = factors < self.args.importance_sampling_clip\n",
    "            fit = torch.mean(indices.to(torch.float32))\n",
    "\n",
    "            loss_pi, approx_kl = self._compute_policy_loss_ppo(dist, factors, indices, old_logp, sb['action'], advantage)\n",
    "            if update_v:\n",
    "                loss_v = self._compute_value_loss(values, sb['discounted_reward'])\n",
    "            else:\n",
    "                loss_v = 0.0\n",
    "\n",
    "            if i == 0:\n",
    "                policy_loss = loss_pi\n",
    "                value_loss = loss_v\n",
    "\n",
    "            loss = loss_v + loss_pi\n",
    "            if approx_kl > 1.5 * self.args.target_kl:\n",
    "                break\n",
    "\n",
    "            loss.backward(retain_graph=True)\n",
    "            self.optim.step()\n",
    "\n",
    "        update_policy_loss = policy_loss.item()\n",
    "        update_value_loss = value_loss.item()\n",
    "\n",
    "        logs = {\n",
    "            \"policy_loss\": update_policy_loss,\n",
    "            \"value_loss\": update_value_loss,\n",
    "            \"fit\": fit.item()\n",
    "        }\n",
    "\n",
    "        return logs\n",
    "    \n",
    "    def decrease_prob(self, sb, lr=0.1) -> None:\n",
    "        self.optim.zero_grad()\n",
    "\n",
    "        dist, _ = self.model(sb['obs'])\n",
    "        dist: Categorical\n",
    "        logps = dist.log_prob(sb['action'])\n",
    "\n",
    "        loss = lr * torch.mean(logps)\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self, ACModelClass, env, args:Config=None, seed=0):\n",
    "        self.env = env\n",
    "\n",
    "        set_random_seed(seed)\n",
    "        model = ACModelClass(use_critic=True)\n",
    "\n",
    "        if args is None:\n",
    "            args = Config()\n",
    "        self.machine = Machine(0.01, model, args)\n",
    "    \n",
    "    def train(self, max_episodes:int=10000, nonstop:bool=False) -> tuple[list[int], list[int]]:\n",
    "        \"\"\"\n",
    "        Train the PPO agent.\n",
    "\n",
    "        Returns:\n",
    "            smooth_rs, num_frames\n",
    "        \"\"\"\n",
    "        print(f'Start! Agent: PPO.')\n",
    "\n",
    "        is_solved = False\n",
    "\n",
    "        SMOOTH_REWARD_WINDOW = 50\n",
    "\n",
    "        rewards = [0]*SMOOTH_REWARD_WINDOW\n",
    "\n",
    "        total_smooth_rs = []\n",
    "        total_num_frames = []\n",
    "\n",
    "        num_frames = 0\n",
    "\n",
    "        pbar = tqdm(range(max_episodes))\n",
    "        for update in pbar:\n",
    "\n",
    "            exps, logs1 = self.machine.collect_experiences(self.env)\n",
    "\n",
    "            logs2 = self.machine.update_parameters(exps)\n",
    "\n",
    "            logs = {**logs1, **logs2}\n",
    "\n",
    "            total_num_frames.append(num_frames)\n",
    "            num_frames += logs[\"num_frames\"]\n",
    "\n",
    "            rewards.append(logs[\"return_per_episode\"])\n",
    "\n",
    "            smooth_reward = np.mean(rewards[-SMOOTH_REWARD_WINDOW:])\n",
    "\n",
    "            total_smooth_rs.append(smooth_reward)\n",
    "\n",
    "            data = {'num_frames':num_frames, 'smooth_reward':smooth_reward,\n",
    "                'reward':logs[\"return_per_episode\"], 'policy_loss':logs[\"policy_loss\"], 'value_loss': logs['value_loss'], 'episode':update}\n",
    "\n",
    "            pbar.set_postfix(data)\n",
    "\n",
    "            if not nonstop and smooth_reward >= self.machine.args.score_threshold:\n",
    "                is_solved = True\n",
    "                break\n",
    "\n",
    "        if not nonstop and is_solved:\n",
    "            print('Solved!')\n",
    "\n",
    "        return total_smooth_rs, total_num_frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACModel(Model):\n",
    "    def __init__(self, use_critic=False):\n",
    "        \"\"\"\n",
    "        Represents an Actor Crictic model that takes a 2d, multi-channeled\n",
    "        image as input.\n",
    "\n",
    "        Parameters\n",
    "        ----\n",
    "        num_actions : int\n",
    "\n",
    "                      The action space of the environment.\n",
    "                      The action space for DoorKey5x5 is 7-dimensional:\n",
    "                      0: turn left,\n",
    "                      1: turn right,\n",
    "                      2: forward,\n",
    "                      3: pickup an object,\n",
    "                      4: drop an object,\n",
    "                      5: activate an object,\n",
    "                      6: done completing task\n",
    "\n",
    "        use_critics : bool\n",
    "\n",
    "                      Critic network will be used in forward pass if flag is set\n",
    "                      to true.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.use_critic = use_critic\n",
    "\n",
    "        # Define actor's model\n",
    "        self.image_conv_actor = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, (2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 2)),\n",
    "            nn.Conv2d(16, 32, (2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, (2, 2)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 7)\n",
    "        )\n",
    "\n",
    "        # Define critic's model\n",
    "        if self.use_critic:\n",
    "            self.image_conv_critic = nn.Sequential(\n",
    "                nn.Conv2d(3, 16, (2, 2)),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d((2, 2)),\n",
    "                nn.Conv2d(16, 32, (2, 2)),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(32, 64, (2, 2)),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "            self.critic = nn.Sequential(\n",
    "                nn.Linear(64, 64),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(64, 1)\n",
    "            )\n",
    "\n",
    "        # Initialize parameters correctly\n",
    "        self.apply(self._init_params)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        \"\"\"\n",
    "        Performs a forward pass through the actor-critic network\n",
    "\n",
    "        Parameters\n",
    "        ----\n",
    "        obs : int tensor. Shape [Batch size, ImWidth, ImHeight, Channels]\n",
    "\n",
    "              input to the network.\n",
    "        ----\n",
    "\n",
    "        returns:\n",
    "\n",
    "        dist : torch.distribution\n",
    "            The distribution of actions from policy. A Categorical distribution\n",
    "            for discreet action spaces.\n",
    "        value : torch.Tensor (Batch size, 1)\n",
    "            value output by critic network\n",
    "        \"\"\"\n",
    "        conv_in = obs.transpose(1, 3).transpose(2, 3) # reshape into [b, c, h, w]\n",
    "\n",
    "        dist, value = None, None\n",
    "\n",
    "        x = self.image_conv_actor(conv_in)\n",
    "        embedding = x.reshape(x.shape[0], -1)\n",
    "\n",
    "        x = self.actor(embedding)\n",
    "        dist = Categorical(logits=F.log_softmax(x, dim=1))\n",
    "\n",
    "        if self.use_critic:\n",
    "            y = self.image_conv_critic(conv_in)\n",
    "            embedding = y.reshape(y.shape[0], -1)\n",
    "\n",
    "            value = self.critic(embedding).squeeze(1)\n",
    "        else:\n",
    "            value = torch.zeros((x.shape[0], 1), device=x.device)\n",
    "        \n",
    "        return dist, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/leg/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.get_frame to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.get_frame` for environment variables or `env.get_wrapper_attr('get_frame')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/leg/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.highlight to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.highlight` for environment variables or `env.get_wrapper_attr('highlight')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/leg/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.tile_size to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.tile_size` for environment variables or `env.get_wrapper_attr('tile_size')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/leg/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.agent_pov to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.agent_pov` for environment variables or `env.get_wrapper_attr('agent_pov')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = get_door_key_env(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m PPO_agent \u001b[38;5;241m=\u001b[39m \u001b[43mPPO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mACModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m smooth_rs, num_frames \u001b[38;5;241m=\u001b[39m PPO_agent\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(num_frames, smooth_rs)\n",
      "Cell \u001b[0;32mIn[26], line 6\u001b[0m, in \u001b[0;36mPPO.__init__\u001b[0;34m(self, ACModelClass, env, args, seed)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;241m=\u001b[39m env\n\u001b[1;32m      5\u001b[0m set_random_seed(seed)\n\u001b[0;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mACModelClass\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_critic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      9\u001b[0m     args \u001b[38;5;241m=\u001b[39m Config()\n",
      "Cell \u001b[0;32mIn[30], line 26\u001b[0m, in \u001b[0;36mACModel.__init__\u001b[0;34m(self, use_critic)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, use_critic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m      3\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    Represents an Actor Crictic model that takes a 2d, multi-channeled\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m    image as input.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03m                  to true.\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_critic \u001b[38;5;241m=\u001b[39m use_critic\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# Define actor's model\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m, in \u001b[0;36mModel.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "PPO_agent = PPO(ACModel, env=env, args=Config(), seed=0)\n",
    "smooth_rs, num_frames = PPO_agent.train()\n",
    "\n",
    "plt.plot(num_frames, smooth_rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
