{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/atticusw/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/Users/atticusw/opt/anaconda3/lib/python3.9/site-packages/gym/envs/registration.py:307: DeprecationWarning: The package name gym_minigrid has been deprecated in favor of minigrid. Please uninstall gym_minigrid and install minigrid with `pip install minigrid`. Future releases will be maintained under the new package name minigrid.\n",
      "  fn()\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "import gym\n",
    "import minigrid\n",
    "from minigrid.envs.doorkey import DoorKeyEnv\n",
    "from minigrid.envs.crossing import CrossingEnv\n",
    "from minigrid.core.world_object import Goal, Lava\n",
    "from minigrid.wrappers import ObservationWrapper\n",
    "from gym.envs.registration import registry, register\n",
    "from typing_extensions import Self\n",
    "from dataclasses import dataclass\n",
    "from typing import Any\n",
    "from copy import deepcopy\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"\n",
    "    Stores algorithmic hyperparameters.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                score_threshold=0.93,\n",
    "                discount=0.995,\n",
    "                lr=1e-3,\n",
    "                max_grad_norm=0.5,\n",
    "                log_interval=10,\n",
    "                gae_lambda=0.95,\n",
    "                clip_ratio=0.2,\n",
    "                target_kl=0.01,\n",
    "                train_ac_iters=5,\n",
    "                use_discounted_reward=True,\n",
    "                use_gae=True,\n",
    "                importance_sampling_clip=2.0,\n",
    "                bad_fit_threshold=0.8,\n",
    "                bad_fit_increment=None,\n",
    "                replay_buffer_capacity=10,\n",
    "                large_buffer_capacity=20):\n",
    "\n",
    "        self.score_threshold = score_threshold # criterion for early stopping. If the rolling average reward (over the last 100 episodes) is greater than it, it ends.\n",
    "        self.discount = discount # discount factor\n",
    "        self.lr = lr # learning rate\n",
    "        self.max_grad_norm = max_grad_norm # the maximum gradient norm (https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html)\n",
    "        self.log_interval = log_interval # logging interval\n",
    "        self.clip_ratio = clip_ratio # clip_ratio of PPO.\n",
    "        self.target_kl = target_kl # target KL divergence for early stoping train_ac_iters for PPO\n",
    "        self.train_ac_iters = train_ac_iters # how many time to train ac_model using current computed old_logps\n",
    "        self.gae_lambda=gae_lambda # lambda in Generalized Advantage Estimation (GAE)\n",
    "        self.use_discounted_reward=use_discounted_reward # whether use discounted reward or not.\n",
    "        self.use_gae = use_gae # whether to use GAE or not.\n",
    "        self.importance_sampling_clip = importance_sampling_clip # importance sampling clip threshold\n",
    "        self.bad_fit_threshold = bad_fit_threshold # threshold for bad fit.\n",
    "        if bad_fit_increment is None:\n",
    "            bad_fit_increment = (1.0 - bad_fit_threshold) / replay_buffer_capacity\n",
    "        self.bad_fit_increment = bad_fit_increment # increment for bad fit.\n",
    "        self.replay_buffer_capacity = replay_buffer_capacity # capacity of replay buffer.\n",
    "        self.large_buffer_capacity = large_buffer_capacity # capacity of large replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Auxiliary functions\n",
    "\n",
    "def get_device() -> torch.device:\n",
    "    \"\"\"\n",
    "    Returns the device to use for training.\n",
    "    \"\"\"\n",
    "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Device:\", get_device())\n",
    "\n",
    "def set_random_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "# Function from https://github.com/ikostrikov/pytorch-a2c-ppo-acktr/blob/master/model.py\n",
    "def init_params(m):\n",
    "    \"\"\"\n",
    "    Initialize parameters of the network.\n",
    "    m: torch.nn.Module\n",
    "    \"\"\"\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Linear\") != -1:\n",
    "        m.weight.data.normal_(0, 1)\n",
    "        m.weight.data *= 1 / torch.sqrt(m.weight.data.pow(2).sum(1, keepdim=True))\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0)\n",
    "\n",
    "class MyDoorKeyEnv(DoorKeyEnv):\n",
    "    def __init__(self, size):\n",
    "        self.render_mode = \"rgb_array\"\n",
    "        super().__init__(size=size)\n",
    "\n",
    "    def _reward(self):\n",
    "        \"\"\"\n",
    "        Compute the reward to be given upon success\n",
    "        \"\"\"\n",
    "        return 1\n",
    "\n",
    "class ImgObsWrapper(ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Use the image as the only observation output, no language/mission.\n",
    "\n",
    "    Parameters:\n",
    "    - env (gym.Env): The environment to wrap.\n",
    "\n",
    "    Methods:\n",
    "    - observation(self, obs): Returns the image from the observation.\n",
    "    - reset(self): Resets the environment and returns the initial observation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        \"\"\"\n",
    "        Initializes the ImgObsWrapper with the given environment.\n",
    "\n",
    "        Parameters:\n",
    "        - env (gym.Env): The environment whose observations are to be wrapped.\n",
    "        \"\"\"\n",
    "        super().__init__(env)\n",
    "        self.observation_space = env.observation_space.spaces[\"image\"]\n",
    "\n",
    "    def observation(self, obs):\n",
    "        \"\"\"\n",
    "        Extracts and returns the image data from the observation.\n",
    "\n",
    "        Parameters:\n",
    "        - obs (dict or tuple): The original observation from the environment, which could be either\n",
    "        a dictionary or a tuple containing a dictionary.\n",
    "\n",
    "        Returns:\n",
    "        - np.ndarray: The image data extracted from the observation.\n",
    "        \"\"\"\n",
    "        if type(obs) == tuple:\n",
    "            return obs[0][\"image\"]\n",
    "        return obs[\"image\"]\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the environment and returns the initial observation image.\n",
    "\n",
    "        Returns:\n",
    "        - np.ndarray: The initial observation image of the reset environment.\n",
    "        \"\"\"\n",
    "        obs = super().reset()\n",
    "        return obs[0]\n",
    "    \n",
    "def get_door_key_env(size):\n",
    "    \"\"\"\n",
    "    Returns a DoorKeyEnv environment with the given size.\n",
    "    \"\"\"\n",
    "    env = MyDoorKeyEnv(size=size)\n",
    "    env = ImgObsWrapper(env)\n",
    "\n",
    "    env.reset()\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    # get an RGB image corresponding to the whole environment or the agent's point of view (https://github.com/Farama-Foundation/Minigrid/blob/master/minigrid/minigrid_env.py#L716)\n",
    "    #            highlight (bool): If true, the agent's field of view or point of view is highlighted with a lighter gray color.\n",
    "    #            tile_size (int): How many pixels will form a tile from the NxM grid.\n",
    "    #            agent_pov (bool): If true, the rendered frame will only contain the point of view of the agent.\n",
    "    frame = env.get_frame(highlight=env.highlight, tile_size=env.tile_size, agent_pov=env.agent_pov)\n",
    "    # show an image to the notebook.\n",
    "    plt.imshow(frame)\n",
    "\n",
    "    return env\n",
    "\n",
    "class MyCrossingEnv(CrossingEnv):\n",
    "    def __init__(self, size, obstacle_type):\n",
    "        \"\"\"\n",
    "        size: odd number\n",
    "        obstacle_type: \n",
    "            - Lava: episode ends when agent steps on lava.\n",
    "            - anything else: lava is wall\n",
    "        \"\"\"\n",
    "        self.render_mode = \"rgb_array\"\n",
    "        super().__init__(size=size, obstacle_type=obstacle_type)\n",
    "    \n",
    "    def _reward(self):\n",
    "        \"\"\"\n",
    "        Compute the reward to be given upon success\n",
    "        \"\"\"\n",
    "        return 1\n",
    "\n",
    "def get_crossing_env(size, obstacle_type):\n",
    "    \"\"\"\n",
    "    Returns a Crossing environment with the given size.\n",
    "    \"\"\"\n",
    "    env = MyCrossingEnv(size=size, obstacle_type=obstacle_type)\n",
    "    env = ImgObsWrapper(env)\n",
    "\n",
    "    env.reset()\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    # get an RGB image corresponding to the whole environment or the agent's point of view (https://github.com/Farama-Foundation/Minigrid/blob/master/minigrid/minigrid_env.py#L716)\n",
    "    #            highlight (bool): If true, the agent's field of view or point of view is highlighted with a lighter gray color.\n",
    "    #            tile_size (int): How many pixels will form a tile from the NxM grid.\n",
    "    #            agent_pov (bool): If true, the rendered frame will only contain the point of view of the agent.\n",
    "    frame = env.get_frame(highlight=env.highlight, tile_size=env.tile_size, agent_pov=env.agent_pov)\n",
    "    # show an image to the notebook.\n",
    "    plt.imshow(frame)\n",
    "\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACModel(nn.Module):\n",
    "    def __init__(self, use_critic=False):\n",
    "        \"\"\"\n",
    "        Represents an Actor Crictic model that takes a 2d, multi-channeled\n",
    "        image as input.\n",
    "\n",
    "        Parameters\n",
    "        ----\n",
    "        num_actions : int\n",
    "\n",
    "                      The action space of the environment.\n",
    "                      The action space for DoorKey5x5 is 7-dimensional:\n",
    "                      0: turn left,\n",
    "                      1: turn right,\n",
    "                      2: forward,\n",
    "                      3: pickup an object,\n",
    "                      4: drop an object,\n",
    "                      5: activate an object,\n",
    "                      6: done completing task\n",
    "\n",
    "        use_critics : bool\n",
    "\n",
    "                      Critic network will be used in forward pass if flag is set\n",
    "                      to true.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.use_critic = use_critic\n",
    "\n",
    "        # Define actor's model\n",
    "        self.image_conv_actor = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, (2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 2)),\n",
    "            nn.Conv2d(16, 32, (2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, (2, 2)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 7)\n",
    "        )\n",
    "\n",
    "        # Define critic's model\n",
    "        if self.use_critic:\n",
    "            self.image_conv_critic = nn.Sequential(\n",
    "                nn.Conv2d(3, 16, (2, 2)),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d((2, 2)),\n",
    "                nn.Conv2d(16, 32, (2, 2)),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(32, 64, (2, 2)),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "            self.critic = nn.Sequential(\n",
    "                nn.Linear(64, 64),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(64, 1)\n",
    "            )\n",
    "\n",
    "        # Initialize parameters correctly\n",
    "        self.apply(init_params)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        \"\"\"\n",
    "        Performs a forward pass through the actor-critic network\n",
    "\n",
    "        Parameters\n",
    "        ----\n",
    "        obs : int tensor. Shape [Batch size, ImWidth, ImHeight, Channels]\n",
    "\n",
    "              input to the network.\n",
    "        ----\n",
    "\n",
    "        returns:\n",
    "\n",
    "        dist : torch.distribution\n",
    "            The distribution of actions from policy. A Categorical distribution\n",
    "            for discreet action spaces.\n",
    "        value : torch.Tensor (Batch size, 1)\n",
    "            value output by critic network\n",
    "        \"\"\"\n",
    "        obs = torch.tensor(obs).float() # convert to float tensor\n",
    "        if len(obs.shape) == 3:\n",
    "            obs = obs.unsqueeze(0) # add batch dimension if not already there\n",
    "            \n",
    "        conv_in = obs.transpose(1, 3).transpose(2, 3) # reshape into [b, c, h, w]\n",
    "\n",
    "        dist, value = None, None\n",
    "\n",
    "        x = self.image_conv_actor(conv_in)\n",
    "        embedding = x.reshape(x.shape[0], -1)\n",
    "\n",
    "        x = self.actor(embedding)\n",
    "        dist = Categorical(logits=F.log_softmax(x, dim=1))\n",
    "\n",
    "        if self.use_critic:\n",
    "            y = self.image_conv_critic(conv_in)\n",
    "            embedding = y.reshape(y.shape[0], -1)\n",
    "\n",
    "            value = self.critic(embedding).squeeze(1)\n",
    "        else:\n",
    "            value = torch.zeros((x.shape[0], 1), device=x.device)\n",
    "        \n",
    "        return dist, value\n",
    "\n",
    "class CuriosityMachine:\n",
    "    def __init__(self, coef, model:nn.Module, num_train:int=5, args:Config=None):\n",
    "        \"\"\"\n",
    "        A machine that determine curiosity of (s, a)\n",
    "\n",
    "        Parameters:\n",
    "            coef: how fast machine learns\n",
    "            model: a model pi(a | s) without critic\n",
    "            train_num: how many times to loop optimizer\n",
    "        \"\"\"\n",
    "        if args is None:\n",
    "            args = Config()\n",
    "        self.args = args\n",
    "        self.ceof = coef\n",
    "        self.model = model\n",
    "        self.optim = torch.optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "        self.num_train = num_train\n",
    "        self.mean = 1.0\n",
    "    \n",
    "    def _update_mean(self, target:float) -> None:\n",
    "        self.mean = 0.95 * self.mean + 0.05 * target\n",
    "    \n",
    "    def update_parameters(self, sb:list) -> float:\n",
    "        \"\"\"\n",
    "        Update model parameters and mean.\n",
    "\n",
    "        Returns:\n",
    "            MSE after num_train updates.\n",
    "        \"\"\"\n",
    "        for i in range(self.num_train):\n",
    "            self.optim.zero_grad()\n",
    "            dist, _ = self.model(sb['obs'])\n",
    "            logps = dist.log_prob(sb['action'])\n",
    "            loss = torch.mean((logps + np.log(7)) ** 2)\n",
    "            if i == 0:\n",
    "                # update mean only once\n",
    "                with torch.no_grad():\n",
    "                    target = torch.mean(torch.abs(logps + np.log(7)))\n",
    "                self._update_mean(target.item())\n",
    "            loss.backward(retain_graph=True)\n",
    "            self.optim.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def modify_rs(self, sb:list, factor:float=0.1) -> None:\n",
    "        \"\"\"\n",
    "        Modify rewards by adding curiosity bonus.\n",
    "        Add a curiosity_reward key to sb.\n",
    "        \"\"\"\n",
    "        rewards = sb['reward']\n",
    "        with torch.no_grad():\n",
    "            dist, _ = self.model(sb['obs'])\n",
    "        logps = dist.log_prob(sb['action'])\n",
    "        curiosity = torch.abs(logps + np.log(7))\n",
    "        curiosity /= self.mean\n",
    "        curiosity = torch.clamp(curiosity, 0.0, 5.0)\n",
    "        curiosity *= factor\n",
    "        rewards += curiosity\n",
    "\n",
    "        sb['curiosity_reward'] = rewards\n",
    "        \n",
    "class Machine:\n",
    "    def __init__(self, entropy_coef, init_model:nn.Module, args:Config=None, smart_discard=False, curiosity_factor:float=0.1):\n",
    "        \"\"\"\n",
    "        A Machine object consists of a Model and its entropy_coef\n",
    "\n",
    "        Args:\n",
    "            entropy_coef: Entropy coefficient.\n",
    "            init_model: Initial model.\n",
    "            args\n",
    "        \"\"\"\n",
    "        if args is None:\n",
    "            self.args = Config()\n",
    "        else:\n",
    "            self.args = args\n",
    "\n",
    "        self.model = init_model\n",
    "        self.optim = torch.optim.Adam(self.model.parameters(), lr=self.args.lr)\n",
    "        self.coef = entropy_coef\n",
    "        self.smart_discard = smart_discard\n",
    "        self.curiosity_factor = curiosity_factor\n",
    "\n",
    "    def copy_model(self, other_model:nn.Module) -> None:\n",
    "        \"\"\"\n",
    "        Copy state dict from 'model'. Reset rs.\n",
    "        \"\"\"\n",
    "        state_dict = other_model.state_dict()\n",
    "        for key, v in state_dict.items():\n",
    "            if key in self.model.state_dict():\n",
    "                self.model.state_dict()[key].copy_(v)\n",
    "\n",
    "    def copy_machine(self, other:Self) -> None:\n",
    "        \"\"\"\n",
    "        Copy state dict from 'other'. Reset rs.\n",
    "        \"\"\"\n",
    "        self.copy_model(other.model)\n",
    "\n",
    "    def _compute_discounted_return(self, rewards):\n",
    "        \"\"\"\n",
    "            rewards: reward obtained at timestep.  Shape: (T,)\n",
    "            discount: discount factor. float\n",
    "\n",
    "        ----\n",
    "        returns: sum of discounted rewards. Shape: (T,)\n",
    "        \"\"\"\n",
    "        device = get_device()\n",
    "        returns = torch.zeros(*rewards.shape, device=device)\n",
    "\n",
    "        R = 0\n",
    "        for t in reversed(range((rewards.shape[0]))):\n",
    "            R = rewards[t] + self.args.discount * R\n",
    "            returns[t] = R\n",
    "        return returns\n",
    "\n",
    "    def _compute_advantage_gae(self, values, rewards, T):\n",
    "        \"\"\"\n",
    "        Compute Adavantage wiht GAE. See Section 4.4.2 in the lecture notes.\n",
    "\n",
    "        values: value at each timestep (T,)\n",
    "        rewards: reward obtained at each timestep.  Shape: (T,)\n",
    "        T: the number of frames, float\n",
    "        gae_lambda: hyperparameter, float\n",
    "        discount: discount factor, float\n",
    "\n",
    "        -----\n",
    "\n",
    "        returns:\n",
    "\n",
    "        advantages : tensor.float. Shape [T,]\n",
    "\n",
    "                    gae advantage term for timesteps 0 to T\n",
    "\n",
    "        \"\"\"\n",
    "        advantages = torch.zeros_like(values)\n",
    "        for i in reversed(range(T)):\n",
    "            next_value = values[i+1]\n",
    "            next_advantage = advantages[i+1]\n",
    "\n",
    "            delta = rewards[i] + self.args.discount * next_value  - values[i]\n",
    "            advantages[i] = delta + self.args.discount * self.args.gae_lambda * next_advantage\n",
    "        return advantages[:T]\n",
    "    \n",
    "    def collect_experiences(self, env:gym.Env):\n",
    "        \"\"\"\n",
    "        Collects rollouts and computes advantages.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        exps : dict\n",
    "            Contains actions, rewards, advantages etc as attributes.\n",
    "            Each attribute, e.g. `exps['reward']` has a shape\n",
    "            (self.num_frames, ...).\n",
    "        logs : dict\n",
    "            Useful stats about the training process, including the average\n",
    "            reward, policy loss, value loss, etc.\n",
    "        \"\"\"\n",
    "        device = get_device()\n",
    "\n",
    "        MAX_FRAMES_PER_EP = 300\n",
    "        shape = (MAX_FRAMES_PER_EP, )\n",
    "\n",
    "        actions = torch.zeros(*shape, device=device, dtype=torch.int)\n",
    "        values = torch.zeros(*shape, device=device)\n",
    "        rewards = torch.zeros(*shape, device=device)\n",
    "        log_probs = torch.zeros(*shape, device=device)\n",
    "        obss = [None]*MAX_FRAMES_PER_EP\n",
    "\n",
    "        obs = env.reset()\n",
    "\n",
    "        total_return = 0\n",
    "\n",
    "        T = 0\n",
    "\n",
    "        while True:\n",
    "            # Do one agent-environment interaction\n",
    "            with torch.no_grad():\n",
    "                dist, value = self.model(obs)\n",
    "\n",
    "            dist: Categorical\n",
    "            action = dist.sample()[0]\n",
    "\n",
    "            obss[T] = obs\n",
    "            obs, reward, done, _, _ = env.step(action.item())\n",
    "\n",
    "            # Update experiences values\n",
    "            actions[T] = action\n",
    "            values[T] = value\n",
    "            rewards[T] = reward\n",
    "            log_probs[T] = dist.log_prob(action)\n",
    "\n",
    "            total_return += reward\n",
    "            T += 1\n",
    "\n",
    "            if done or T >= MAX_FRAMES_PER_EP-1:\n",
    "                break\n",
    "\n",
    "        success = (total_return > 0.1)\n",
    "        \n",
    "        discounted_reward = self._compute_discounted_return(rewards[:T])\n",
    "        exps = dict(\n",
    "            obs = torch.tensor(np.array(obss[:T]), device=device),\n",
    "            action = actions[:T],\n",
    "            value  = values[:T],\n",
    "            reward = rewards[:T],\n",
    "            log_prob = log_probs[:T],\n",
    "            discounted_reward = discounted_reward,\n",
    "            T = T\n",
    "        )\n",
    "\n",
    "        logs = {\n",
    "            \"return_per_episode\": total_return,\n",
    "            \"num_frames\": T,\n",
    "            'success': success\n",
    "        }\n",
    "\n",
    "        return exps, logs\n",
    "\n",
    "    def _compute_policy_loss_ppo(self, dist:Categorical, factors, indices, old_logp, actions, advantages):\n",
    "        \"\"\"\n",
    "        Computes the policy loss for PPO.\n",
    "\n",
    "        obs: observeration to pass into acmodel. shape: (T,)\n",
    "        init_logp: log probabilities we get from the agent performing the action. shape: (T,)\n",
    "        old_logp: log probabilities from previous timestep. shape: (T,)\n",
    "        actions: action at this timestep. shape: (T,ImWidth,ImHeight,Channels)\n",
    "        advantages: the computed advantages. shape: (T,)\n",
    "\n",
    "        ---\n",
    "        returns\n",
    "\n",
    "        policy_loss : ppo policy loss as shown in line 6 of PPO alg. tensor.float. Shape (,1)\n",
    "        approx_kl: an appoximation of the kl_divergence. tensor.float. Shape (,1)\n",
    "        \"\"\"\n",
    "        policy_loss, approx_kl = 0, 0\n",
    "\n",
    "        entropy = dist.entropy()\n",
    "        logps = dist.log_prob(actions)\n",
    "        r_terms = torch.exp(logps - old_logp)\n",
    "        ppo_loss = torch.min(r_terms * advantages, torch.clamp(r_terms, 1 - self.args.clip_ratio, 1 + self.args.clip_ratio) * advantages)\n",
    "        \n",
    "        policy_loss_tensor = factors * ppo_loss + self.coef * entropy\n",
    "\n",
    "        policy_loss = - torch.mean(policy_loss_tensor[indices])\n",
    "\n",
    "        # approx_kl = torch.sum(torch.exp(old_logp) * (old_logp - logps)) / torch.sum(torch.exp(old_logp))\n",
    "        approx_kl = torch.mean((old_logp - logps) ** 2) / 2\n",
    "\n",
    "        return policy_loss, approx_kl\n",
    "    \n",
    "    def _compute_value_loss(self, values, returns):\n",
    "        value_loss = torch.mean((values - returns) ** 2)\n",
    "\n",
    "        return value_loss\n",
    "\n",
    "    def update_parameters(self, sb, update_v=True, curiositor:CuriosityMachine=None):\n",
    "        MAX_FRAMES_PER_EP = 300\n",
    "        T = sb['T']\n",
    "        with torch.no_grad():\n",
    "            dist, values = self.model(sb['obs'])\n",
    "        values = values.reshape(-1)\n",
    "        dist: Categorical\n",
    "        old_logp = dist.log_prob(sb['action'])\n",
    "        init_logp = sb['log_prob']\n",
    "        if curiositor is None:\n",
    "            not_full_reward = sb['reward']\n",
    "        else:\n",
    "            curiositor.modify_rs(sb, factor=self.curiosity_factor)\n",
    "            not_full_reward = sb['curiosity_reward']\n",
    "\n",
    "        # add 0 to end of values until it has length MAX_FRAMES_PER_EP\n",
    "        values_extended = torch.cat([values, torch.zeros((MAX_FRAMES_PER_EP - len(values), ), device=get_device())], dim=0)\n",
    "        full_reward = torch.cat([not_full_reward, torch.zeros((MAX_FRAMES_PER_EP - len(not_full_reward), ), device=get_device())], dim=0)\n",
    "\n",
    "\n",
    "        if self.args.use_gae:\n",
    "            advantage = self._compute_advantage_gae(values_extended, full_reward, T)\n",
    "        else:\n",
    "            advantage = sb['discounted_reward'] - values.reshape(-1)\n",
    "\n",
    "        for i in range(self.args.train_ac_iters):\n",
    "            self.optim.zero_grad()\n",
    "            dist, values = self.model(sb['obs'])\n",
    "            values = values.reshape(-1)\n",
    "            # policy loss\n",
    "            factors = torch.exp(old_logp - init_logp)\n",
    "            if self.smart_discard:\n",
    "                indices = (factors < self.args.importance_sampling_clip) & (torch.exp(old_logp) < (1.0 + torch.exp(init_logp - old_logp)) / 2)\n",
    "            else:\n",
    "                indices = factors < self.args.importance_sampling_clip\n",
    "            fit = torch.mean(indices.to(torch.float32))\n",
    "\n",
    "            loss_pi, approx_kl = self._compute_policy_loss_ppo(dist, factors, indices, old_logp, sb['action'], advantage)\n",
    "            if update_v:\n",
    "                loss_v = self._compute_value_loss(values, sb['discounted_reward'])\n",
    "            else:\n",
    "                loss_v = 0.0\n",
    "\n",
    "            if i == 0:\n",
    "                policy_loss = loss_pi\n",
    "                value_loss = loss_v\n",
    "\n",
    "            loss = loss_v + loss_pi\n",
    "            if approx_kl > 1.5 * self.args.target_kl:\n",
    "                break\n",
    "\n",
    "            loss.backward(retain_graph=True)\n",
    "            self.optim.step()\n",
    "\n",
    "        update_policy_loss = policy_loss.item()\n",
    "        update_value_loss = value_loss.item()\n",
    "\n",
    "        logs = {\n",
    "            \"policy_loss\": update_policy_loss,\n",
    "            \"value_loss\": update_value_loss,\n",
    "            \"fit\": fit.item()\n",
    "        }\n",
    "\n",
    "        return logs\n",
    "    \n",
    "    def decrease_prob(self, sb, lr=0.1) -> None:\n",
    "        self.optim.zero_grad()\n",
    "\n",
    "        dist, _ = self.model(sb['obs'])\n",
    "        dist: Categorical\n",
    "        logps = dist.log_prob(sb['action'])\n",
    "\n",
    "        loss = lr * torch.mean(logps)\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "\n",
    "class RandomMachine:\n",
    "    def __init__(self, args:Config=None):\n",
    "        self.args = args if args is not None else Config()\n",
    "\n",
    "    def _compute_discounted_return(self, rewards):\n",
    "        \"\"\"\n",
    "            rewards: reward obtained at timestep.  Shape: (T,)\n",
    "            discount: discount factor. float\n",
    "\n",
    "        ----\n",
    "        returns: sum of discounted rewards. Shape: (T,)\n",
    "        \"\"\"\n",
    "        device = get_device()\n",
    "        returns = torch.zeros(*rewards.shape, device=device)\n",
    "\n",
    "        R = 0\n",
    "        for t in reversed(range((rewards.shape[0]))):\n",
    "            R = rewards[t] + self.args.discount * R\n",
    "            returns[t] = R\n",
    "        return returns\n",
    "    \n",
    "    def collect_experiences(self, env:gym.Env):\n",
    "        \"\"\"\n",
    "        Collects rollouts.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        exps : dict\n",
    "            Contains actions, rewards, advantages etc as attributes.\n",
    "            Each attribute, e.g. `exps['reward']` has a shape\n",
    "            (self.num_frames, ...).\n",
    "        logs : dict\n",
    "            Useful stats about the training process, including the average\n",
    "            reward, policy loss, value loss, etc.\n",
    "        \"\"\"\n",
    "        device = get_device()\n",
    "        num_actions = env.action_space.n\n",
    "\n",
    "        MAX_FRAMES_PER_EP = 300\n",
    "        shape = (MAX_FRAMES_PER_EP, )\n",
    "\n",
    "        actions = torch.zeros(*shape, device=device, dtype=torch.int)\n",
    "        values = torch.zeros(*shape, device=device)\n",
    "        rewards = torch.zeros(*shape, device=device)\n",
    "        log_probs = torch.zeros(*shape, device=device)\n",
    "        obss = [None]*MAX_FRAMES_PER_EP\n",
    "\n",
    "        obs = env.reset()\n",
    "\n",
    "        total_return = 0\n",
    "\n",
    "        T = 0\n",
    "\n",
    "        while True:\n",
    "            # Do one agent-environment interaction\n",
    "            action = torch.randint(num_actions, size=(1,)).to(device)\n",
    "\n",
    "            obss[T] = obs\n",
    "            obs, reward, done, _, _ = env.step(action.item())\n",
    "\n",
    "            # Update experiences values\n",
    "            actions[T] = action\n",
    "            values[T] = 0\n",
    "            rewards[T] = reward\n",
    "            log_probs[T] = -np.log(num_actions)\n",
    "\n",
    "            total_return += reward\n",
    "            T += 1\n",
    "\n",
    "            if done or T >= MAX_FRAMES_PER_EP-1:\n",
    "                break\n",
    "\n",
    "        success = (total_return > 0.1)\n",
    "        \n",
    "        discounted_reward = self._compute_discounted_return(rewards[:T])\n",
    "        exps = dict(\n",
    "            obs = torch.tensor(np.array(obss[:T]), device=device),\n",
    "            action = actions[:T],\n",
    "            value  = values[:T],\n",
    "            reward = rewards[:T],\n",
    "            log_prob = log_probs[:T],\n",
    "            discounted_reward = discounted_reward,\n",
    "            T = T\n",
    "        )\n",
    "\n",
    "        logs = {\n",
    "            \"return_per_episode\": total_return,\n",
    "            \"num_frames\": T,\n",
    "            'success': success\n",
    "        }\n",
    "\n",
    "        return exps, logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self, ACModelClass, env, args:Config=None, seed=0):\n",
    "        \"\"\"\n",
    "        The PPO agent.\n",
    "        \"\"\"\n",
    "\n",
    "        self.env = env\n",
    "\n",
    "        set_random_seed(seed)\n",
    "        model = ACModelClass(use_critic=True)\n",
    "\n",
    "        if args is None:\n",
    "            args = Config()\n",
    "        self.machine = Machine(0.01, model, args)\n",
    "    \n",
    "    def train(self, max_episodes:int=10000, nonstop:bool=False, max_frames=float('inf')) -> tuple[list[int], list[float]]:\n",
    "        \"\"\"\n",
    "        Train the PPO agent.\n",
    "\n",
    "        Returns:\n",
    "            num_frames, smooth_rs\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f'Start! Agent: PPO.')\n",
    "\n",
    "        is_solved = False\n",
    "\n",
    "        SMOOTH_REWARD_WINDOW = 50\n",
    "\n",
    "        rewards = [0]*SMOOTH_REWARD_WINDOW\n",
    "\n",
    "        total_smooth_rs = []\n",
    "        total_num_frames = []\n",
    "\n",
    "        num_frames = 0\n",
    "\n",
    "        pbar = tqdm(range(max_episodes))\n",
    "        for update in pbar:\n",
    "\n",
    "            exps, logs1 = self.machine.collect_experiences(self.env)\n",
    "\n",
    "            logs2 = self.machine.update_parameters(exps)\n",
    "\n",
    "            logs = {**logs1, **logs2}\n",
    "\n",
    "            total_num_frames.append(num_frames)\n",
    "            num_frames += logs[\"num_frames\"]\n",
    "\n",
    "            rewards.append(logs[\"return_per_episode\"])\n",
    "\n",
    "            smooth_reward = np.mean(rewards[-SMOOTH_REWARD_WINDOW:])\n",
    "\n",
    "            total_smooth_rs.append(smooth_reward)\n",
    "\n",
    "            data = {'num_frames':num_frames, 'smooth_reward':smooth_reward,\n",
    "                'reward':logs[\"return_per_episode\"], 'policy_loss':logs[\"policy_loss\"], 'value_loss': logs['value_loss'], 'episode':update}\n",
    "\n",
    "            pbar.set_postfix(data)\n",
    "\n",
    "            if not nonstop and smooth_reward >= self.machine.args.score_threshold:\n",
    "                is_solved = True\n",
    "                break\n",
    "            if num_frames >= max_frames:\n",
    "                break\n",
    "\n",
    "        if not nonstop and is_solved:\n",
    "            print('Solved!')\n",
    "\n",
    "        return total_num_frames, total_smooth_rs\n",
    "\n",
    "class RRR:\n",
    "    def __init__(self, ACModelClass, env, args:Config=None, seed=0):\n",
    "        \"\"\"\n",
    "        The RRR agent.\n",
    "        \"\"\"\n",
    "\n",
    "        self.env = env\n",
    "\n",
    "        set_random_seed(seed)\n",
    "        m1 = ACModelClass(use_critic=True)\n",
    "\n",
    "        if args is None:\n",
    "            args = Config()\n",
    "\n",
    "        self.exploiter = Machine(0.01, m1, args)\n",
    "\n",
    "        m2 = ACModelClass(use_critic=False)\n",
    "        m3 = ACModelClass(use_critic=False)\n",
    "        m4 = ACModelClass(use_critic=False)\n",
    "\n",
    "        self.explorer_random = Machine(0.03, m2, args)\n",
    "        self.explorer_thirsty = Machine(0.02, m3, args)\n",
    "        self.explorer_thirsty.copy_machine(self.exploiter)\n",
    "\n",
    "        self.temp_machine = Machine(0.01, m3, args)\n",
    "        self.explorer_bengbuzhu = Machine(0.5, m4, args)\n",
    "\n",
    "    def _replay(self, machine:Machine, buffer:list, cutoff:float) -> float:\n",
    "        \"\"\"\n",
    "        Replay random sample from buffer on machine.\n",
    "\n",
    "        Args:\n",
    "            machine: Machine to replay on\n",
    "            buffer: list of experiences\n",
    "            cutoff: if fit < cutoff, delete sb from buffer\n",
    "\n",
    "        Returns:\n",
    "            fit\n",
    "        \"\"\"\n",
    "\n",
    "        idx = np.random.randint(len(buffer))\n",
    "        sb = buffer[idx]\n",
    "        logs_replay = machine.update_parameters(sb)\n",
    "        fit = logs_replay['fit']\n",
    "        if fit < cutoff:\n",
    "            # delete sb from buffer\n",
    "            buffer.pop(idx)\n",
    "\n",
    "        return fit\n",
    "\n",
    "    def _add_sb_to_buffer(self, exps, buffer:list, capacity:int) -> None:\n",
    "        buffer.append(exps)\n",
    "        if len(buffer) > capacity:\n",
    "            buffer.pop(0)\n",
    "\n",
    "\n",
    "    def train(self, max_episodes:int=10000, nonstop:bool=False, max_frames=float('inf')) -> tuple[list[int], list[float], list[float]]:\n",
    "        \"\"\"\n",
    "        Train the agent.\n",
    "\n",
    "        Returns:\n",
    "            num_frames, smooth_rs, fits\n",
    "        \"\"\"\n",
    "\n",
    "        print('Start! Agent: RRR.')\n",
    "        RANDOM_MODE = 0 # Bengbuzhu explores\n",
    "        EXPLORE_MODE = 1\n",
    "        EXPLOIT_MODE = 2\n",
    "        mode = RANDOM_MODE\n",
    "\n",
    "        is_solved = False\n",
    "\n",
    "        SMOOTH_REWARD_WINDOW = 50\n",
    "\n",
    "        rewards = [0]*SMOOTH_REWARD_WINDOW\n",
    "\n",
    "        total_smooth_rs = []\n",
    "        total_num_frames = []\n",
    "        larger_buffer_r = []\n",
    "        buffer_r = []\n",
    "        buffer_no_r = []\n",
    "        fits = []\n",
    "\n",
    "        num_frames = 0\n",
    "\n",
    "        pbar = tqdm(range(max_episodes))\n",
    "        for update in pbar:\n",
    "            total_num_frames.append(num_frames)\n",
    "\n",
    "            if mode == RANDOM_MODE:\n",
    "                exps, logs1 = self.explorer_bengbuzhu.collect_experiences(self.env)\n",
    "                self.explorer_bengbuzhu.update_parameters(exps)\n",
    "                logs2 = self.exploiter.update_parameters(exps)\n",
    "\n",
    "            elif mode == EXPLORE_MODE:\n",
    "                if np.random.rand() < 0.5:\n",
    "                    m = self.explorer_random\n",
    "                else:\n",
    "                    m = self.explorer_thirsty\n",
    "\n",
    "                exps, logs1 = m.collect_experiences(self.env)\n",
    "\n",
    "                m.update_parameters(exps)\n",
    "                logs2 = self.exploiter.update_parameters(exps)\n",
    "\n",
    "                if len(larger_buffer_r) >= 1:\n",
    "                    self._replay(self.explorer_thirsty, larger_buffer_r, cutoff=0.0)\n",
    "\n",
    "            elif mode == EXPLOIT_MODE:\n",
    "                exps, logs1 = self.exploiter.collect_experiences(self.env)\n",
    "\n",
    "                logs2 = self.exploiter.update_parameters(exps)\n",
    "\n",
    "                assert len(buffer_r) > 0, f'buffer_r should not be empty.'\n",
    "\n",
    "                cutoff = self.exploiter.args.bad_fit_threshold + self.exploiter.args.bad_fit_increment * (len(buffer_r) - 1)\n",
    "\n",
    "                if not logs1['success'] and total_smooth_rs[-1] <= 0.5:\n",
    "                    fit = self._replay(self.exploiter, buffer_r, cutoff)\n",
    "                    fits.append(fit)\n",
    "\n",
    "                if len(buffer_r) == 0:\n",
    "                    print(f'At episode {update}, we lost all data in replay buffer...')\n",
    "                    mode = EXPLORE_MODE\n",
    "                    self.explorer_thirsty.copy_machine(self.exploiter)\n",
    "                    self.explorer_random.copy_machine(self.temp_machine)\n",
    "                    self.temp_machine.copy_machine(self.exploiter)\n",
    "\n",
    "            else:\n",
    "                raise ValueError(f'Invalid mode: {mode}')\n",
    "\n",
    "            logs = {**logs1, **logs2}\n",
    "\n",
    "            num_frames += logs[\"num_frames\"]\n",
    "\n",
    "            rewards.append(logs[\"return_per_episode\"])\n",
    "\n",
    "            if logs['success']:\n",
    "                if mode == RANDOM_MODE:\n",
    "                    print(f'First successful data collected at episode {update}! We will be accelerating.')\n",
    "                elif mode == EXPLORE_MODE:\n",
    "                    if m is self.explorer_random:\n",
    "                        info_print = 'random explorer'\n",
    "                    elif m is self.explorer_thirsty:\n",
    "                        info_print = 'thirsty explorer'\n",
    "                    else:\n",
    "                        raise ValueError(f'Invalid explorer: {m}')\n",
    "                    \n",
    "                    print(f'At episode {update}, {info_print} collected a successful data.')\n",
    "                elif mode == EXPLOIT_MODE:\n",
    "                    pass\n",
    "                else:\n",
    "                    raise ValueError(f'Invalid mode: {mode}')\n",
    "                self._add_sb_to_buffer(exps, buffer_r, self.exploiter.args.replay_buffer_capacity)\n",
    "                self._add_sb_to_buffer(exps, larger_buffer_r, self.exploiter.args.large_buffer_capacity)\n",
    "                mode = EXPLOIT_MODE\n",
    "            else:\n",
    "                self._add_sb_to_buffer(exps, buffer_no_r, self.exploiter.args.replay_buffer_capacity)\n",
    "\n",
    "            smooth_reward = np.mean(rewards[-SMOOTH_REWARD_WINDOW:])\n",
    "\n",
    "            total_smooth_rs.append(smooth_reward)\n",
    "\n",
    "            data = {'num_frames':num_frames, 'smooth_reward':smooth_reward,\n",
    "                'reward':logs[\"return_per_episode\"], 'episode':update}\n",
    "\n",
    "            pbar.set_postfix(data)\n",
    "\n",
    "            if not nonstop and smooth_reward >= self.exploiter.args.score_threshold:\n",
    "                    is_solved = True\n",
    "                    break\n",
    "            if num_frames >= max_frames:\n",
    "                break\n",
    "\n",
    "        if is_solved:\n",
    "            print('Solved!')\n",
    "\n",
    "        return total_num_frames, total_smooth_rs, fits\n",
    "\n",
    "class BetaRRR:\n",
    "    def __init__(self, ACModelClass, env, args:Config=None, seed=0):\n",
    "        \"\"\"\n",
    "        The RRR agent.\n",
    "        \"\"\"\n",
    "\n",
    "        self.env = env\n",
    "\n",
    "        set_random_seed(seed)\n",
    "        m1 = ACModelClass(use_critic=True)\n",
    "\n",
    "        if args is None:\n",
    "            args = Config()\n",
    "\n",
    "        self.exploiter = Machine(0.01, m1, args, smart_discard=True)\n",
    "\n",
    "        m2 = ACModelClass(use_critic=False)\n",
    "        m3 = ACModelClass(use_critic=False)\n",
    "        m4 = ACModelClass(use_critic=False)\n",
    "\n",
    "        self.explorer_random = Machine(0.03, m2, args)\n",
    "        self.explorer_thirsty = Machine(0.02, m3, args)\n",
    "\n",
    "        self.explorer_bengbuzhu = Machine(0.5, m4, args)\n",
    "\n",
    "    def _replay(self, machine:Machine, buffer:list, cutoff:float) -> float:\n",
    "        \"\"\"\n",
    "        Replay random sample from buffer on machine.\n",
    "\n",
    "        Args:\n",
    "            machine: Machine to replay on\n",
    "            buffer: list of experiences\n",
    "            cutoff: if fit < cutoff, delete sb from buffer\n",
    "\n",
    "        Returns:\n",
    "            fit\n",
    "        \"\"\"\n",
    "\n",
    "        idx = np.random.randint(len(buffer))\n",
    "        sb = buffer[idx]\n",
    "        logs_replay = machine.update_parameters(sb)\n",
    "        fit = logs_replay['fit']\n",
    "        if fit < cutoff:\n",
    "            # delete sb from buffer\n",
    "            buffer.pop(idx)\n",
    "\n",
    "        return fit\n",
    "\n",
    "    def _add_sb_to_buffer(self, exps, buffer:list, capacity:int) -> None:\n",
    "        buffer.append(exps)\n",
    "        if len(buffer) > capacity:\n",
    "            buffer.pop(0)\n",
    "\n",
    "    def train(self, max_episodes:int=10000, nonstop:bool=False, max_frames=float('inf')) -> tuple[list[int], list[float], list[float]]:\n",
    "        \"\"\"\n",
    "        Train the agent.\n",
    "\n",
    "        Returns:\n",
    "            num_frames, smooth_rs, fits\n",
    "        \"\"\"\n",
    "\n",
    "        print('Start! Agent: beta RRR.')\n",
    "        RANDOM_MODE = 0\n",
    "        EXPLORE_MODE = 1\n",
    "        EXPLOIT_MODE = 2\n",
    "        mode = RANDOM_MODE\n",
    "\n",
    "        is_solved = False\n",
    "\n",
    "        SMOOTH_REWARD_WINDOW = 50\n",
    "\n",
    "        rewards = [0]*SMOOTH_REWARD_WINDOW\n",
    "\n",
    "        total_smooth_rs = []\n",
    "        total_num_frames = []\n",
    "        larger_buffer_r = []\n",
    "        buffer_r = []\n",
    "        buffer_no_r = []\n",
    "        fits = []\n",
    "        cutoff_lock = True\n",
    "\n",
    "        num_frames = 0\n",
    "\n",
    "        pbar = tqdm(range(max_episodes))\n",
    "        for update in pbar:\n",
    "            total_num_frames.append(num_frames)\n",
    "\n",
    "            if mode == RANDOM_MODE:\n",
    "                exps, logs1 = self.explorer_bengbuzhu.collect_experiences(self.env)\n",
    "                self.explorer_bengbuzhu.update_parameters(exps)\n",
    "                logs2 = self.exploiter.update_parameters(exps)\n",
    "\n",
    "            elif mode == EXPLORE_MODE:\n",
    "                if np.random.rand() < 0.5:\n",
    "                    m = self.explorer_random\n",
    "                else:\n",
    "                    m = self.explorer_thirsty\n",
    "\n",
    "                exps, logs1 = m.collect_experiences(self.env)\n",
    "\n",
    "                m.update_parameters(exps)\n",
    "                logs2 = self.exploiter.update_parameters(exps)\n",
    "\n",
    "                if len(larger_buffer_r) >= 1:\n",
    "                    self._replay(self.explorer_thirsty, larger_buffer_r, cutoff=0.0)\n",
    "\n",
    "            elif mode == EXPLOIT_MODE:\n",
    "                exps, logs1 = self.exploiter.collect_experiences(self.env)\n",
    "\n",
    "                logs2 = self.exploiter.update_parameters(exps)\n",
    "\n",
    "                assert len(buffer_r) > 0, f'buffer_r should not be empty.'\n",
    "\n",
    "                if cutoff_lock:\n",
    "                    cutoff = self.exploiter.args.bad_fit_threshold\n",
    "                else:\n",
    "                    cutoff = self.exploiter.args.bad_fit_threshold + self.exploiter.args.bad_fit_increment * (len(buffer_r) - 1)\n",
    "\n",
    "                if not logs1['success'] and total_smooth_rs[-1] <= 0.5:\n",
    "                    fit = self._replay(self.exploiter, buffer_r, cutoff)\n",
    "                    fits.append(fit)\n",
    "\n",
    "                if len(buffer_r) == 0:\n",
    "                    print(f'At episode {update}, we lost all data in replay buffer...')\n",
    "                    mode = EXPLORE_MODE\n",
    "                    self.explorer_thirsty.copy_machine(self.exploiter)\n",
    "                    self.explorer_random.copy_machine(self.exploiter)\n",
    "\n",
    "            else:\n",
    "                raise ValueError(f'Invalid mode: {mode}')\n",
    "\n",
    "            logs = {**logs1, **logs2}\n",
    "\n",
    "            num_frames += logs[\"num_frames\"]\n",
    "\n",
    "            rewards.append(logs[\"return_per_episode\"])\n",
    "\n",
    "            if logs['success']:\n",
    "                if mode == RANDOM_MODE:\n",
    "                    print(f'First successful data collected at episode {update}! We will be accelerating.')\n",
    "                elif mode == EXPLORE_MODE:\n",
    "                    if m is self.explorer_random:\n",
    "                        info_print = 'random explorer'\n",
    "                    elif m is self.explorer_thirsty:\n",
    "                        info_print = 'thirsty explorer'\n",
    "                    else:\n",
    "                        raise ValueError(f'Invalid explorer: {m}')\n",
    "\n",
    "                    buffer_r = deepcopy(larger_buffer_r)\n",
    "                    if len(buffer_r) > self.exploiter.args.replay_buffer_capacity:\n",
    "                        buffer_r = buffer_r[-self.exploiter.args.replay_buffer_capacity:]\n",
    "                    cutoff_lock = True\n",
    "                    \n",
    "                    print(f'At episode {update}, {info_print} collected a successful data.')\n",
    "                elif mode == EXPLOIT_MODE:\n",
    "                    cutoff_lock = False\n",
    "                else:\n",
    "                    raise ValueError(f'Invalid mode: {mode}')\n",
    "\n",
    "                self._add_sb_to_buffer(exps, buffer_r, self.exploiter.args.replay_buffer_capacity)\n",
    "                self._add_sb_to_buffer(exps, larger_buffer_r, self.exploiter.args.large_buffer_capacity)\n",
    "                mode = EXPLOIT_MODE\n",
    "            else:\n",
    "                self._add_sb_to_buffer(exps, buffer_no_r, self.exploiter.args.replay_buffer_capacity)\n",
    "\n",
    "            smooth_reward = np.mean(rewards[-SMOOTH_REWARD_WINDOW:])\n",
    "\n",
    "            total_smooth_rs.append(smooth_reward)\n",
    "\n",
    "            data = {'num_frames':num_frames, 'smooth_reward':smooth_reward,\n",
    "                'reward':logs[\"return_per_episode\"], 'episode':update}\n",
    "\n",
    "            pbar.set_postfix(data)\n",
    "\n",
    "            if not nonstop and smooth_reward >= self.exploiter.args.score_threshold:\n",
    "                    is_solved = True\n",
    "                    break\n",
    "            if num_frames >= max_frames:\n",
    "                break\n",
    "\n",
    "        if is_solved:\n",
    "            print('Solved!')\n",
    "\n",
    "        return total_num_frames, total_smooth_rs, fits\n",
    "\n",
    "class WeakRRR:\n",
    "    def __init__(self, ACModelClass, env, args:Config=None, seed=0):\n",
    "        \"\"\"\n",
    "        The RRR agent.\n",
    "        \"\"\"\n",
    "\n",
    "        self.env = env\n",
    "\n",
    "        set_random_seed(seed)\n",
    "        m1 = ACModelClass(use_critic=True)\n",
    "\n",
    "        if args is None:\n",
    "            args = Config()\n",
    "\n",
    "        self.exploiter = Machine(0.01, m1, args)\n",
    "\n",
    "    def _replay(self, machine:Machine, buffer:list, cutoff:float) -> float:\n",
    "        \"\"\"\n",
    "        Replay random sample from buffer on machine.\n",
    "\n",
    "        Args:\n",
    "            machine: Machine to replay on\n",
    "            buffer: list of experiences\n",
    "            cutoff: if fit < cutoff, delete sb from buffer\n",
    "\n",
    "        Returns:\n",
    "            fit\n",
    "        \"\"\"\n",
    "\n",
    "        idx = np.random.randint(len(buffer))\n",
    "        sb = buffer[idx]\n",
    "        logs_replay = machine.update_parameters(sb)\n",
    "        fit = logs_replay['fit']\n",
    "        if fit < cutoff:\n",
    "            # delete sb from buffer\n",
    "            buffer.pop(idx)\n",
    "        \n",
    "        return fit\n",
    "    \n",
    "    def _add_sb_to_buffer(self, exps, buffer:list, capacity:int) -> None:\n",
    "        buffer.append(exps)\n",
    "        if len(buffer) > capacity:\n",
    "            buffer.pop(0)\n",
    "\n",
    "\n",
    "    def train(self, max_episodes:int=10000, nonstop:bool=False, max_frames=float('inf')) -> tuple[list[int], list[float], list[float]]:\n",
    "        \"\"\"\n",
    "        Train the agent.\n",
    "\n",
    "        Returns:\n",
    "            num_frames, smooth_rs, fits\n",
    "        \"\"\"\n",
    "\n",
    "        print('Start! Agent: weak RRR.')\n",
    "\n",
    "        is_solved = False\n",
    "\n",
    "        SMOOTH_REWARD_WINDOW = 50\n",
    "\n",
    "        rewards = [0]*SMOOTH_REWARD_WINDOW\n",
    "\n",
    "        total_smooth_rs = []\n",
    "        total_num_frames = []\n",
    "        buffer_r = []\n",
    "        fits = []\n",
    "\n",
    "        num_frames = 0\n",
    "\n",
    "        pbar = tqdm(range(max_episodes))\n",
    "        for update in pbar:\n",
    "            total_num_frames.append(num_frames)\n",
    "\n",
    "            exps, logs1 = self.exploiter.collect_experiences(self.env)\n",
    "\n",
    "            logs2 = self.exploiter.update_parameters(exps)\n",
    "\n",
    "            cutoff = self.exploiter.args.bad_fit_threshold + self.exploiter.args.bad_fit_increment * (len(buffer_r) - 1)\n",
    "            \n",
    "            if len(buffer_r) > 0 and not logs1['success'] and total_smooth_rs[-1] <= 0.5:\n",
    "                fit = self._replay(self.exploiter, buffer_r, cutoff)\n",
    "                fits.append(fit)\n",
    "            \n",
    "            logs = {**logs1, **logs2}\n",
    "\n",
    "            num_frames += logs[\"num_frames\"]\n",
    "\n",
    "            rewards.append(logs[\"return_per_episode\"])\n",
    "\n",
    "            if logs['success']:\n",
    "                self._add_sb_to_buffer(exps, buffer_r, self.exploiter.args.replay_buffer_capacity)\n",
    "\n",
    "            smooth_reward = np.mean(rewards[-SMOOTH_REWARD_WINDOW:])\n",
    "\n",
    "            total_smooth_rs.append(smooth_reward)\n",
    "\n",
    "            data = {'num_frames':num_frames, 'smooth_reward':smooth_reward,\n",
    "                'reward':logs[\"return_per_episode\"], 'episode':update}\n",
    "\n",
    "            pbar.set_postfix(data)\n",
    "\n",
    "            if not nonstop and smooth_reward >= self.exploiter.args.score_threshold:\n",
    "                    is_solved = True\n",
    "                    break\n",
    "            if num_frames >= max_frames:\n",
    "                break\n",
    "    \n",
    "        if is_solved:\n",
    "            print('Solved!')\n",
    "\n",
    "        return total_num_frames, total_smooth_rs, fits\n",
    "\n",
    "class RandomRRR:\n",
    "    def __init__(self, ACModelClass, env, args:Config=None, seed=0):\n",
    "        \"\"\"\n",
    "        The RRR agent.\n",
    "        \"\"\"\n",
    "\n",
    "        self.env = env\n",
    "\n",
    "        set_random_seed(seed)\n",
    "        m1 = ACModelClass(use_critic=True)\n",
    "\n",
    "        if args is None:\n",
    "            args = Config()\n",
    "\n",
    "        self.exploiter = Machine(0.01, m1, args, smart_discard=False)\n",
    "\n",
    "        m2 = ACModelClass(use_critic=False)\n",
    "        m3 = ACModelClass(use_critic=False)\n",
    "        m4 = ACModelClass(use_critic=False)\n",
    "\n",
    "        self.explorer_random = Machine(0.03, m2, args)\n",
    "        self.explorer_thirsty = Machine(0.02, m3, args)\n",
    "\n",
    "        self.randomer = RandomMachine(args)\n",
    "\n",
    "    def _replay(self, machine:Machine, buffer:list, cutoff:float) -> float:\n",
    "        \"\"\"\n",
    "        Replay random sample from buffer on machine.\n",
    "\n",
    "        Args:\n",
    "            machine: Machine to replay on\n",
    "            buffer: list of experiences\n",
    "            cutoff: if fit < cutoff, delete sb from buffer\n",
    "\n",
    "        Returns:\n",
    "            fit\n",
    "        \"\"\"\n",
    "\n",
    "        idx = np.random.randint(len(buffer))\n",
    "        sb = buffer[idx]\n",
    "        logs_replay = machine.update_parameters(sb)\n",
    "        fit = logs_replay['fit']\n",
    "        if fit < cutoff:\n",
    "            # delete sb from buffer\n",
    "            buffer.pop(idx)\n",
    "\n",
    "        return fit\n",
    "\n",
    "    def _add_sb_to_buffer(self, exps, buffer:list, capacity:int) -> None:\n",
    "        buffer.append(exps)\n",
    "        if len(buffer) > capacity:\n",
    "            buffer.pop(0)\n",
    "\n",
    "    def train(self, max_episodes:int=10000, nonstop:bool=False, max_frames=float('inf')) -> tuple[list[int], list[float], list[float]]:\n",
    "        \"\"\"\n",
    "        Train the agent.\n",
    "\n",
    "        Returns:\n",
    "            num_frames, smooth_rs, fits\n",
    "        \"\"\"\n",
    "\n",
    "        print('Start! Agent: random RRR.')\n",
    "        RANDOM_MODE = 0\n",
    "        EXPLORE_MODE = 1\n",
    "        EXPLOIT_MODE = 2\n",
    "        mode = RANDOM_MODE\n",
    "\n",
    "        is_solved = False\n",
    "\n",
    "        SMOOTH_REWARD_WINDOW = 50\n",
    "\n",
    "        rewards = [0]*SMOOTH_REWARD_WINDOW\n",
    "\n",
    "        total_smooth_rs = []\n",
    "        total_num_frames = []\n",
    "        larger_buffer_r = []\n",
    "        buffer_r = []\n",
    "        buffer_no_r = []\n",
    "        fits = []\n",
    "\n",
    "        num_frames = 0\n",
    "\n",
    "        pbar = tqdm(range(max_episodes))\n",
    "        for update in pbar:\n",
    "            total_num_frames.append(num_frames)\n",
    "\n",
    "            if mode == RANDOM_MODE:\n",
    "                exps, logs1 = self.randomer.collect_experiences(self.env)\n",
    "                logs2 = self.exploiter.update_parameters(exps)\n",
    "\n",
    "            elif mode == EXPLORE_MODE:\n",
    "                if np.random.rand() < 0.5:\n",
    "                    m = self.explorer_random\n",
    "                else:\n",
    "                    m = self.explorer_thirsty\n",
    "\n",
    "                exps, logs1 = m.collect_experiences(self.env)\n",
    "\n",
    "                m.update_parameters(exps)\n",
    "                logs2 = self.exploiter.update_parameters(exps)\n",
    "\n",
    "                if len(larger_buffer_r) >= 1:\n",
    "                    self._replay(self.explorer_thirsty, larger_buffer_r, cutoff=0.0)\n",
    "\n",
    "            elif mode == EXPLOIT_MODE:\n",
    "                exps, logs1 = self.exploiter.collect_experiences(self.env)\n",
    "\n",
    "                logs2 = self.exploiter.update_parameters(exps)\n",
    "\n",
    "                assert len(buffer_r) > 0, f'buffer_r should not be empty.'\n",
    "\n",
    "                cutoff = self.exploiter.args.bad_fit_threshold + self.exploiter.args.bad_fit_increment * (len(buffer_r) - 1)\n",
    "\n",
    "                if not logs1['success'] and total_smooth_rs[-1] <= 0.5:\n",
    "                    fit = self._replay(self.exploiter, buffer_r, cutoff)\n",
    "                    fits.append(fit)\n",
    "\n",
    "                if len(buffer_r) == 0:\n",
    "                    print(f'At episode {update}, we lost all data in replay buffer...')\n",
    "                    mode = EXPLORE_MODE\n",
    "                    self.explorer_thirsty.copy_machine(self.exploiter)\n",
    "                    self.explorer_random.copy_machine(self.exploiter)\n",
    "\n",
    "            else:\n",
    "                raise ValueError(f'Invalid mode: {mode}')\n",
    "\n",
    "            logs = {**logs1, **logs2}\n",
    "\n",
    "            num_frames += logs[\"num_frames\"]\n",
    "\n",
    "            rewards.append(logs[\"return_per_episode\"])\n",
    "\n",
    "            if logs['success']:\n",
    "                if mode == RANDOM_MODE:\n",
    "                    print(f'First successful data collected at episode {update}! We will be accelerating.')\n",
    "                elif mode == EXPLORE_MODE:\n",
    "                    if m is self.explorer_random:\n",
    "                        info_print = 'random explorer'\n",
    "                    elif m is self.explorer_thirsty:\n",
    "                        info_print = 'thirsty explorer'\n",
    "                    else:\n",
    "                        raise ValueError(f'Invalid explorer: {m}')\n",
    "                    \n",
    "                    print(f'At episode {update}, {info_print} collected a successful data.')\n",
    "                elif mode == EXPLOIT_MODE:\n",
    "                    pass\n",
    "                else:\n",
    "                    raise ValueError(f'Invalid mode: {mode}')\n",
    "\n",
    "                self._add_sb_to_buffer(exps, buffer_r, self.exploiter.args.replay_buffer_capacity)\n",
    "                self._add_sb_to_buffer(exps, larger_buffer_r, self.exploiter.args.large_buffer_capacity)\n",
    "                mode = EXPLOIT_MODE\n",
    "            else:\n",
    "                self._add_sb_to_buffer(exps, buffer_no_r, self.exploiter.args.replay_buffer_capacity)\n",
    "\n",
    "            smooth_reward = np.mean(rewards[-SMOOTH_REWARD_WINDOW:])\n",
    "\n",
    "            total_smooth_rs.append(smooth_reward)\n",
    "\n",
    "            data = {'num_frames':num_frames, 'smooth_reward':smooth_reward,\n",
    "                'reward':logs[\"return_per_episode\"], 'episode':update}\n",
    "\n",
    "            pbar.set_postfix(data)\n",
    "\n",
    "            if not nonstop and smooth_reward >= self.exploiter.args.score_threshold:\n",
    "                is_solved = True\n",
    "                break\n",
    "            if num_frames >= max_frames:\n",
    "                break\n",
    "\n",
    "        if is_solved:\n",
    "            print('Solved!')\n",
    "\n",
    "        return total_num_frames, total_smooth_rs, fits\n",
    "\n",
    "class CuriosityRRR:\n",
    "    def __init__(self, ACModelClass, env, args:Config=None, seed=0):\n",
    "        \"\"\"\n",
    "        The RRR agent.\n",
    "        \"\"\"\n",
    "\n",
    "        self.env = env\n",
    "\n",
    "        set_random_seed(seed)\n",
    "        m1 = ACModelClass(use_critic=True)\n",
    "\n",
    "        if args is None:\n",
    "            args = Config()\n",
    "\n",
    "        self.exploiter = Machine(0.01, m1, args, curiosity_factor=0.03)\n",
    "\n",
    "        m2 = ACModelClass(use_critic=True)\n",
    "        m3 = ACModelClass(use_critic=True)\n",
    "        m4 = ACModelClass(use_critic=False)\n",
    "\n",
    "        self.explorer_thirsty = Machine(0.02, m2, args, curiosity_factor=0.1)\n",
    "        self.starter = Machine(0.02, m3, args, curiosity_factor=0.1)\n",
    "\n",
    "        self.curiositor = CuriosityMachine(0.02, m4, args=args)\n",
    "\n",
    "    def _replay(self, machine:Machine, buffer:list, cutoff:float) -> float:\n",
    "        \"\"\"\n",
    "        Replay random sample from buffer on machine.\n",
    "\n",
    "        Args:\n",
    "            machine: Machine to replay on\n",
    "            buffer: list of experiences\n",
    "            cutoff: if fit < cutoff, delete sb from buffer\n",
    "\n",
    "        Returns:\n",
    "            fit\n",
    "        \"\"\"\n",
    "\n",
    "        idx = np.random.randint(len(buffer))\n",
    "        sb = buffer[idx]\n",
    "        logs_replay = machine.update_parameters(sb, curiositor=self.curiositor)\n",
    "        fit = logs_replay['fit']\n",
    "        if fit < cutoff:\n",
    "            # delete sb from buffer\n",
    "            buffer.pop(idx)\n",
    "\n",
    "        return fit\n",
    "\n",
    "    def _add_sb_to_buffer(self, exps, buffer:list, capacity:int) -> None:\n",
    "        buffer.append(exps)\n",
    "        if len(buffer) > capacity:\n",
    "            buffer.pop(0)\n",
    "\n",
    "\n",
    "    def train(self, max_episodes:int=10000, nonstop:bool=False, max_frames=float('inf')) -> tuple[list[int], list[float], list[float]]:\n",
    "        \"\"\"\n",
    "        Train the agent.\n",
    "\n",
    "        Returns:\n",
    "            num_frames, smooth_rs, fits\n",
    "        \"\"\"\n",
    "\n",
    "        print('Start! Agent: RRR.')\n",
    "        RANDOM_MODE = 0\n",
    "        EXPLORE_MODE = 1\n",
    "        EXPLOIT_MODE = 2\n",
    "        mode = RANDOM_MODE\n",
    "\n",
    "        is_solved = False\n",
    "\n",
    "        SMOOTH_REWARD_WINDOW = 50\n",
    "\n",
    "        rewards = [0]*SMOOTH_REWARD_WINDOW\n",
    "\n",
    "        total_smooth_rs = []\n",
    "        total_num_frames = []\n",
    "        larger_buffer_r = []\n",
    "        buffer_r = []\n",
    "        fits = []\n",
    "\n",
    "        num_frames = 0\n",
    "\n",
    "        pbar = tqdm(range(max_episodes))\n",
    "        for update in pbar:\n",
    "            total_num_frames.append(num_frames)\n",
    "\n",
    "            if mode == RANDOM_MODE:\n",
    "                exps, logs1 = self.starter.collect_experiences(self.env)\n",
    "                self.starter.update_parameters(exps, curiositor=self.curiositor)\n",
    "                logs2 = self.exploiter.update_parameters(exps, curiositor=self.curiositor)\n",
    "\n",
    "            elif mode == EXPLORE_MODE:\n",
    "                exps, logs1 = self.explorer_thirsty.collect_experiences(self.env)\n",
    "\n",
    "                self.explorer_thirsty.update_parameters(exps, curiositor=self.curiositor)\n",
    "                logs2 = self.exploiter.update_parameters(exps, curiositor=self.curiositor)\n",
    "\n",
    "                if len(larger_buffer_r) >= 1:\n",
    "                    self._replay(self.explorer_thirsty, larger_buffer_r, cutoff=0.0)\n",
    "\n",
    "            elif mode == EXPLOIT_MODE:\n",
    "                exps, logs1 = self.exploiter.collect_experiences(self.env)\n",
    "\n",
    "                logs2 = self.exploiter.update_parameters(exps, curiositor=self.curiositor)\n",
    "\n",
    "                assert len(buffer_r) > 0, f'buffer_r should not be empty.'\n",
    "\n",
    "                cutoff = self.exploiter.args.bad_fit_threshold + self.exploiter.args.bad_fit_increment * (len(buffer_r) - 1)\n",
    "\n",
    "                if not logs1['success'] and total_smooth_rs[-1] <= 0.5:\n",
    "                    fit = self._replay(self.exploiter, buffer_r, cutoff)\n",
    "                    fits.append(fit)\n",
    "\n",
    "                if len(buffer_r) == 0:\n",
    "                    print(f'At episode {update}, we lost all data in replay buffer...')\n",
    "                    mode = EXPLORE_MODE\n",
    "                    self.explorer_thirsty.copy_machine(self.exploiter)\n",
    "\n",
    "            else:\n",
    "                raise ValueError(f'Invalid mode: {mode}')\n",
    "\n",
    "            logs = {**logs1, **logs2}\n",
    "\n",
    "            num_frames += logs[\"num_frames\"]\n",
    "\n",
    "            rewards.append(logs[\"return_per_episode\"])\n",
    "\n",
    "            if logs['success']:\n",
    "                if mode == RANDOM_MODE:\n",
    "                    print(f'First successful data collected at episode {update}! We will be accelerating.')\n",
    "                elif mode == EXPLORE_MODE:\n",
    "                    print(f'At episode {update}, thirsty explorer collected a successful data.')\n",
    "                elif mode == EXPLOIT_MODE:\n",
    "                    pass\n",
    "                else:\n",
    "                    raise ValueError(f'Invalid mode: {mode}')\n",
    "                self._add_sb_to_buffer(exps, buffer_r, self.exploiter.args.replay_buffer_capacity)\n",
    "                self._add_sb_to_buffer(exps, larger_buffer_r, self.exploiter.args.large_buffer_capacity)\n",
    "                mode = EXPLOIT_MODE\n",
    "            else:\n",
    "                self.curiositor.update_parameters(exps)\n",
    "\n",
    "            smooth_reward = np.mean(rewards[-SMOOTH_REWARD_WINDOW:])\n",
    "\n",
    "            total_smooth_rs.append(smooth_reward)\n",
    "\n",
    "            data = {'num_frames':num_frames, 'smooth_reward':smooth_reward,\n",
    "                'reward':logs[\"return_per_episode\"], 'episode':update}\n",
    "\n",
    "            pbar.set_postfix(data)\n",
    "\n",
    "            if not nonstop and smooth_reward >= self.exploiter.args.score_threshold:\n",
    "                    is_solved = True\n",
    "                    break\n",
    "            if num_frames >= max_frames:\n",
    "                break\n",
    "\n",
    "        if is_solved:\n",
    "            print('Solved!')\n",
    "\n",
    "        return total_num_frames, total_smooth_rs, fits\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "size = 5\n",
    "seed = 10\n",
    "env = get_door_key_env(size)\n",
    "#change this for different sizes\n",
    "max_episodes = 10000 \n",
    "\n",
    "## Or crossing env:\n",
    "env = get_crossing_env(size, Lava)\n",
    "max_episodes = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 0th eval\n",
      "Start! Agent: RRR.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0423a62fe7494d6ab7bcc5be7d178a1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8m/r3kmm3xs1lngf9bhwg216mlc0000gn/T/ipykernel_49312/674935607.py:84: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  obs = torch.tensor(obs).float() # convert to float tensor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First successful data collected at episode 17! We will be accelerating.\n",
      "At episode 19, we lost all data in replay buffer...\n",
      "At episode 22, thirsty explorer collected a successful data.\n",
      "At episode 24, we lost all data in replay buffer...\n",
      "At episode 40, random explorer collected a successful data.\n",
      "At episode 48, we lost all data in replay buffer...\n",
      "At episode 53, thirsty explorer collected a successful data.\n",
      "At episode 54, we lost all data in replay buffer...\n",
      "At episode 60, thirsty explorer collected a successful data.\n",
      "At episode 61, we lost all data in replay buffer...\n",
      "At episode 62, random explorer collected a successful data.\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "\n",
    "    print(f\"Starting {i}th eval\")\n",
    "\n",
    "    hps = [(0.9, 2.0), (0.8, 2.0), (0.77, 2.0), (0.7, 2.0), (0.6, 2.0), (0.5, 2.0)]\n",
    "    agents = []\n",
    "    logs = []\n",
    "\n",
    "    for hp in hps:\n",
    "        agent = RRR(ACModel, env=env, args=Config(bad_fit_threshold=hp[0], importance_sampling_clip=hp[1]), seed=seed)\n",
    "        agents.append(agent)\n",
    "\n",
    "    for agent in agents:\n",
    "        num_frames, smooth_rs, fits = agent.train(max_episodes)\n",
    "        logs.append((num_frames, smooth_rs, fits))\n",
    "\n",
    "    plt.figure()\n",
    "    for j in range(len(hps)):\n",
    "        log = logs[j]\n",
    "        plt.plot(log[0], log[1], label=f'threshold={hps[j][0]}, clip={hps[j][1]}')\n",
    "        \n",
    "    plt.xlabel('Time steps')\n",
    "    plt.ylabel('Average reward (smoothed)')\n",
    "    # Change this for different envs\n",
    "    plt.title(f'DoorKeyEnv, size = {size}, seed = {seed}')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
